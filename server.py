# -- coding: utf-8 --
import asyncio
import copy
import json
import os
import re
from fastapi import BackgroundTasks, FastAPI, File, HTTPException, UploadFile, WebSocket, Request
import logging
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
from pydantic import BaseModel
from fastapi import status
from fastapi.responses import JSONResponse, StreamingResponse
import uuid
import time
from typing import List, Dict
import shortuuid
from py.mcp_clients import McpClient
from contextlib import asynccontextmanager
import argparse
parser = argparse.ArgumentParser(description="Run the ASGI application server.")
parser.add_argument("--host", default="127.0.0.1", help="Host for the ASGI server, default is 127.0.0.1")
parser.add_argument("--port", type=int, default=3456, help="Port for the ASGI server, default is 3456")
args = parser.parse_args()
HOST = args.host
PORT = args.port

os.environ["no_proxy"] = "localhost,127.0.0.1"
local_timezone = None
logger = None
settings = None
client = None
reasoner_client = None
mcp_client_list = {}
locales = {}
_TOOL_HOOKS = {}

from py.get_setting import load_settings,save_settings,base_path,configure_host_port,UPLOAD_FILES_DIR,AGENT_DIR
from py.llm_tool import get_image_base64,get_image_media_type


configure_host_port(args.host, args.port)

@asynccontextmanager
async def lifespan(app: FastAPI): 
    from py.get_setting import init_db
    await init_db()
    global settings, client, reasoner_client, mcp_client_list,local_timezone,logger,locales
    with open(base_path + "/config/locales.json", "r", encoding="utf-8") as f:
        locales = json.load(f)
    from tzlocal import get_localzone
    local_timezone = get_localzone()
    logger = logging.getLogger(__name__)
    settings = await load_settings()
    if settings:
        client = AsyncOpenAI(api_key=settings['api_key'], base_url=settings['base_url'])
        reasoner_client = AsyncOpenAI(api_key=settings['reasoner']['api_key'],base_url=settings['reasoner']['base_url'])
    else:
        client = AsyncOpenAI()
        reasoner_client = AsyncOpenAI()
    if settings:
        for server_name,server_config in settings['mcpServers'].items():
            mcp_client_list[server_name] = McpClient()
            # åˆå§‹åŒ–mcpå®¢æˆ·ç«¯ï¼Œé™åˆ¶10ç§’å†…ï¼Œå¦åˆ™è·³è¿‡
            try:
                await asyncio.wait_for(mcp_client_list[server_name].initialize(server_name, server_config), timeout=5)
            except asyncio.TimeoutError:
                logger.error(f"Failed to initialize MCP client for {server_name} in 5 seconds")
                mcp_client_list[server_name].disabled = True
                del settings['mcpServers'][server_name]
        await save_settings(settings)
    yield

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def t(text: str) -> str:
    global locales
    settings = await load_settings()
    target_language = settings["systemSettings"]["language"]
    return locales[target_language].get(text, text)

async def dispatch_tool(tool_name: str, tool_params: dict) -> str:
    global mcp_client_list,_TOOL_HOOKS
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        jina_crawler_async,
        Crawl4Ai_search_async, 
    )
    from py.know_base import query_knowledge_base
    from py.agent_tool import agent_tool_call
    from py.a2a_tool import a2a_tool_call
    from py.llm_tool import custom_llm_tool
    from py.pollinations import pollinations_image
    _TOOL_HOOKS = {
        "DDGsearch_async": DDGsearch_async,
        "searxng_async": searxng_async,
        "Tavily_search_async": Tavily_search_async,
        "query_knowledge_base": query_knowledge_base,
        "jina_crawler_async": jina_crawler_async,
        "Crawl4Ai_search_async": Crawl4Ai_search_async,
        "agent_tool_call": agent_tool_call,
        "a2a_tool_call": a2a_tool_call,
        "custom_llm_tool": custom_llm_tool,
        "pollinations_image":pollinations_image,
    }
    if "multi_tool_use." in tool_name:
        tool_name = tool_name.replace("multi_tool_use.", "")
    if tool_name not in _TOOL_HOOKS:
        for server_name, mcp_client in mcp_client_list.items():
            if tool_name in mcp_client.tools_list:
                result = await mcp_client.call_tool(tool_name, tool_params)
                return str(result.model_dump())
        return None
    tool_call = _TOOL_HOOKS[tool_name]
    try:
        ret_out = await tool_call(**tool_params)
        return ret_out
    except Exception as e:
        logger.error(f"Error calling tool {tool_name}: {e}")
        return f"Error calling tool {tool_name}: {e}"


class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str = None
    temperature: float = 0.7
    tools: dict = None
    stream: bool = False
    max_tokens: int = None
    top_p: float = 1
    frequency_penalty: float = 0
    presence_penalty: float = 0
    fileLinks: List[str] = None

async def message_without_images(messages: List[Dict]) -> List[Dict]:
    if messages:
        for message in messages:
            if 'content' in message:
                # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
                if isinstance(message['content'], list):
                    for item in message['content']:
                        if isinstance(item, dict) and item['type'] == 'text':
                            message['content'] = item['text']
                            break
    return messages

async def images_in_messages(messages: List[Dict]) -> List[Dict]:
    import hashlib
    images = []
    index = 0
    for message in messages:
        image_urls = []
        if 'content' in message:
            # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
            if isinstance(message['content'], list):
                for item in message['content']:
                    if isinstance(item, dict) and item['type'] == 'image_url':
                        # å¦‚æœitem["image_url"]["url"]æ˜¯httpæˆ–httpså¼€å¤´ï¼Œåˆ™è½¬æ¢æˆbase64
                        if item["image_url"]["url"].startswith("http"):
                            image_url = item["image_url"]["url"]
                            base64_image = await get_image_base64(image_url)
                            media_type = await get_image_media_type(image_url)
                            item["image_url"]["url"] = f"data:{media_type};base64,{base64_image}"
                            item["image_url"]["hash"] = hashlib.md5(item["image_url"]["url"].encode()).hexdigest()
                        image_urls.append(item)
        if image_urls:
            images.append({'index': index, 'images': image_urls})
        index += 1
    return images

async def images_add_in_messages(request_messages: List[Dict], images: List[Dict], settings: dict) -> List[Dict]:
    messages=copy.deepcopy(request_messages)
    if settings['vision']['enabled']:
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": item['image_url']['url']}}]
                            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
                            response = await client.chat.completions.create(
                                model=settings['vision']['model'],
                                messages = [{"role": "user", "content": images_content}],
                                temperature=settings['vision']['temperature'],
                            )
                            messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "w", encoding='utf-8') as f:
                                f.write(str(response.choices[0].message.content))
    else:           
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            messages[index]['content'] = [{"type": "text", "text": messages[index]['content']}]
                            messages[index]['content'].append({"type": "image_url", "image_url": {"url": item['image_url']['url']}})
    return messages

async def tools_change_messages(request: ChatRequest, settings: dict):
    if settings['tools']['time']['enabled']:
        time_message = f"å½“å‰ç³»ç»Ÿæ—¶é—´ï¼š{local_timezone}  {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += time_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': time_message})
    if settings['tools']['inference']['enabled']:
        inference_message = "å›ç­”ç”¨æˆ·å‰è¯·å…ˆæ€è€ƒæ¨ç†ï¼Œå†å›ç­”é—®é¢˜ï¼Œä½ çš„æ€è€ƒæ¨ç†çš„è¿‡ç¨‹å¿…é¡»æ”¾åœ¨<think>ä¸</think>ä¹‹é—´ã€‚\n\n"
        request.messages[-1]['content'] = f"{inference_message}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
    if settings['tools']['formula']['enabled']:
        latex_message = "\n\nå½“ä½ æƒ³ä½¿ç”¨latexå…¬å¼æ—¶ï¼Œä½ å¿…é¡»æ˜¯ç”¨ ['$', '$'] ä½œä¸ºè¡Œå†…å…¬å¼å®šç•Œç¬¦ï¼Œä»¥åŠ ['$$', '$$'] ä½œä¸ºè¡Œé—´å…¬å¼å®šç•Œç¬¦ã€‚\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += latex_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': latex_message})
    if settings['tools']['language']['enabled']:
        language_message = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€æ¨ç†åˆ†ææ€è€ƒï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨ç†åˆ†æï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += language_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': language_message})
    return request

async def generate_stream_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url):
    global mcp_client_list
    images = await images_in_messages(request.messages)
    request.messages = await message_without_images(request.messages)
    from py.load_files import get_files_content
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool
    open_tag = "<think>"
    close_tag = "</think>"
    try:
        tools = request.tools or []
        if mcp_client_list:
            for server_name, mcp_client in mcp_client_list.items():
                if server_name in settings['mcpServers']:
                    if 'disabled' not in settings['mcpServers'][server_name]:
                        settings['mcpServers'][server_name]['disabled'] = False
                    if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                        function = await mcp_client.get_openai_functions()
                        if function:
                            tools.extend(function)
        get_llm_tool_fuction = await get_llm_tool(settings)
        if get_llm_tool_fuction:
            tools.append(get_llm_tool_fuction)
        get_agent_tool_fuction = await get_agent_tool(settings)
        if get_agent_tool_fuction:
            tools.append(get_agent_tool_fuction)
        get_a2a_tool_fuction = await get_a2a_tool(settings)
        if get_a2a_tool_fuction:
            tools.append(get_a2a_tool_fuction)
        if settings['tools']['pollinations']['enabled']:
            tools.append(pollinations_image_tool)
        source_prompt = ""
        if request.fileLinks:
            print("fileLinks",request.fileLinks)
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            fileLinks_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += fileLinks_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': fileLinks_message})
            source_prompt += fileLinks_message
        user_prompt = request.messages[-1]['content']
        request = await tools_change_messages(request, settings)
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        extra_params['enable_thinking'] = False
        async def stream_generator(user_prompt):
            kb_list = []
            if settings["knowledgeBases"]:
                for kb in settings["knowledgeBases"]:
                    if kb["enabled"] and kb["processingStatus"] == "completed":
                        kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
            if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
                if kb_list:
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": f"{await t("KB_search")}\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    all_kb_content = ""
                    # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                    for kb in kb_list:
                        kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                        all_kb_content += kb_content +"\n\n"
                    if all_kb_content:
                        kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                        request.messages[-1]['content'] += f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}"
                                                # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥UPLOAD_FILES_DIRæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                            f.write(str(all_kb_content))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[{await t("search_result")}]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
            if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
                if kb_list:
                    kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                    if request.messages and request.messages[0]['role'] == 'system':
                        request.messages[0]['content'] += kb_list_message
                    else:
                        request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
            else:
                kb_list = []
            if settings['webSearch']['enabled']:
                if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": f"{await t("web_search")}\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        results = await DDGsearch_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'searxng':
                        results = await searxng_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'tavily':
                        results = await Tavily_search_async(user_prompt)
                    if results:
                        request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}\n\nè¯·æ ¹æ®è”ç½‘æœç´¢ç»“æœç»„ç»‡ä½ çš„å›ç­”ï¼Œå¹¶ç¡®ä¿ä½ çš„å›ç­”æ˜¯å‡†ç¡®çš„ã€‚"
                        # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                            f.write(str(results))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[{await t("search_result")}]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
                if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        tools.append(duckduckgo_tool)
                    elif settings['webSearch']['engine'] == 'searxng':
                        tools.append(searxng_tool)
                    elif settings['webSearch']['engine'] == 'tavily':
                        tools.append(tavily_tool)
                    if settings['webSearch']['crawler'] == 'jina':
                        tools.append(jina_crawler_tool)
                    elif settings['webSearch']['crawler'] == 'crawl4ai':
                        tools.append(Crawl4Ai_tool)
            if kb_list:
                tools.append(kb_tool)
            if settings['tools']['deepsearch']['enabled']: 
                deepsearch_messages = copy.deepcopy(request.messages)
                deepsearch_messages[-1]['content'] += "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚"
                print(request.messages[-1]['content'])
                response = await client.chat.completions.create(
                    model=model,
                    messages=deepsearch_messages,
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                user_prompt = response.choices[0].message.content
                deepsearch_chunk = {
                    "choices": [{
                        "delta": {
                            "reasoning_content": f"\n\nğŸ’–{await t("start_task")}{user_prompt}\n\n",
                        }
                    }]
                }
                yield f"data: {json.dumps(deepsearch_chunk)}\n\n"
                request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
                print(request.messages[-1]['content'])
            # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
            if settings['reasoner']['enabled']:
                reasoner_messages = copy.deepcopy(request.messages)
                if settings['tools']['deepsearch']['enabled']: 
                    reasoner_messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=True,
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                    in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                    
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue
                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            current_content = delta.get("content", "")
                            buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                            
                            # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                            while True:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                        non_reasoning = buffer[:start_pos]
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                else:
                                    # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                        reasoning_part = buffer[:end_pos]
                                        chunk_dict["choices"][0]["delta"] = {
                                            "reasoning_content": reasoning_part,
                                            "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                        }
                                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                                        full_reasoning += reasoning_part
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                        if buffer:
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": buffer,
                                                "content": ""
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += buffer
                                            buffer = ""
                                        break  # ç­‰å¾…æ›´å¤šå†…å®¹
                else:
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=True,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue

                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            reasoning_content = delta.get("reasoning_content", "")
                            if reasoning_content:
                                full_reasoning += reasoning_content
                        yield f"data: {json.dumps(chunk_dict)}\n\n"

                # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
            # çŠ¶æ€è·Ÿè¸ªå˜é‡
            in_reasoning = False
            reasoning_buffer = []
            content_buffer = []
            if settings['tools']['deepsearch']['enabled']: 
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            tool_calls = []
            full_content = ""
            search_not_done = False
            search_task = ""
            async for chunk in response:
                if not chunk.choices:
                    continue
                choice = chunk.choices[0]
                if choice.delta.tool_calls:  # function_calling
                    for idx, tool_call in enumerate(choice.delta.tool_calls):
                        tool = choice.delta.tool_calls[idx]
                        if len(tool_calls) <= idx:
                            tool_calls.append(tool)
                            continue
                        if tool.function.arguments:
                            # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                            tool_calls[idx].function.arguments += tool.function.arguments
                else:
                    # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                    chunk_dict = chunk.model_dump()
                    delta = chunk_dict["choices"][0]["delta"]
                    
                    # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                    delta.setdefault("content", "")
                    delta.setdefault("reasoning_content", "")
                    
                    # ä¼˜å…ˆå¤„ç† reasoning_content
                    if delta["reasoning_content"]:
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        continue

                    # å¤„ç†å†…å®¹
                    current_content = delta["content"]
                    buffer = current_content
                    
                    while buffer:
                        if not in_reasoning:
                            # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                            start_pos = buffer.find(open_tag)
                            if start_pos != -1:
                                # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                content_buffer.append(buffer[:start_pos])
                                buffer = buffer[start_pos+len(open_tag):]
                                in_reasoning = True
                            else:
                                content_buffer.append(buffer)
                                buffer = ""
                        else:
                            # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                            end_pos = buffer.find(close_tag)
                            if end_pos != -1:
                                # å¤„ç†æ€è€ƒå†…å®¹
                                reasoning_buffer.append(buffer[:end_pos])
                                buffer = buffer[end_pos+len(close_tag):]
                                in_reasoning = False
                            else:
                                reasoning_buffer.append(buffer)
                                buffer = ""
                    
                    # æ„é€ æ–°çš„deltaå†…å®¹
                    new_content = "".join(content_buffer)
                    new_reasoning = "".join(reasoning_buffer)
                    
                    # æ›´æ–°chunkå†…å®¹
                    delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                    delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                    
                    # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                    if in_reasoning:
                        content_buffer = [new_content.split(open_tag)[-1]] 
                    else:
                        content_buffer = []
                    reasoning_buffer = []
                    
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    full_content += delta.get("content", "")
            # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
            if content_buffer or reasoning_buffer:
                final_chunk = {
                    "choices": [{
                        "delta": {
                            "content": "".join(content_buffer),
                            "reasoning_content": "".join(reasoning_buffer)
                        }
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                full_content += final_chunk["choices"][0]["delta"].get("content", "")
            if tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled']: 
                search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{full_content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
                response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "system",
                        "content": source_prompt,
                        },
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                try:
                    response_content = json.loads(response_content)
                except json.JSONDecodeError:
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâŒ{await t("task_error")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                if response_content["status"] == "done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâœ…{await t("task_done")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ{await t("task_not_done")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ“{await t("task_need_more_info")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
            reasoner_messages = copy.deepcopy(request.messages)
            while tool_calls or search_not_done:
                full_content = ""
                if tool_calls:
                    response_content = tool_calls[0].function
                    if response_content.name in  ["DDGsearch_async","searxng_async", "Tavily_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("web_search")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in  ["jina_crawler_async","Crawl4Ai_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("web_search_more")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in ["query_knowledge_base"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("knowledge_base")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    else:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("call")}{response_content.name}{await t("tool")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    print(response_content.arguments)
                    modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                    print(modified_data)
                    # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                    data_list = json.loads(modified_data)
                    results = await dispatch_tool(response_content.name, data_list[0])
                    if results is None:
                        chunk = {
                            "id": "extra_tools",
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "tool_calls":modified_data,
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        break
                    request.messages.append(
                        {
                            "tool_calls": [
                                {
                                    "id": tool_calls[0].id,
                                    "function": {
                                        "arguments": json.dumps(data_list[0]),
                                        "name": response_content.name,
                                    },
                                    "type": tool_calls[0].type,
                                }
                            ],
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    request.messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_calls[0].id,
                            "name": response_content.name,
                            "content": str(results),
                        }
                    )
                    if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                        request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
                    reasoner_messages.append(
                        {
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    reasoner_messages.append(
                        {
                            "role": "user",
                            "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                        }
                    )
                    # è·å–æ—¶é—´æˆ³å’Œuuid
                    timestamp = time.time()
                    uid = str(uuid.uuid4())
                    # æ„é€ æ–‡ä»¶å
                    filename = f"{timestamp}_{uid}.txt"
                    # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                    with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                        f.write(str(results))            
                    # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                    fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                    tool_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\n[{response_content.name}{await t("tool_result")}]({fileLink})\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(tool_chunk)}\n\n"
                # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
                if settings['reasoner']['enabled']:
                    if tools:
                        reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                    for modelProvider in settings['modelProviders']: 
                        if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                            vendor = modelProvider['vendor']
                            break
                    msg = await images_add_in_messages(reasoner_messages, images,settings)
                    if vendor == 'Ollama':
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            temperature=settings['reasoner']['temperature']
                        )
                        full_reasoning = ""
                        buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                        in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                        
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                current_content = delta.get("content", "")
                                buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                                
                                # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                                while True:
                                    if not in_reasoning:
                                        # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                        start_pos = buffer.find(open_tag)
                                        if start_pos != -1:
                                            # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                            non_reasoning = buffer[:start_pos]
                                            buffer = buffer[start_pos+len(open_tag):]
                                            in_reasoning = True
                                        else:
                                            break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                    else:
                                        # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                        end_pos = buffer.find(close_tag)
                                        if end_pos != -1:
                                            # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                            reasoning_part = buffer[:end_pos]
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": reasoning_part,
                                                "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += reasoning_part
                                            buffer = buffer[end_pos+len(close_tag):]
                                            in_reasoning = False
                                        else:
                                            # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                            if buffer:
                                                chunk_dict["choices"][0]["delta"] = {
                                                    "reasoning_content": buffer,
                                                    "content": ""
                                                }
                                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                                full_reasoning += buffer
                                                buffer = ""
                                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                    else:
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                            temperature=settings['reasoner']['temperature']
                        )
                        full_reasoning = ""
                        # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue

                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                reasoning_content = delta.get("reasoning_content", "")
                                if reasoning_content:
                                    full_reasoning += reasoning_content
                            yield f"data: {json.dumps(chunk_dict)}\n\n"

                    # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                    request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
                msg = await images_add_in_messages(request.messages, images,settings)
                if tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        tools=tools,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p or settings['top_p'],
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p or settings['top_p'],
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                tool_calls = []
                async for chunk in response:
                    if not chunk.choices:
                        continue
                    if chunk.choices:
                        choice = chunk.choices[0]
                        if choice.delta.tool_calls:  # function_calling
                            for idx, tool_call in enumerate(choice.delta.tool_calls):
                                tool = choice.delta.tool_calls[idx]
                                if len(tool_calls) <= idx:
                                    tool_calls.append(tool)
                                    continue
                                if tool.function.arguments:
                                    # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                                    tool_calls[idx].function.arguments += tool.function.arguments
                        else:
                            # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0]["delta"]
                            
                            # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                            delta.setdefault("content", "")
                            delta.setdefault("reasoning_content", "")

                             # ä¼˜å…ˆå¤„ç† reasoning_content
                            if delta["reasoning_content"]:
                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                continue
                            
                            # å¤„ç†å†…å®¹
                            current_content = delta["content"]
                            buffer = current_content
                            
                            while buffer:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                        content_buffer.append(buffer[:start_pos])
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        content_buffer.append(buffer)
                                        buffer = ""
                                else:
                                    # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # å¤„ç†æ€è€ƒå†…å®¹
                                        reasoning_buffer.append(buffer[:end_pos])
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        reasoning_buffer.append(buffer)
                                        buffer = ""
                            
                            # æ„é€ æ–°çš„deltaå†…å®¹
                            new_content = "".join(content_buffer)
                            new_reasoning = "".join(reasoning_buffer)
                            
                            # æ›´æ–°chunkå†…å®¹
                            delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                            delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                            
                            # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                            if in_reasoning:
                                content_buffer = [new_content.split(open_tag)[-1]] 
                            else:
                                content_buffer = []
                            reasoning_buffer = []
                            
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            full_content += delta.get("content", "")
                # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
                if content_buffer or reasoning_buffer:
                    final_chunk = {
                        "choices": [{
                            "delta": {
                                "content": "".join(content_buffer),
                                "reasoning_content": "".join(reasoning_buffer)
                            }
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                    full_content += final_chunk["choices"][0]["delta"].get("content", "")
                if tool_calls:
                    pass
                elif settings['tools']['deepsearch']['enabled']: 
                    search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{full_content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
                    response = await client.chat.completions.create(
                        model=model,
                        messages=[                        
                            {
                            "role": "system",
                            "content": source_prompt,
                            },
                            {
                            "role": "user",
                            "content": search_prompt,
                            }
                        ],
                        temperature=0.5,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                    response_content = response.choices[0].message.content
                    # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                    if "```json" in response_content:
                        try:
                            response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                        except:
                            # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                            response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                    try:
                        response_content = json.loads(response_content)
                    except json.JSONDecodeError:
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâŒ{await t("task_error")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                    if response_content["status"] == "done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâœ…{await t("task_done")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "not_done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ{await t("task_not_done")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "need_more_info":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ“{await t("task_need_more_info")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
            yield "data: [DONE]\n\n"

        return StreamingResponse(
            stream_generator(user_prompt),
            media_type="text/event-stream",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={"error": {"message": e.message, "type": "api_error", "code": e.code}}
        )

async def generate_complete_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url):
    global mcp_client_list
    from py.load_files import get_files_content
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool
    images = await images_in_messages(request.messages)
    request.messages = await message_without_images(request.messages)
    open_tag = "<think>"
    close_tag = "</think>"
    tools = request.tools or []
    tools = request.tools or []
    print(tools)
    if mcp_client_list:
        for server_name, mcp_client in mcp_client_list.items():
            if server_name in settings['mcpServers']:
                if 'disabled' not in settings['mcpServers'][server_name]:
                    settings['mcpServers'][server_name]['disabled'] = False
                if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                    function = await mcp_client.get_openai_functions()
                    if function:
                        tools.extend(function)
    get_llm_tool_fuction = await get_llm_tool(settings)
    if get_llm_tool_fuction:
        tools.append(get_llm_tool_fuction)
    get_agent_tool_fuction = await get_agent_tool(settings)
    if get_agent_tool_fuction:
        tools.append(get_agent_tool_fuction)
    get_a2a_tool_fuction = await get_a2a_tool(settings)
    if get_a2a_tool_fuction:
        tools.append(get_a2a_tool_fuction)
    if settings['tools']['pollinations']['enabled']:
        tools.append(pollinations_image_tool)
    search_not_done = False
    search_task = ""
    try:
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        extra_params['enable_thinking'] = False
        if request.fileLinks:
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            system_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += system_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': system_message})
        kb_list = []
        user_prompt = request.messages[-1]['content']
        if settings["knowledgeBases"]:
            for kb in settings["knowledgeBases"]:
                if kb["enabled"] and kb["processingStatus"] == "completed":
                    kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
        if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                all_kb_content = ""
                # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                for kb in kb_list:
                    kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                    all_kb_content += kb_content +"\n\n"
                if all_kb_content:
                    kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                    request.messages[-1]['content'] += f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}"
        if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += kb_list_message
                else:
                    request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
        else:
            kb_list = []
        request = await tools_change_messages(request, settings)
        if settings['webSearch']['enabled']:
            if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    results = await DDGsearch_async(user_prompt)
                elif settings['webSearch']['engine'] == 'searxng':
                    results = await searxng_async(user_prompt)
                elif settings['webSearch']['engine'] == 'tavily':
                    results = await Tavily_search_async(user_prompt)
                if results:
                    request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}"
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    tools.append(duckduckgo_tool)
                elif settings['webSearch']['engine'] == 'searxng':
                    tools.append(searxng_tool)
                elif settings['webSearch']['engine'] == 'tavily':
                    tools.append(tavily_tool)
                if settings['webSearch']['crawler'] == 'jina':
                    tools.append(jina_crawler_tool)
                elif settings['webSearch']['crawler'] == 'crawl4ai':
                    tools.append(Crawl4Ai_tool)
        if kb_list:
            tools.append(kb_tool)
        if settings['tools']['deepsearch']['enabled']: 
            deepsearch_messages = copy.deepcopy(request.messages)
            deepsearch_messages[-1]['content'] += "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚"
            response = await client.chat.completions.create(
                model=model,
                messages=deepsearch_messages,
                temperature=0.5, 
                max_tokens=512,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            user_prompt = response.choices[0].message.content
            request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
        if settings['reasoner']['enabled']:
            reasoner_messages = copy.deepcopy(request.messages)
            if settings['tools']['deepsearch']['enabled']: 
                reasoner_messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
            if tools:
                reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
            for modelProvider in settings['modelProviders']: 
                if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                    vendor = modelProvider['vendor']
                    break
            msg = await images_add_in_messages(reasoner_messages, images,settings)    
            if vendor == 'Ollama':
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    temperature=settings['reasoner']['temperature']
                )
                # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                start_index = reasoning_content.find(open_tag) + len(open_tag)
                end_index = reasoning_content.find(close_tag)
                if start_index != -1 and end_index != -1:
                    reasoning_content = reasoning_content[start_index:end_index]
                else:
                    reasoning_content = ""
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoning_content
            else:
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    temperature=settings['reasoner']['temperature']
                )
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        if settings['tools']['deepsearch']['enabled']: 
            request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
        msg = await images_add_in_messages(request.messages, images,settings)
        if tools:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                tools=tools,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p or settings['top_p'],
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p or settings['top_p'],
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
        if response.choices[0].message.tool_calls:
            pass
        elif settings['tools']['deepsearch']['enabled']: 
            search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{response.choices[0].message.content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
            search_response = await client.chat.completions.create(
                model=model,
                messages=[
                    {
                    "role": "user",
                    "content": search_prompt,
                    }
                ],
                temperature=0.5,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            response_content = search_response.choices[0].message.content
            print(response_content)
            # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
            if "```json" in response_content:
                try:
                    response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                except:
                    # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                    response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
            response_content = json.loads(response_content)
            if response_content["status"] == "done":
                search_not_done = False
            elif response_content["status"] == "not_done":
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": search_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "need_more_info":
                search_not_done = False
        reasoner_messages = copy.deepcopy(request.messages)
        while response.choices[0].message.tool_calls or search_not_done:
            if response.choices[0].message.tool_calls:
                assistant_message = response.choices[0].message
                response_content = assistant_message.tool_calls[0].function
                print(response_content.name)
                modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                data_list = json.loads(modified_data)
                # å­˜å‚¨å¤„ç†ç»“æœ
                results = []
                for data in data_list:
                    result = await dispatch_tool(response_content.name, data) # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                    if result is not None:
                        # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                        results.append(json.dumps(result))

                # å°†æ‰€æœ‰ç»“æœæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
                combined_results = ''.join(results)
                if combined_results:
                    results = combined_results
                else:
                    results = None
                print(results)
                if results is None:
                    break
                request.messages.append(
                    {
                        "tool_calls": [
                            {
                                "id": assistant_message.tool_calls[0].id,
                                "function": {
                                    "arguments": response_content.arguments,
                                    "name": response_content.name,
                                },
                                "type": assistant_message.tool_calls[0].type,
                            }
                        ],
                        "role": "assistant",
                        "content": str(response_content),
                    }
                )
                request.messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": assistant_message.tool_calls[0].id,
                        "name": response_content.name,
                        "content": str(results),
                    }
                )
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
            reasoner_messages.append(
                {
                    "role": "assistant",
                    "content": str(response_content),
                }
            )
            reasoner_messages.append(
                {
                    "role": "user",
                    "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                }
            )
            if settings['reasoner']['enabled']:

                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        temperature=settings['reasoner']['temperature']
                    )
                    # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                    reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                    # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                    start_index = reasoning_content.find(open_tag) + len(open_tag)
                    end_index = reasoning_content.find(close_tag)
                    if start_index != -1 and end_index != -1:
                        reasoning_content = reasoning_content[start_index:end_index]
                    else:
                        reasoning_content = ""
                    request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoning_content
                else:
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            print(response)
            if response.choices[0].message.tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled']: 
                search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{response.choices[0].message.content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
                search_response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = search_response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                response_content = json.loads(response_content)
                if response_content["status"] == "done":
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": search_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    search_not_done = False
       # å¤„ç†å“åº”å†…å®¹
        response_dict = response.model_dump()
        content = response_dict["choices"][0]['message']['content']
        if open_tag in content and close_tag in content:
            reasoning_content = re.search(fr'{open_tag}(.*?)\{close_tag}', content, re.DOTALL)
            if reasoning_content:
                # å­˜å‚¨åˆ° reasoning_content å­—æ®µ
                response_dict["choices"][0]['message']['reasoning_content'] = reasoning_content.group(1).strip()
                # ç§»é™¤åŸå†…å®¹ä¸­çš„æ ‡ç­¾éƒ¨åˆ†
                response_dict["choices"][0]['message']['content'] = re.sub(fr'{open_tag}(.*?)\{close_tag}', '', content, flags=re.DOTALL).strip()
        if settings['reasoner']['enabled']:
            response_dict["choices"][0]['message']['reasoning_content'] = reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        return JSONResponse(content=response_dict)
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={"error": {"message": e.message, "type": "api_error", "code": e.code}}
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/models")
async def get_models():
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            Model(
                id=agent["name"],  
                created=0,  
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
            for agent in agents.values()  
        ]
        # æ„é€ å®Œæ•´ SyncPage å“åº”
        response = SyncPage[Model](
            object="list",
            data=model_data,
            has_more=False  # æ·»åŠ åˆ†é¡µæ ‡è®°
        )
        # ç›´æ¥è¿”å›æ¨¡å‹å­—å…¸ï¼Œç”± FastAPI è‡ªåŠ¨åºåˆ—åŒ–ä¸º JSON
        return response.model_dump()  
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": e.message,
                    "type": e.type or "api_error",
                    "code": e.code
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

class ProviderModelRequest(BaseModel):
    url: str
    api_key: str

@app.post("/v1/providers/models")
async def fetch_provider_models(request: ProviderModelRequest):
    try:
        # ä½¿ç”¨ä¼ å…¥çš„provideré…ç½®åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        client = AsyncOpenAI(api_key=request.api_key, base_url=request.url)
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = await client.models.list()
        # æå–æ¨¡å‹IDå¹¶è¿”å›
        return JSONResponse(content={"data": [model.id for model in model_list.data]})
    except Exception as e:
        # å¤„ç†å¼‚å¸¸ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions")
async def chat_endpoint(request: ChatRequest,fastapi_request: Request):
    fastapi_base_url = str(fastapi_request.base_url)
    global client, settings,reasoner_client,mcp_client_list
    model = request.model or 'super-model' # é»˜è®¤ä½¿ç”¨ 'super-model'
    if model == 'super-model':
        current_settings = await load_settings()
        # åŠ¨æ€æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        if (current_settings['api_key'] != settings['api_key'] 
            or current_settings['base_url'] != settings['base_url']):
            client = AsyncOpenAI(
                api_key=current_settings['api_key'],
                base_url=current_settings['base_url'] or "https://api.openai.com/v1",
            )
        if (current_settings['reasoner']['api_key'] != settings['reasoner']['api_key'] 
            or current_settings['reasoner']['base_url'] != settings['reasoner']['base_url']):
            reasoner_client = AsyncOpenAI(
                api_key=current_settings['reasoner']['api_key'],
                base_url=current_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
            )
        if current_settings != settings:
            settings = current_settings
        try:
            if request.stream:
                return await generate_stream_response(client,reasoner_client, request, settings,fastapi_base_url)
            return await generate_complete_response(client,reasoner_client, request, settings,fastapi_base_url)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    else:
        current_settings = await load_settings()
        agentSettings = current_settings['agents'].get(model, {})
        if not agentSettings:
            for agentId , agentConfig in current_settings['agents'].items():
                if current_settings['agents'][agentId]['name'] == model:
                    agentSettings = current_settings['agents'][agentId]
                    break
        if not agentSettings:
            return JSONResponse(
                status_code=404,
                content={"error": {"message": f"Agent {model} not found", "type": "not_found", "code": 404}}
            )
        if agentSettings['config_path']:
            with open(agentSettings['config_path'], 'r' , encoding='utf-8') as f:
                agent_settings = json.load(f)
            # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
            if agentSettings['system_prompt']:
                if request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] = agentSettings['system_prompt'] + "\n\n" + request.messages[0]['content']
                else:
                    request.messages.insert(0, {'role': 'system', 'content': agentSettings['system_prompt']})
        agent_client = AsyncOpenAI(
            api_key=agent_settings['api_key'],
            base_url=agent_settings['base_url'] or "https://api.openai.com/v1",
        )
        agent_reasoner_client = AsyncOpenAI(
            api_key=agent_settings['reasoner']['api_key'],
            base_url=agent_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
        )
        try:
            if request.stream:
                return await generate_stream_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url)
            return await generate_complete_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    
# æ·»åŠ çŠ¶æ€å­˜å‚¨
mcp_status = {}
@app.post("/create_mcp")
async def create_mcp_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    mcp_id = data.get("mcpId")
    
    if not mcp_id:
        raise HTTPException(status_code=400, detail="Missing mcpId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_mcp, mcp_id)
    
    return {"success": True, "message": "MCPæœåŠ¡å™¨åˆå§‹åŒ–å·²å¼€å§‹"}
@app.get("/mcp_status/{mcp_id}")
async def get_mcp_status(mcp_id: str):
    status = mcp_status.get(mcp_id, "not_found")
    return {"mcp_id": mcp_id, "status": status}
async def process_mcp(mcp_id: str):
    global mcp_client_list
    mcp_status[mcp_id] = "initializing"
    try:
        # è·å–å¯¹åº”æœåŠ¡å™¨çš„é…ç½®
        cur_settings = await load_settings()
        server_config = cur_settings['mcpServers'][mcp_id]
        
        # æ‰§è¡Œåˆå§‹åŒ–é€»è¾‘
        if mcp_id not in mcp_client_list:
            mcp_client_list[mcp_id] = McpClient()    
            await asyncio.wait_for(mcp_client_list[mcp_id].initialize(mcp_id, server_config), timeout=10)
        else:
            mcp_client_list[mcp_id].disabled = False
        mcp_status[mcp_id] = "ready"
        
    except Exception as e:
        mcp_client_list[mcp_id].disabled = True
        mcp_status[mcp_id] = f"failed: {str(e)}"
        # æ¸…ç†å¤±è´¥é…ç½®
        cur_settings['mcpServers'].pop(mcp_id, None)
        await save_settings(cur_settings)

@app.post("/api/remove_mcp")
async def remove_mcp_server(request: Request):
    global settings, mcp_client_list
    try:
        data = await request.json()
        server_name = data.get("serverName", "")

        if not server_name:
            raise HTTPException(status_code=400, detail="No server names provided")

        # ç§»é™¤æŒ‡å®šçš„MCPæœåŠ¡å™¨
        current_settings = await load_settings()
        if server_name in current_settings['mcpServers']:
            del current_settings['mcpServers'][server_name]
            await save_settings(current_settings)
            settings = current_settings

            # ä»mcp_client_listä¸­ç§»é™¤
            if server_name in mcp_client_list:
                mcp_client_list[server_name].disabled = True

            return JSONResponse({"success": True, "removed": server_name})
        else:
            raise HTTPException(status_code=404, detail="Server not found")
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON format")
    except Exception as e:
        logger.error(f"ç§»é™¤MCPæœåŠ¡å™¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/a2a/initialize")
async def initialize_a2a(request: Request):
    from python_a2a import A2AClient
    data = await request.json()
    try:
        client = A2AClient(data['url'])
        agent_card = client.agent_card.to_json()
        agent_card = json.loads(agent_card)
        return JSONResponse({
            **agent_card,
            "status": "ready",
            "enabled": True
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

# åœ¨ç°æœ‰è·¯ç”±ä¹‹åæ·»åŠ healthè·¯ç”±
@app.get("/health")
async def health_check():
    return {"status": "ok"}


@app.post("/load_file")
async def load_file_endpoint(request: Request, files: List[UploadFile] = File(None)):
    fastapi_base_url = str(request.base_url)
    logger.info(f"Received request with content type: {request.headers.get('Content-Type')}")
    file_links = []
    
    content_type = request.headers.get('Content-Type', '')
    
    try:
        if 'multipart/form-data' in content_type:
            # å¤„ç†æµè§ˆå™¨ä¸Šä¼ çš„æ–‡ä»¶
            if not files:
                raise HTTPException(status_code=400, detail="No files provided")
            
            for file in files:
                file_extension = os.path.splitext(file.filename)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                with open(destination, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file.filename
                }
                file_links.append(file_link)
        
        elif 'application/json' in content_type:
            # å¤„ç†Electronå‘é€çš„JSONæ–‡ä»¶è·¯å¾„
            data = await request.json()
            logger.info(f"Processing JSON data: {data}")
            
            for file_info in data.get("files", []):
                file_path = file_info.get("path")
                file_name = file_info.get("name", os.path.basename(file_path))
                
                if not os.path.isfile(file_path):
                    logger.error(f"File not found: {file_path}")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                file_extension = os.path.splitext(file_name)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•
                with open(file_path, "rb") as src, open(destination, "wb") as dst:
                    dst.write(src.read())
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file_name
                }
                file_links.append(file_link)
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported Content-Type")
        
        return JSONResponse(content={"success": True, "fileLinks": file_links})
    
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create_kb")
async def create_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")
    
    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_kb, kb_id)
    
    return {"success": True, "message": "çŸ¥è¯†åº“å¤„ç†å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢çŠ¶æ€"}

# æ·»åŠ çŠ¶æ€å­˜å‚¨
kb_status = {}
@app.get("/kb_status/{kb_id}")
async def get_kb_status(kb_id: int):
    status = kb_status.get(kb_id, "not_found")
    return {"kb_id": kb_id, "status": status}

# ä¿®æ”¹ process_kb
async def process_kb(kb_id: int):
    kb_status[kb_id] = "processing"
    try:
        from py.know_base import process_knowledge_base
        await process_knowledge_base(kb_id)
        kb_status[kb_id] = "completed"
    except Exception as e:
        kb_status[kb_id] = f"failed: {str(e)}"


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    current_settings = await load_settings()
    await websocket.send_json({"type": "settings", "data": current_settings})
    
    try:
        while True:
            data = await websocket.receive_json()
            if data.get("type") == "save_settings":
                await save_settings(data.get("data", {}))
                await websocket.send_json({"type": "settings_saved", "success": True})
            elif data.get("type") == "get_settings":
                settings = await load_settings()
                await websocket.send_json({"type": "settings", "data": settings})
            elif data.get("type") == "save_agent":
                current_settings = await load_settings()
                
                # ç”Ÿæˆæ™ºèƒ½ä½“IDå’Œé…ç½®è·¯å¾„
                agent_id = str(shortuuid.ShortUUID().random(length=8))
                config_path = os.path.join(AGENT_DIR, f"{agent_id}.json")
                
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(current_settings, f, indent=4, ensure_ascii=False)
                
                # æ›´æ–°ä¸»é…ç½®
                current_settings['agents'][agent_id] = {
                    "id": agent_id,
                    "name": data['data']['name'],
                    "system_prompt": data['data']['system_prompt'],
                    "config_path": config_path,
                    "enabled": False,
                }
                await save_settings(current_settings)
                
                # å¹¿æ’­æ›´æ–°åçš„é…ç½®
                await websocket.send_json({
                    "type": "settings",
                    "data": current_settings
                })
    except Exception as e:
        print(f"WebSocket error: {e}")

app.mount("/uploaded_files", StaticFiles(directory=UPLOAD_FILES_DIR), name="uploaded_files")
app.mount("/node_modules", StaticFiles(directory=os.path.join(base_path, "node_modules")), name="node_modules")
app.mount("/", StaticFiles(directory=os.path.join(base_path, "static"), html=True), name="static")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=HOST, port=PORT)
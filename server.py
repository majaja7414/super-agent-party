# -- coding: utf-8 --
import asyncio
import copy
import datetime
from functools import partial
import json
import multiprocessing
from multiprocessing import Manager, Event
import os
import re
import shutil
import signal
from fastapi import BackgroundTasks, FastAPI, File, HTTPException, UploadFile, WebSocket, Request
from fastapi_mcp import FastApiMCP
import logging
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
from pydantic import BaseModel
from fastapi import status
from fastapi.responses import JSONResponse, StreamingResponse
import uuid
import time
from typing import List, Dict,Optional
import shortuuid
from py.mcp_clients import McpClient
from contextlib import asynccontextmanager,suppress
import requests
import asyncio
from concurrent.futures import ThreadPoolExecutor
import botpy
from botpy.message import C2CMessage,GroupMessage
import argparse
from mem0 import Memory
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)
parser = argparse.ArgumentParser(description="Run the ASGI application server.")
parser.add_argument("--host", default="127.0.0.1", help="Host for the ASGI server, default is 127.0.0.1")
parser.add_argument("--port", type=int, default=3456, help="Port for the ASGI server, default is 3456")
args = parser.parse_args()
HOST = args.host
PORT = args.port

os.environ["no_proxy"] = "localhost,127.0.0.1"
local_timezone = None
settings = None
client = None
reasoner_client = None
mcp_client_list = {}
locales = {}
_TOOL_HOOKS = {}

ALLOWED_EXTENSIONS = [
  # åŠå…¬æ–‡æ¡£
  'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'pdf', 'pages', 
  'numbers', 'key', 'rtf', 'odt',
  
  # ç¼–ç¨‹å¼€å‘
  'js', 'ts', 'py', 'java', 'c', 'cpp', 'h', 'hpp', 'go', 'rs',
  'swift', 'kt', 'dart', 'rb', 'php', 'html', 'css', 'scss', 'less',
  'vue', 'svelte', 'jsx', 'tsx', 'json', 'xml', 'yml', 'yaml', 
  'sql', 'sh',
  
  # æ•°æ®é…ç½®
  'csv', 'tsv', 'txt', 'md', 'log', 'conf', 'ini', 'env', 'toml'
]
ALLOWED_IMAGE_EXTENSIONS = ['png', 'jpg', 'jpeg', 'gif', 'webp', 'bmp']

from py.get_setting import load_settings,save_settings,base_path,configure_host_port,UPLOAD_FILES_DIR,AGENT_DIR,MEMORY_CACHE_DIR,KB_DIR
from py.llm_tool import get_image_base64,get_image_media_type


configure_host_port(args.host, args.port)

@asynccontextmanager
async def lifespan(app: FastAPI): 
    from py.get_setting import init_db
    await init_db()
    global settings, client, reasoner_client, mcp_client_list,local_timezone,logger,locales
    with open(base_path + "/config/locales.json", "r", encoding="utf-8") as f:
        locales = json.load(f)
    from tzlocal import get_localzone
    local_timezone = get_localzone()
    settings = await load_settings()
    if settings:
        client = AsyncOpenAI(api_key=settings['api_key'], base_url=settings['base_url'])
        reasoner_client = AsyncOpenAI(api_key=settings['reasoner']['api_key'],base_url=settings['reasoner']['base_url'])
    else:
        client = AsyncOpenAI()
        reasoner_client = AsyncOpenAI()
    mcp_init_tasks = []
    async def init_mcp_with_timeout(server_name, server_config):
        """å¸¦è¶…æ—¶å¤„ç†çš„å¼‚æ­¥åˆå§‹åŒ–å‡½æ•°"""
        try:
            mcp_client = McpClient()
            if not server_config['disabled']:
                await asyncio.wait_for(
                    mcp_client.initialize(server_name, server_config),
                    timeout=6
                )
            return server_name, mcp_client, None
        except asyncio.TimeoutError:
            logger.error(f"MCP client {server_name} initialization timeout")
            return server_name, None, "timeout"
        except Exception as e:
            logger.error(f"MCP client {server_name} initialization failed: {str(e)}")
            return server_name, None, "error"
    if settings:
        # åˆ›å»ºæ‰€æœ‰åˆå§‹åŒ–ä»»åŠ¡
        for server_name, server_config in settings['mcpServers'].items():
            task = asyncio.create_task(init_mcp_with_timeout(server_name, server_config))
            mcp_init_tasks.append(task)
        # ç«‹å³ç»§ç»­æ‰§è¡Œä¸ç­‰å¾…
        # é€šè¿‡å›è°ƒå¤„ç†ç»“æœ
        async def check_results():
            """åå°æ”¶é›†ä»»åŠ¡ç»“æœ"""
            for task in asyncio.as_completed(mcp_init_tasks):
                server_name, mcp_client, error = await task
                if error:
                    settings['mcpServers'][server_name]['disabled'] = True
                    settings['mcpServers'][server_name]['processingStatus'] = 'server_error'
                    mcp_client_list[server_name] = McpClient()
                    mcp_client_list[server_name].disabled = True
                else:
                    mcp_client_list[server_name] = mcp_client
            await save_settings(settings)  # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€ä¿å­˜
            await broadcast_settings_update(settings)  # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€å¹¿æ’­
        # åœ¨åå°è¿è¡Œç»“æœæ”¶é›†
        asyncio.create_task(check_results())
    yield
# WebSocketç«¯ç‚¹å¢åŠ è¿æ¥ç®¡ç†
active_connections = []
# æ–°å¢å¹¿æ’­å‡½æ•°
async def broadcast_settings_update(settings):
    """å‘æ‰€æœ‰WebSocketè¿æ¥æ¨é€é…ç½®æ›´æ–°"""
    for connection in active_connections:  # éœ€è¦ç»´æŠ¤å…¨å±€è¿æ¥åˆ—è¡¨
        try:
            await connection.send_json({
                "type": "settings",
                "data": settings  # ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„æœ€æ–°é…ç½®
            })
        except Exception as e:
            logger.error(f"Broadcast failed: {e}")

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def t(text: str) -> str:
    global locales
    settings = await load_settings()
    target_language = settings["systemSettings"]["language"]
    return locales[target_language].get(text, text)

async def get_image_content(image_url: str) -> str:
    import hashlib
    settings = await load_settings()
    base64_image = await get_image_base64(image_url)
    media_type = await get_image_media_type(image_url)
    url= f"data:{media_type};base64,{base64_image}"
    image_hash = hashlib.md5(image_url.encode()).hexdigest()
    content = ""
    if settings['vision']['enabled']:
        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt")):
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "r", encoding='utf-8') as f:
                content += f"\n\nå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
        else:
            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": url}}]
            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
            response = await client.chat.completions.create(
                model=settings['vision']['model'],
                messages = [{"role": "user", "content": images_content}],
                temperature=settings['vision']['temperature'],
            )
            content = f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "w", encoding='utf-8') as f:
                f.write(str(response.choices[0].message.content))
    else:           
        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt")):
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "r", encoding='utf-8') as f:
                content += f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
        else:
            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": url}}]
            client = AsyncOpenAI(api_key=settings['api_key'],base_url=settings['base_url'])
            response = await client.chat.completions.create(
                model=settings['model'],
                messages = [{"role": "user", "content": images_content}],
                temperature=settings['temperature'],
            )
            content = f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "w", encoding='utf-8') as f:
                f.write(str(response.choices[0].message.content))
    return content

async def dispatch_tool(tool_name: str, tool_params: dict,settings: dict) -> str:
    global mcp_client_list,_TOOL_HOOKS
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        jina_crawler_async,
        Crawl4Ai_search_async, 
    )
    from py.know_base import query_knowledge_base
    from py.agent_tool import agent_tool_call
    from py.a2a_tool import a2a_tool_call
    from py.llm_tool import custom_llm_tool
    from py.pollinations import pollinations_image
    from py.load_files import get_file_content
    from py.code_interpreter import e2b_code_async,local_run_code_async
    from py.custom_http import fetch_custom_http
    _TOOL_HOOKS = {
        "DDGsearch_async": DDGsearch_async,
        "searxng_async": searxng_async,
        "Tavily_search_async": Tavily_search_async,
        "query_knowledge_base": query_knowledge_base,
        "jina_crawler_async": jina_crawler_async,
        "Crawl4Ai_search_async": Crawl4Ai_search_async,
        "agent_tool_call": agent_tool_call,
        "a2a_tool_call": a2a_tool_call,
        "custom_llm_tool": custom_llm_tool,
        "pollinations_image":pollinations_image,
        "get_file_content":get_file_content,
        "get_image_content": get_image_content,
        "e2b_code_async": e2b_code_async,
        "local_run_code_async": local_run_code_async
    }
    if "multi_tool_use." in tool_name:
        tool_name = tool_name.replace("multi_tool_use.", "")
    if "custom_http_" in tool_name:
        tool_name = tool_name.replace("custom_http_", "")
        print(tool_name)
        settings_custom_http = settings['custom_http']
        for custom in settings_custom_http:
            if custom['name'] == tool_name:
                tool_custom_http = custom
                break
        method = tool_custom_http['method']
        url = tool_custom_http['url']
        headers = tool_custom_http['headers']
        result = await fetch_custom_http(method, url, headers, tool_params)
        return str(result)
    if tool_name not in _TOOL_HOOKS:
        for server_name, mcp_client in mcp_client_list.items():
            if tool_name in mcp_client.tools_list:
                result = await mcp_client.call_tool(tool_name, tool_params)
                return str(result.model_dump())
        return None
    tool_call = _TOOL_HOOKS[tool_name]
    try:
        ret_out = await tool_call(**tool_params)
        return ret_out
    except Exception as e:
        logger.error(f"Error calling tool {tool_name}: {e}")
        return f"Error calling tool {tool_name}: {e}"


class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str = None
    temperature: float = 0.7
    tools: dict = None
    stream: bool = False
    max_tokens: int = None
    top_p: float = 1
    frequency_penalty: float = 0
    presence_penalty: float = 0
    fileLinks: List[str] = None
    enable_thinking: bool = False
    enable_deep_research: bool = False
    enable_web_search: bool = False

async def message_without_images(messages: List[Dict]) -> List[Dict]:
    if messages:
        for message in messages:
            if 'content' in message:
                # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
                if isinstance(message['content'], list):
                    for item in message['content']:
                        if isinstance(item, dict) and item['type'] == 'text':
                            message['content'] = item['text']
                            break
    return messages

async def images_in_messages(messages: List[Dict],fastapi_base_url: str) -> List[Dict]:
    import hashlib
    images = []
    index = 0
    for message in messages:
        image_urls = []
        if 'content' in message:
            # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
            if isinstance(message['content'], list):
                for item in message['content']:
                    if isinstance(item, dict) and item['type'] == 'image_url':
                        # å¦‚æœitem["image_url"]["url"]æ˜¯httpæˆ–httpså¼€å¤´ï¼Œåˆ™è½¬æ¢æˆbase64
                        if item["image_url"]["url"].startswith("http"):
                            image_url = item["image_url"]["url"]
                            # å¯¹image_urlåˆ†è§£å‡ºbaseURLï¼Œä¸fastapi_base_urlæ¯”è¾ƒï¼Œå¦‚æœç›¸åŒï¼Œå°†image_urlçš„baseURLæ›¿æ¢æˆ127.0.0.1:PORT
                            if fastapi_base_url in image_url:
                                image_url = image_url.replace(fastapi_base_url, f"http://127.0.0.1:{PORT}/")
                            base64_image = await get_image_base64(image_url)
                            media_type = await get_image_media_type(image_url)
                            item["image_url"]["url"] = f"data:{media_type};base64,{base64_image}"
                            item["image_url"]["hash"] = hashlib.md5(item["image_url"]["url"].encode()).hexdigest()
                        image_urls.append(item)
        if image_urls:
            images.append({'index': index, 'images': image_urls})
        index += 1
    return images

async def images_add_in_messages(request_messages: List[Dict], images: List[Dict], settings: dict) -> List[Dict]:
    messages=copy.deepcopy(request_messages)
    if settings['vision']['enabled']:
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": item['image_url']['url']}}]
                            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
                            response = await client.chat.completions.create(
                                model=settings['vision']['model'],
                                messages = [{"role": "user", "content": images_content}],
                                temperature=settings['vision']['temperature'],
                            )
                            messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "w", encoding='utf-8') as f:
                                f.write(str(response.choices[0].message.content))
    else:           
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            messages[index]['content'] = [{"type": "text", "text": messages[index]['content']}]
                            messages[index]['content'].append({"type": "image_url", "image_url": {"url": item['image_url']['url']}})
    return messages

async def tools_change_messages(request: ChatRequest, settings: dict):
    if settings['tools']['time']['enabled']:
        time_message = f"æ¶ˆæ¯å‘é€æ—¶é—´ï¼š{local_timezone}  {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n\n"
        request.messages[-1]['content'] = time_message + request.messages[-1]['content']
    if settings['tools']['inference']['enabled']:
        inference_message = "å›ç­”ç”¨æˆ·å‰è¯·å…ˆæ€è€ƒæ¨ç†ï¼Œå†å›ç­”é—®é¢˜ï¼Œä½ çš„æ€è€ƒæ¨ç†çš„è¿‡ç¨‹å¿…é¡»æ”¾åœ¨<think>ä¸</think>ä¹‹é—´ã€‚\n\n"
        request.messages[-1]['content'] = f"{inference_message}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
    if settings['tools']['formula']['enabled']:
        latex_message = "\n\nå½“ä½ æƒ³ä½¿ç”¨latexå…¬å¼æ—¶ï¼Œä½ å¿…é¡»æ˜¯ç”¨ ['$', '$'] ä½œä¸ºè¡Œå†…å…¬å¼å®šç•Œç¬¦ï¼Œä»¥åŠ ['$$', '$$'] ä½œä¸ºè¡Œé—´å…¬å¼å®šç•Œç¬¦ã€‚\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += latex_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': latex_message})
    if settings['tools']['language']['enabled']:
        language_message = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€æ¨ç†åˆ†ææ€è€ƒï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨ç†åˆ†æï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += language_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': language_message})
    return request

def get_drs_stage(DRS_STAGE):
    if DRS_STAGE == 1:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºæ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µï¼Œä½ éœ€è¦åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶ç»™å‡ºæ˜ç¡®çš„éœ€æ±‚æè¿°ã€‚å¦‚æœç”¨æˆ·çš„éœ€æ±‚æè¿°ä¸æ˜ç¡®ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
    elif DRS_STAGE == 2:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºæŸ¥è¯¢æœç´¢é˜¶æ®µï¼Œåˆ©ç”¨ä½ çš„çŸ¥è¯†åº“ã€äº’è”ç½‘æœç´¢ã€æ•°æ®åº“æŸ¥è¯¢å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼Œè¿™äº›å·¥å…·ä¸ä¸€å®šä¼šæä¾›ï¼‰ï¼ŒæŸ¥è¯¢å®Œæˆä»»åŠ¡æ‰€éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ã€‚"
    elif DRS_STAGE == 3:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºç”Ÿæˆç»“æœé˜¶æ®µï¼Œæ ¹æ®å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®Œæˆä»»åŠ¡ï¼Œç”Ÿæˆå›ç­”ã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä½ ç”Ÿæˆä¸€ä¸ªè¶…è¿‡2000å­—çš„å›ç­”ï¼Œä½ å¯ä»¥å°è¯•å°†è¯¥ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯æ¬¡åªå®Œæˆå…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚"
    else:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºç”Ÿæˆç»“æœé˜¶æ®µï¼Œæ ¹æ®å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®Œæˆä»»åŠ¡ï¼Œç”Ÿæˆå›ç­”ã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä½ ç”Ÿæˆä¸€ä¸ªè¶…è¿‡2000å­—çš„å›ç­”ï¼Œä½ å¯ä»¥å°è¯•å°†è¯¥ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯æ¬¡åªå®Œæˆå…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚"
    return drs_msg  

def get_drs_stage_name(DRS_STAGE):
    if DRS_STAGE == 1:
        drs_stage_name = "æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ"
    elif DRS_STAGE == 2:
        drs_stage_name = "æŸ¥è¯¢æœç´¢é˜¶æ®µ"
    elif DRS_STAGE == 3:
        drs_stage_name = "ç”Ÿæˆç»“æœé˜¶æ®µ"
    else:
        drs_stage_name = "ç”Ÿæˆç»“æœé˜¶æ®µ"
    return drs_stage_name

def get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content):
    drs_stage_name = get_drs_stage_name(DRS_STAGE)
    if DRS_STAGE == 1:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

### å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}

### å¦‚æœä¸éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®éœ€æ±‚ï¼Œè¿›å…¥å¹¶è¿›å…¥æŸ¥è¯¢æœç´¢é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "search",
    "unfinished_task": ""
}}
"""
    elif DRS_STAGE == 2:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

### å¦‚æœéœ€è¦ç»§ç»­æŸ¥è¯¢ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_search",
    "unfinished_task": "è¿™é‡Œå¡«å…¥ç»§ç»­æŸ¥è¯¢çš„ä¿¡æ¯"
}}

### å¦‚æœä¸éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®éœ€æ±‚ï¼Œè¿›å…¥å¹¶è¿›å…¥æŸ¥è¯¢æœç´¢é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "answer",
    "unfinished_task": ""
}}
"""    
    else:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

å¦‚æœåˆå§‹ä»»åŠ¡å·²å®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœåˆå§‹ä»»åŠ¡æœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}
"""    
    return search_prompt

async def generate_stream_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search):
    global mcp_client_list
    DRS_STAGE = 1 # 1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ
    images = await images_in_messages(request.messages,fastapi_base_url)
    request.messages = await message_without_images(request.messages)
    from py.load_files import get_files_content,file_tool,image_tool
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base,rerank_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool
    from py.code_interpreter import e2b_code_tool,local_run_code_tool
    m0 = None
    if settings["memorySettings"]["is_memory"]:
        memoryId = settings["memorySettings"]["selectedMemory"]
        cur_memory = None
        for memory in settings["memories"]:
            if memory["id"] == memoryId:
                cur_memory = memory
                break
        if cur_memory:
            
            config={
                "embedder": {
                    "provider": 'openai',
                    "config": {
                        "model": cur_memory['model'],
                        "api_key": cur_memory['api_key'],
                        "openai_base_url":cur_memory["base_url"],
                        "embedding_dims":1024
                    },
                },
                "llm": {
                    "provider": 'openai',
                    "config": {
                        "model": settings['model'],
                        "api_key": settings['api_key'],
                        "openai_base_url":settings["base_url"]
                    }
                },
                "vector_store": {
                    "provider": "faiss",
                    "config": {
                        "collection_name": "agent-party",
                        "path": os.path.join(MEMORY_CACHE_DIR,memoryId),
                        "distance_strategy": "euclidean",
                        "embedding_model_dims": 1024
                    }
                }
            }
            m0 = Memory.from_config(config)
    open_tag = "<think>"
    close_tag = "</think>"
    try:
        tools = request.tools or []
        if mcp_client_list:
            for server_name, mcp_client in mcp_client_list.items():
                if server_name in settings['mcpServers']:
                    if 'disabled' not in settings['mcpServers'][server_name]:
                        settings['mcpServers'][server_name]['disabled'] = False
                    if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                        function = await mcp_client.get_openai_functions()
                        if function:
                            tools.extend(function)
        get_llm_tool_fuction = await get_llm_tool(settings)
        if get_llm_tool_fuction:
            tools.append(get_llm_tool_fuction)
        get_agent_tool_fuction = await get_agent_tool(settings)
        if get_agent_tool_fuction:
            tools.append(get_agent_tool_fuction)
        get_a2a_tool_fuction = await get_a2a_tool(settings)
        if get_a2a_tool_fuction:
            tools.append(get_a2a_tool_fuction)
        if settings['tools']['pollinations']['enabled']:
            tools.append(pollinations_image_tool)
        if settings['tools']['getFile']['enabled']:
            tools.append(file_tool)
            tools.append(image_tool)
        if settings["codeSettings"]['enabled']:
            if settings["codeSettings"]["engine"] == "e2b":
                tools.append(e2b_code_tool)
            elif settings["codeSettings"]["engine"] == "sandbox":
                tools.append(local_run_code_tool)
        if settings["custom_http"]:
            for custom_http in settings["custom_http"]:
                if custom_http["enabled"]:
                    if custom_http['body'] == "":
                        custom_http['body'] = "{}"
                    custom_http_tool = {
                        "type": "function",
                        "function": {
                            "name": f"custom_http_{custom_http['name']}",
                            "description": f"{custom_http['description']}",
                            "parameters": json.loads(custom_http['body']),
                        },
                    }
                    tools.append(custom_http_tool)
        print(tools)
        source_prompt = ""
        if request.fileLinks:
            print("fileLinks",request.fileLinks)
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            fileLinks_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += fileLinks_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': fileLinks_message})
            source_prompt += fileLinks_message
        user_prompt = request.messages[-1]['content']
        if m0:
            lore_content = ""
            assistant_reply = ""
            # æ‰¾å‡ºrequest.messagesä¸­ä¸Šæ¬¡çš„assistantå›å¤
            for i in range(len(request.messages)-1, -1, -1):
                if request.messages[i]['role'] == 'assistant':
                    assistant_reply = request.messages[i]['content']
                    break
            if cur_memory["lorebook"]:
                for lore in cur_memory["lorebook"]:
                    if lore["name"] != "" and (lore["name"] in user_prompt or lore["name"] in assistant_reply):
                        lore_content = lore_content + "\n\n" + f"{lore['name']}ï¼š{lore['value']}"
            memoryLimit = settings["memorySettings"]["memoryLimit"]
            try:
                relevant_memories = m0.search(query=user_prompt, user_id=memoryId, limit=memoryLimit)
                relevant_memories = json.dumps(relevant_memories, ensure_ascii=False)
            except Exception as e:
                print("m0.search error:",e)
                relevant_memories = ""
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"
            else:
                request.messages.insert(0, {'role': 'system', 'content': "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"})
            if cur_memory["basic_character"]:
                print("æ·»åŠ è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"})
            if lore_content:
                print("æ·»åŠ ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"})
        request = await tools_change_messages(request, settings)
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        async def stream_generator(user_prompt,DRS_STAGE):
            kb_list = []
            if settings["knowledgeBases"]:
                for kb in settings["knowledgeBases"]:
                    if kb["enabled"] and kb["processingStatus"] == "completed":
                        kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
            if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
                if kb_list:
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": f"{await t("KB_search")}\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    all_kb_content = []
                    # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                    for kb in kb_list:
                        kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                        all_kb_content.extend(kb_content)
                        if settings["KBSettings"]["is_rerank"]:
                            all_kb_content = await rerank_knowledge_base(user_prompt,all_kb_content)
                    if all_kb_content:
                        all_kb_content = json.dumps(all_kb_content, ensure_ascii=False, indent=4)
                        kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                        request.messages[-1]['content'] += f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}"
                                                # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥UPLOAD_FILES_DIRæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                            f.write(str(all_kb_content))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[{await t("search_result")}]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
            if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
                if kb_list:
                    kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                    print(kb_list_message)
                    if request.messages and request.messages[0]['role'] == 'system':
                        request.messages[0]['content'] += kb_list_message
                    else:
                        request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
            else:
                kb_list = []
            if settings['webSearch']['enabled'] or enable_web_search:
                if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": f"{await t("web_search")}\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        results = await DDGsearch_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'searxng':
                        results = await searxng_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'tavily':
                        results = await Tavily_search_async(user_prompt)
                    if results:
                        request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}\n\nè¯·æ ¹æ®è”ç½‘æœç´¢ç»“æœç»„ç»‡ä½ çš„å›ç­”ï¼Œå¹¶ç¡®ä¿ä½ çš„å›ç­”æ˜¯å‡†ç¡®çš„ã€‚"
                        # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                            f.write(str(results))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[{await t("search_result")}]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
                if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        tools.append(duckduckgo_tool)
                    elif settings['webSearch']['engine'] == 'searxng':
                        tools.append(searxng_tool)
                    elif settings['webSearch']['engine'] == 'tavily':
                        tools.append(tavily_tool)
                    if settings['webSearch']['crawler'] == 'jina':
                        tools.append(jina_crawler_tool)
                    elif settings['webSearch']['crawler'] == 'crawl4ai':
                        tools.append(Crawl4Ai_tool)
            if kb_list:
                tools.append(kb_tool)
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                deepsearch_messages = copy.deepcopy(request.messages)
                deepsearch_messages[-1]['content'] += "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚"
                print(request.messages[-1]['content'])
                response = await client.chat.completions.create(
                    model=model,
                    messages=deepsearch_messages,
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                user_prompt = response.choices[0].message.content
                deepsearch_chunk = {
                    "choices": [{
                        "delta": {
                            "reasoning_content": f"\n\nğŸ’–{await t("start_task")}{user_prompt}\n\n",
                        }
                    }]
                }
                yield f"data: {json.dumps(deepsearch_chunk)}\n\n"
                request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
                print(request.messages[-1]['content'])
            # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
            if settings['reasoner']['enabled'] or enable_thinking:
                reasoner_messages = copy.deepcopy(request.messages)
                if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    reasoner_messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
                    drs_msg = get_drs_stage(DRS_STAGE)
                    if drs_msg:
                        reasoner_messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=True,
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                    in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                    
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue
                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            current_content = delta.get("content", "")
                            buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                            
                            # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                            while True:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                        non_reasoning = buffer[:start_pos]
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                else:
                                    # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                        reasoning_part = buffer[:end_pos]
                                        chunk_dict["choices"][0]["delta"] = {
                                            "reasoning_content": reasoning_part,
                                            "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                        }
                                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                                        full_reasoning += reasoning_part
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                        if buffer:
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": buffer,
                                                "content": ""
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += buffer
                                            buffer = ""
                                        break  # ç­‰å¾…æ›´å¤šå†…å®¹
                else:
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=True,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue

                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            reasoning_content = delta.get("reasoning_content", "")
                            if reasoning_content:
                                full_reasoning += reasoning_content
                        yield f"data: {json.dumps(chunk_dict)}\n\n"

                # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
            # çŠ¶æ€è·Ÿè¸ªå˜é‡
            in_reasoning = False
            reasoning_buffer = []
            content_buffer = []
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
                drs_msg = get_drs_stage(DRS_STAGE)
                if drs_msg:
                    request.messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            tool_calls = []
            full_content = ""
            search_not_done = False
            search_task = ""
            async for chunk in response:
                if not chunk.choices:
                    continue
                choice = chunk.choices[0]
                if choice.delta.tool_calls:  # function_calling
                    for idx, tool_call in enumerate(choice.delta.tool_calls):
                        tool = choice.delta.tool_calls[idx]
                        if len(tool_calls) <= idx:
                            tool_calls.append(tool)
                            continue
                        if tool.function.arguments:
                            # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                            tool_calls[idx].function.arguments += tool.function.arguments
                else:
                    # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                    chunk_dict = chunk.model_dump()
                    delta = chunk_dict["choices"][0]["delta"]
                    
                    # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                    delta.setdefault("content", "")
                    delta.setdefault("reasoning_content", "")
                    
                    # ä¼˜å…ˆå¤„ç† reasoning_content
                    if delta["reasoning_content"]:
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        continue

                    # å¤„ç†å†…å®¹
                    current_content = delta["content"]
                    buffer = current_content
                    
                    while buffer:
                        if not in_reasoning:
                            # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                            start_pos = buffer.find(open_tag)
                            if start_pos != -1:
                                # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                content_buffer.append(buffer[:start_pos])
                                buffer = buffer[start_pos+len(open_tag):]
                                in_reasoning = True
                            else:
                                content_buffer.append(buffer)
                                buffer = ""
                        else:
                            # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                            end_pos = buffer.find(close_tag)
                            if end_pos != -1:
                                # å¤„ç†æ€è€ƒå†…å®¹
                                reasoning_buffer.append(buffer[:end_pos])
                                buffer = buffer[end_pos+len(close_tag):]
                                in_reasoning = False
                            else:
                                reasoning_buffer.append(buffer)
                                buffer = ""
                    
                    # æ„é€ æ–°çš„deltaå†…å®¹
                    new_content = "".join(content_buffer)
                    new_reasoning = "".join(reasoning_buffer)
                    
                    # æ›´æ–°chunkå†…å®¹
                    delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                    delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                    
                    # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                    if in_reasoning:
                        content_buffer = [new_content.split(open_tag)[-1]] 
                    else:
                        content_buffer = []
                    reasoning_buffer = []
                    
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    full_content += delta.get("content", "")
            # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
            if content_buffer or reasoning_buffer:
                final_chunk = {
                    "choices": [{
                        "delta": {
                            "content": "".join(content_buffer),
                            "reasoning_content": "".join(reasoning_buffer)
                        }
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                full_content += final_chunk["choices"][0]["delta"].get("content", "")
            if tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content)
                response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "system",
                        "content": source_prompt,
                        },
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                try:
                    response_content = json.loads(response_content)
                except json.JSONDecodeError:
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâŒ{await t("task_error")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                if response_content["status"] == "done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâœ…{await t("task_done")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ{await t("task_not_done")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    DRS_STAGE = 2
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ“{await t("task_need_more_info")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
                elif response_content["status"] == "search":
                    DRS_STAGE = 2
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nğŸ”{await t("enter_search_stage")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                elif response_content["status"] == "need_more_search":
                    DRS_STAGE = 2
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nğŸ”{await t("need_more_search")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "answer":
                    DRS_STAGE = 3
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ­{await t("enter_answer_stage")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                print("DRS_STAGE:", DRS_STAGE)
            reasoner_messages = copy.deepcopy(request.messages)
            while tool_calls or search_not_done:
                full_content = ""
                if tool_calls:
                    response_content = tool_calls[0].function
                    if response_content.name in  ["DDGsearch_async","searxng_async", "Tavily_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("web_search")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in  ["jina_crawler_async","Crawl4Ai_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("web_search_more")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in ["query_knowledge_base"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("knowledge_base")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    else:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("call")}{response_content.name}{await t("tool")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    print(response_content.arguments)
                    modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                    print(modified_data)
                    # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                    data_list = json.loads(modified_data)
                    results = await dispatch_tool(response_content.name, data_list[0],settings)
                    if results is None:
                        chunk = {
                            "id": "extra_tools",
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "tool_calls":modified_data,
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        break
                    if response_content.name in ["query_knowledge_base"]:
                        if settings["KBSettings"]["is_rerank"]:
                            results = await rerank_knowledge_base(user_prompt,results)
                        results = json.dumps(results, ensure_ascii=False, indent=4)
                    request.messages.append(
                        {
                            "tool_calls": [
                                {
                                    "id": tool_calls[0].id,
                                    "function": {
                                        "arguments": json.dumps(data_list[0]),
                                        "name": response_content.name,
                                    },
                                    "type": tool_calls[0].type,
                                }
                            ],
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    request.messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_calls[0].id,
                            "name": response_content.name,
                            "content": str(results),
                        }
                    )
                    if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                        request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
                    reasoner_messages.append(
                        {
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    reasoner_messages.append(
                        {
                            "role": "user",
                            "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                        }
                    )
                    # è·å–æ—¶é—´æˆ³å’Œuuid
                    timestamp = time.time()
                    uid = str(uuid.uuid4())
                    # æ„é€ æ–‡ä»¶å
                    filename = f"{timestamp}_{uid}.txt"
                    # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                    with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                        f.write(str(results))            
                    # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                    fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                    tool_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\n[{response_content.name}{await t("tool_result")}]({fileLink})\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(tool_chunk)}\n\n"
                # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
                if settings['reasoner']['enabled'] or enable_thinking:
                    if tools:
                        reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                    for modelProvider in settings['modelProviders']: 
                        if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                            vendor = modelProvider['vendor']
                            break
                    msg = await images_add_in_messages(reasoner_messages, images,settings)
                    if vendor == 'Ollama':
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            temperature=settings['reasoner']['temperature']
                        )
                        full_reasoning = ""
                        buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                        in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                        
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                current_content = delta.get("content", "")
                                buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                                
                                # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                                while True:
                                    if not in_reasoning:
                                        # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                        start_pos = buffer.find(open_tag)
                                        if start_pos != -1:
                                            # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                            non_reasoning = buffer[:start_pos]
                                            buffer = buffer[start_pos+len(open_tag):]
                                            in_reasoning = True
                                        else:
                                            break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                    else:
                                        # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                        end_pos = buffer.find(close_tag)
                                        if end_pos != -1:
                                            # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                            reasoning_part = buffer[:end_pos]
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": reasoning_part,
                                                "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += reasoning_part
                                            buffer = buffer[end_pos+len(close_tag):]
                                            in_reasoning = False
                                        else:
                                            # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                            if buffer:
                                                chunk_dict["choices"][0]["delta"] = {
                                                    "reasoning_content": buffer,
                                                    "content": ""
                                                }
                                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                                full_reasoning += buffer
                                                buffer = ""
                                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                    else:
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                            temperature=settings['reasoner']['temperature']
                        )
                        full_reasoning = ""
                        # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue

                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                reasoning_content = delta.get("reasoning_content", "")
                                if reasoning_content:
                                    full_reasoning += reasoning_content
                            yield f"data: {json.dumps(chunk_dict)}\n\n"

                    # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                    request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
                msg = await images_add_in_messages(request.messages, images,settings)
                if tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        tools=tools,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p or settings['top_p'],
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p or settings['top_p'],
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                tool_calls = []
                async for chunk in response:
                    if not chunk.choices:
                        continue
                    if chunk.choices:
                        choice = chunk.choices[0]
                        if choice.delta.tool_calls:  # function_calling
                            for idx, tool_call in enumerate(choice.delta.tool_calls):
                                tool = choice.delta.tool_calls[idx]
                                if len(tool_calls) <= idx:
                                    tool_calls.append(tool)
                                    continue
                                if tool.function.arguments:
                                    # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                                    tool_calls[idx].function.arguments += tool.function.arguments
                        else:
                            # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0]["delta"]
                            
                            # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                            delta.setdefault("content", "")
                            delta.setdefault("reasoning_content", "")

                             # ä¼˜å…ˆå¤„ç† reasoning_content
                            if delta["reasoning_content"]:
                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                continue
                            
                            # å¤„ç†å†…å®¹
                            current_content = delta["content"]
                            buffer = current_content
                            
                            while buffer:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                        content_buffer.append(buffer[:start_pos])
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        content_buffer.append(buffer)
                                        buffer = ""
                                else:
                                    # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # å¤„ç†æ€è€ƒå†…å®¹
                                        reasoning_buffer.append(buffer[:end_pos])
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        reasoning_buffer.append(buffer)
                                        buffer = ""
                            
                            # æ„é€ æ–°çš„deltaå†…å®¹
                            new_content = "".join(content_buffer)
                            new_reasoning = "".join(reasoning_buffer)
                            
                            # æ›´æ–°chunkå†…å®¹
                            delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                            delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                            
                            # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                            if in_reasoning:
                                content_buffer = [new_content.split(open_tag)[-1]] 
                            else:
                                content_buffer = []
                            reasoning_buffer = []
                            
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            full_content += delta.get("content", "")
                # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
                if content_buffer or reasoning_buffer:
                    final_chunk = {
                        "choices": [{
                            "delta": {
                                "content": "".join(content_buffer),
                                "reasoning_content": "".join(reasoning_buffer)
                            }
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                    full_content += final_chunk["choices"][0]["delta"].get("content", "")
                if tool_calls:
                    pass
                elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content)
                    response = await client.chat.completions.create(
                        model=model,
                        messages=[                        
                            {
                            "role": "system",
                            "content": source_prompt,
                            },
                            {
                            "role": "user",
                            "content": search_prompt,
                            }
                        ],
                        temperature=0.5,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                    response_content = response.choices[0].message.content
                    # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                    if "```json" in response_content:
                        try:
                            response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                        except:
                            # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                            response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                    try:
                        response_content = json.loads(response_content)
                    except json.JSONDecodeError:
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâŒ{await t("task_error")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                    if response_content["status"] == "done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâœ…{await t("task_done")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "not_done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ{await t("task_not_done")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "need_more_info":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ“{await t("task_need_more_info")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "search":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nğŸ”{await t("enter_search_stage")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        drs_msg = get_drs_stage(DRS_STAGE)
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": drs_msg,
                            }
                        )
                    elif response_content["status"] == "need_more_search":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nğŸ”{await t("need_more_search")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "answer":
                        DRS_STAGE = 3
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ­{await t("enter_answer_stage")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        drs_msg = get_drs_stage(DRS_STAGE)
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": drs_msg,
                            }
                        )
                    print("DRS_STAGE:", DRS_STAGE)
            yield "data: [DONE]\n\n"
            if m0:
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                    {
                        "role": "assistant",
                        "content": full_content,
                    }
                ]
                executor = ThreadPoolExecutor()
                async def add_async():
                    loop = asyncio.get_event_loop()
                    # ç»‘å®š user_id å…³é”®å­—å‚æ•°
                    func = partial(m0.add, user_id=memoryId)
                    # ä¼ é€’ messages ä½œä¸ºä½ç½®å‚æ•°
                    await loop.run_in_executor(executor, func, messages)
                    print("çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")

                asyncio.create_task(add_async())
                print("çŸ¥è¯†åº“æ›´æ–°ä»»åŠ¡å·²æäº¤")
            return
        
        return StreamingResponse(
            stream_generator(user_prompt, DRS_STAGE),
            media_type="text/event-stream",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    except Exception as e:
        # å¦‚æœe.status_codeå­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å®ƒä½œä¸ºHTTPçŠ¶æ€ç ï¼Œå¦åˆ™ä½¿ç”¨500
        return JSONResponse(
            status_code=getattr(e, "status_code", 500),
            content={"error": str(e)},
        )

async def generate_complete_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search):
    global mcp_client_list
    DRS_STAGE = 1 # 1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ
    from py.load_files import get_files_content,file_tool,image_tool
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base,rerank_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool
    from py.code_interpreter import e2b_code_tool,local_run_code_tool
    m0 = None
    if settings["memorySettings"]["is_memory"]:
        memoryId = settings["memorySettings"]["selectedMemory"]
        cur_memory = None
        for memory in settings["memories"]:
            if memory["id"] == memoryId:
                cur_memory = memory
                break
        if cur_memory:

            config={
                "embedder": {
                    "provider": 'openai',
                    "config": {
                        "model": cur_memory['model'],
                        "api_key": cur_memory['api_key'],
                        "openai_base_url":cur_memory["base_url"],
                        "embedding_dims":1024
                    },
                },
                "llm": {
                    "provider": 'openai',
                    "config": {
                        "model": settings['model'],
                        "api_key": settings['api_key'],
                        "openai_base_url":settings["base_url"]
                    }
                },
                "vector_store": {
                    "provider": "faiss",
                    "config": {
                        "collection_name": "agent-party",
                        "path": os.path.join(MEMORY_CACHE_DIR,memoryId),
                        "distance_strategy": "euclidean",
                        "embedding_model_dims": 1024
                    }
                }
            }
            m0 = Memory.from_config(config)
    images = await images_in_messages(request.messages,fastapi_base_url)
    request.messages = await message_without_images(request.messages)
    open_tag = "<think>"
    close_tag = "</think>"
    tools = request.tools or []
    tools = request.tools or []
    if mcp_client_list:
        for server_name, mcp_client in mcp_client_list.items():
            if server_name in settings['mcpServers']:
                if 'disabled' not in settings['mcpServers'][server_name]:
                    settings['mcpServers'][server_name]['disabled'] = False
                if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                    function = await mcp_client.get_openai_functions()
                    if function:
                        tools.extend(function)
    get_llm_tool_fuction = await get_llm_tool(settings)
    if get_llm_tool_fuction:
        tools.append(get_llm_tool_fuction)
    get_agent_tool_fuction = await get_agent_tool(settings)
    if get_agent_tool_fuction:
        tools.append(get_agent_tool_fuction)
    get_a2a_tool_fuction = await get_a2a_tool(settings)
    if get_a2a_tool_fuction:
        tools.append(get_a2a_tool_fuction)
    if settings['tools']['pollinations']['enabled']:
        tools.append(pollinations_image_tool)
    if settings['tools']['getFile']['enabled']:
        tools.append(file_tool)
        tools.append(image_tool)
    if settings["codeSettings"]['enabled']:
        if settings["codeSettings"]["engine"] == "e2b":
            tools.append(e2b_code_tool)
        elif settings["codeSettings"]["engine"] == "sandbox":
            tools.append(local_run_code_tool)
    if settings["custom_http"]:
        for custom_http in settings["custom_http"]:
            if custom_http["enabled"]:
                if custom_http['body'] == "":
                    custom_http['body'] = "{}"
                custom_http_tool = {
                    "type": "function",
                    "function": {
                        "name": f"custom_http_{custom_http['name']}",
                        "description": f"{custom_http['description']}",
                        "parameters": json.loads(custom_http['body']),
                    },
                }
                tools.append(custom_http_tool)
    search_not_done = False
    search_task = ""
    print(tools)
    try:
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        if request.fileLinks:
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            system_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += system_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': system_message})
        kb_list = []
        user_prompt = request.messages[-1]['content']
        if m0:
            lore_content = ""
            assistant_reply = ""
            # æ‰¾å‡ºrequest.messagesä¸­ä¸Šæ¬¡çš„assistantå›å¤
            for i in range(len(request.messages)-1, -1, -1):
                if request.messages[i]['role'] == 'assistant':
                    assistant_reply = request.messages[i]['content']
                    break
            if cur_memory["lorebook"]:
                for lore in cur_memory["lorebook"]:
                    if lore["name"] != "" and (lore["name"] in user_prompt or lore["name"] in assistant_reply):
                        lore_content = lore_content + "\n\n" + f"{lore['name']}ï¼š{lore['value']}"
            memoryLimit = settings["memorySettings"]["memoryLimit"]
            try:
                print("æŸ¥è¯¢è®°å¿†")
                relevant_memories = m0.search(query=user_prompt, user_id=memoryId, limit=memoryLimit)
                relevant_memories = json.dumps(relevant_memories, ensure_ascii=False)
                print("æŸ¥è¯¢è®°å¿†ç»“æŸ")
            except Exception as e:
                print("m0.search error:",e)
                relevant_memories = ""
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"
            else:
                request.messages.insert(0, {'role': 'system', 'content': "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"})
            if cur_memory["basic_character"]:
                print("æ·»åŠ è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"})
            if lore_content:
                print("æ·»åŠ ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"})
        if settings["knowledgeBases"]:
            for kb in settings["knowledgeBases"]:
                if kb["enabled"] and kb["processingStatus"] == "completed":
                    kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
        if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                all_kb_content = []
                # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                for kb in kb_list:
                    kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                    all_kb_content.extend(kb_content)
                    if settings["KBSettings"]["is_rerank"]:
                        all_kb_content = await rerank_knowledge_base(user_prompt,all_kb_content)
                if all_kb_content:
                    kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                    request.messages[-1]['content'] += f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}"
        if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += kb_list_message
                else:
                    request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
        else:
            kb_list = []
        request = await tools_change_messages(request, settings)
        if settings['webSearch']['enabled'] or enable_web_search:
            if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    results = await DDGsearch_async(user_prompt)
                elif settings['webSearch']['engine'] == 'searxng':
                    results = await searxng_async(user_prompt)
                elif settings['webSearch']['engine'] == 'tavily':
                    results = await Tavily_search_async(user_prompt)
                if results:
                    request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}"
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    tools.append(duckduckgo_tool)
                elif settings['webSearch']['engine'] == 'searxng':
                    tools.append(searxng_tool)
                elif settings['webSearch']['engine'] == 'tavily':
                    tools.append(tavily_tool)
                if settings['webSearch']['crawler'] == 'jina':
                    tools.append(jina_crawler_tool)
                elif settings['webSearch']['crawler'] == 'crawl4ai':
                    tools.append(Crawl4Ai_tool)
        if kb_list:
            tools.append(kb_tool)
        if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            deepsearch_messages = copy.deepcopy(request.messages)
            deepsearch_messages[-1]['content'] += "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚"
            response = await client.chat.completions.create(
                model=model,
                messages=deepsearch_messages,
                temperature=0.5, 
                max_tokens=512,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            user_prompt = response.choices[0].message.content
            request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
        if settings['reasoner']['enabled'] or enable_thinking:
            reasoner_messages = copy.deepcopy(request.messages)
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                drs_msg = get_drs_stage(DRS_STAGE)
                if drs_msg:
                    reasoner_messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
                reasoner_messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
            if tools:
                reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
            for modelProvider in settings['modelProviders']: 
                if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                    vendor = modelProvider['vendor']
                    break
            msg = await images_add_in_messages(reasoner_messages, images,settings)    
            if vendor == 'Ollama':
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    temperature=settings['reasoner']['temperature']
                )
                # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                start_index = reasoning_content.find(open_tag) + len(open_tag)
                end_index = reasoning_content.find(close_tag)
                if start_index != -1 and end_index != -1:
                    reasoning_content = reasoning_content[start_index:end_index]
                else:
                    reasoning_content = ""
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoning_content
            else:
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    temperature=settings['reasoner']['temperature']
                )
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
            drs_msg = get_drs_stage(DRS_STAGE)
            if drs_msg:
                request.messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
        msg = await images_add_in_messages(request.messages, images,settings)
        if tools:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                tools=tools,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p or settings['top_p'],
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p or settings['top_p'],
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
        if response.choices[0].message.tool_calls:
            pass
        elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,response.choices[0].message.content)
            research_response = await client.chat.completions.create(
                model=model,
                messages=[
                    {
                    "role": "user",
                    "content": search_prompt,
                    }
                ],
                temperature=0.5,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            response_content = research_response.choices[0].message.content
            print(response_content)
            # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
            if "```json" in response_content:
                try:
                    response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                except:
                    # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                    response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
            response_content = json.loads(response_content)
            if response_content["status"] == "done":
                search_not_done = False
            elif response_content["status"] == "not_done":
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "need_more_info":
                DRS_STAGE = 2
                search_not_done = False
            elif response_content["status"] == "search":
                DRS_STAGE = 2
                search_not_done = True
                drs_msg = get_drs_stage(DRS_STAGE)
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": drs_msg,
                    }
                )
            elif response_content["status"] == "need_more_search":
                DRS_STAGE = 2
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "answer":
                DRS_STAGE = 3
                search_not_done = True
                drs_msg = get_drs_stage(DRS_STAGE)
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": drs_msg,
                    }
                )
        reasoner_messages = copy.deepcopy(request.messages)
        while response.choices[0].message.tool_calls or search_not_done:
            if response.choices[0].message.tool_calls:
                assistant_message = response.choices[0].message
                response_content = assistant_message.tool_calls[0].function
                print(response_content.name)
                modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                data_list = json.loads(modified_data)
                # å­˜å‚¨å¤„ç†ç»“æœ
                results = []
                for data in data_list:
                    result = await dispatch_tool(response_content.name, data,settings) # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                    if result is not None:
                        # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                        results.append(json.dumps(result))

                # å°†æ‰€æœ‰ç»“æœæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
                combined_results = ''.join(results)
                if combined_results:
                    results = combined_results
                else:
                    results = None
                print(results)
                if results is None:
                    break
                if response_content.name in ["query_knowledge_base"]:
                    if settings["KBSettings"]["is_rerank"]:
                        results = await rerank_knowledge_base(user_prompt,results)
                    results = json.dumps(results, ensure_ascii=False, indent=4)
                request.messages.append(
                    {
                        "tool_calls": [
                            {
                                "id": assistant_message.tool_calls[0].id,
                                "function": {
                                    "arguments": response_content.arguments,
                                    "name": response_content.name,
                                },
                                "type": assistant_message.tool_calls[0].type,
                            }
                        ],
                        "role": "assistant",
                        "content": str(response_content),
                    }
                )
                request.messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": assistant_message.tool_calls[0].id,
                        "name": response_content.name,
                        "content": str(results),
                    }
                )
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
            reasoner_messages.append(
                {
                    "role": "assistant",
                    "content": str(response_content),
                }
            )
            reasoner_messages.append(
                {
                    "role": "user",
                    "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                }
            )
            if settings['reasoner']['enabled'] or enable_thinking:

                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        temperature=settings['reasoner']['temperature']
                    )
                    # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                    reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                    # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                    start_index = reasoning_content.find(open_tag) + len(open_tag)
                    end_index = reasoning_content.find(close_tag)
                    if start_index != -1 and end_index != -1:
                        reasoning_content = reasoning_content[start_index:end_index]
                    else:
                        reasoning_content = ""
                    request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoning_content
                else:
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            print(response)
            if response.choices[0].message.tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,response.choices[0].message.content)
                research_response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = research_response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                response_content = json.loads(response_content)
                if response_content["status"] == "done":
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    DRS_STAGE = 2
                    search_not_done = False
                elif response_content["status"] == "search":
                    DRS_STAGE = 2
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                elif response_content["status"] == "need_more_search":
                    DRS_STAGE = 2
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "answer":
                    DRS_STAGE = 3
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
       # å¤„ç†å“åº”å†…å®¹
        response_dict = response.model_dump()
        content = response_dict["choices"][0]['message']['content']
        if open_tag in content and close_tag in content:
            reasoning_content = re.search(fr'{open_tag}(.*?)\{close_tag}', content, re.DOTALL)
            if reasoning_content:
                # å­˜å‚¨åˆ° reasoning_content å­—æ®µ
                response_dict["choices"][0]['message']['reasoning_content'] = reasoning_content.group(1).strip()
                # ç§»é™¤åŸå†…å®¹ä¸­çš„æ ‡ç­¾éƒ¨åˆ†
                response_dict["choices"][0]['message']['content'] = re.sub(fr'{open_tag}(.*?)\{close_tag}', '', content, flags=re.DOTALL).strip()
        if settings['reasoner']['enabled'] or enable_thinking:
            response_dict["choices"][0]['message']['reasoning_content'] = reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        if m0:
            messages=[
                {
                    "role": "user",
                    "content": user_prompt,
                },
                {
                    "role": "assistant",
                    "content": response_dict["choices"][0]['message']['content'],
                }
            ]
            executor = ThreadPoolExecutor()
            async def add_async():
                loop = asyncio.get_event_loop()
                # ç»‘å®š user_id å…³é”®å­—å‚æ•°
                func = partial(m0.add, user_id=memoryId)
                # ä¼ é€’ messages ä½œä¸ºä½ç½®å‚æ•°
                await loop.run_in_executor(executor, func, messages)
                print("çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")

            asyncio.create_task(add_async())
        return JSONResponse(content=response_dict)
    except Exception as e:
        return JSONResponse(
            content={"error": {"message": e.message, "type": "api_error", "code": e.code}}
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/models")
async def get_models():
    """
    è·å–æ¨¡å‹åˆ—è¡¨
    """
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            Model(
                id=agent["name"],  
                created=0,  
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
            for agent in agents.values()  
        ]
        # æ·»åŠ é»˜è®¤çš„ 'super-model'
        model_data.append(
            Model(
                id='super-model',
                created=0,
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
        )

        # æ„é€ å®Œæ•´ SyncPage å“åº”
        response = SyncPage[Model](
            object="list",
            data=model_data,
            has_more=False  # æ·»åŠ åˆ†é¡µæ ‡è®°
        )
        # ç›´æ¥è¿”å›æ¨¡å‹å­—å…¸ï¼Œç”± FastAPI è‡ªåŠ¨åºåˆ—åŒ–ä¸º JSON
        return response.model_dump()  
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": e.message,
                    "type": e.type or "api_error",
                    "code": e.code
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/agents",operation_id="get_agents")
async def get_agents():
    """
    è·å–æ¨¡å‹åˆ—è¡¨
    """
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            {
                "name": agent["name"],
                "description": agent["system_prompt"],
            }
            for agent in agents.values()  
        ]
        # æ·»åŠ é»˜è®¤çš„ 'super-model'
        model_data.append(
            {
                "name": 'super-model',
                "description": "Super-Agent-Party default agent",
            }
        )
        return model_data
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": e.message,
                    "type": e.type or "api_error",
                    "code": e.code
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

class ProviderModelRequest(BaseModel):
    url: str
    api_key: str

@app.post("/v1/providers/models")
async def fetch_provider_models(request: ProviderModelRequest):
    try:
        # ä½¿ç”¨ä¼ å…¥çš„provideré…ç½®åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        client = AsyncOpenAI(api_key=request.api_key, base_url=request.url)
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = await client.models.list()
        # æå–æ¨¡å‹IDå¹¶è¿”å›
        return JSONResponse(content={"data": [model.id for model in model_list.data]})
    except Exception as e:
        # å¤„ç†å¼‚å¸¸ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions", operation_id="chat_with_agent_party")
async def chat_endpoint(request: ChatRequest,fastapi_request: Request):
    """
    ç”¨æ¥ä¸agent partyä¸­çš„æ¨¡å‹èŠå¤©
    messages: å¿…å¡«é¡¹ï¼ŒèŠå¤©è®°å½•ï¼ŒåŒ…æ‹¬roleå’Œcontent
    model: å¯é€‰é¡¹ï¼Œé»˜è®¤ä½¿ç”¨ 'super-model'ï¼Œå¯ä»¥ç”¨get_models()è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    stream: å¯é€‰é¡¹ï¼Œé»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æµå¼å“åº”
    enable_thinking: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
    enable_deep_research: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æ·±åº¦ç ”ç©¶æ¨¡å¼
    enable_web_search: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
    """
    fastapi_base_url = str(fastapi_request.base_url)
    global client, settings,reasoner_client,mcp_client_list
    model = request.model or 'super-model' # é»˜è®¤ä½¿ç”¨ 'super-model'
    enable_thinking = request.enable_thinking or False
    enable_deep_research = request.enable_deep_research or False
    enable_web_search = request.enable_web_search or False
    if model == 'super-model':
        current_settings = await load_settings()
        # åŠ¨æ€æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        if (current_settings['api_key'] != settings['api_key'] 
            or current_settings['base_url'] != settings['base_url']):
            client = AsyncOpenAI(
                api_key=current_settings['api_key'],
                base_url=current_settings['base_url'] or "https://api.openai.com/v1",
            )
        if (current_settings['reasoner']['api_key'] != settings['reasoner']['api_key'] 
            or current_settings['reasoner']['base_url'] != settings['reasoner']['base_url']):
            reasoner_client = AsyncOpenAI(
                api_key=current_settings['reasoner']['api_key'],
                base_url=current_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
            )
        if current_settings != settings:
            settings = current_settings
        try:
            if request.stream:
                return await generate_stream_response(client,reasoner_client, request, settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
            return await generate_complete_response(client,reasoner_client, request, settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    else:
        current_settings = await load_settings()
        agentSettings = current_settings['agents'].get(model, {})
        if not agentSettings:
            for agentId , agentConfig in current_settings['agents'].items():
                if current_settings['agents'][agentId]['name'] == model:
                    agentSettings = current_settings['agents'][agentId]
                    break
        if not agentSettings:
            return JSONResponse(
                status_code=404,
                content={"error": {"message": f"Agent {model} not found", "type": "not_found", "code": 404}}
            )
        if agentSettings['config_path']:
            with open(agentSettings['config_path'], 'r' , encoding='utf-8') as f:
                agent_settings = json.load(f)
            # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
            if agentSettings['system_prompt']:
                if request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] = agentSettings['system_prompt'] + "\n\n" + request.messages[0]['content']
                else:
                    request.messages.insert(0, {'role': 'system', 'content': agentSettings['system_prompt']})
        agent_client = AsyncOpenAI(
            api_key=agent_settings['api_key'],
            base_url=agent_settings['base_url'] or "https://api.openai.com/v1",
        )
        agent_reasoner_client = AsyncOpenAI(
            api_key=agent_settings['reasoner']['api_key'],
            base_url=agent_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
        )
        try:
            if request.stream:
                return await generate_stream_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
            return await generate_complete_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )

# æ·»åŠ çŠ¶æ€å­˜å‚¨
mcp_status = {}
@app.post("/create_mcp")
async def create_mcp_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    mcp_id = data.get("mcpId")
    
    if not mcp_id:
        raise HTTPException(status_code=400, detail="Missing mcpId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_mcp, mcp_id)
    
    return {"success": True, "message": "MCPæœåŠ¡å™¨åˆå§‹åŒ–å·²å¼€å§‹"}
@app.get("/mcp_status/{mcp_id}")
async def get_mcp_status(mcp_id: str):
    status = mcp_status.get(mcp_id, "not_found")
    return {"mcp_id": mcp_id, "status": status}
async def process_mcp(mcp_id: str):
    global mcp_client_list
    mcp_status[mcp_id] = "initializing"
    try:
        # è·å–å¯¹åº”æœåŠ¡å™¨çš„é…ç½®
        cur_settings = await load_settings()
        server_config = cur_settings['mcpServers'][mcp_id]
        
        # æ‰§è¡Œåˆå§‹åŒ–é€»è¾‘
        mcp_client_list[mcp_id] = McpClient()    
        await asyncio.wait_for(mcp_client_list[mcp_id].initialize(mcp_id, server_config), timeout=6)
        mcp_status[mcp_id] = "ready"
        mcp_client_list[mcp_id].disabled = False
        
    except Exception as e:
        mcp_client_list[mcp_id].disabled = True
        mcp_status[mcp_id] = f"failed: {str(e)}"

@app.post("/api/remove_mcp")
async def remove_mcp_server(request: Request):
    global settings, mcp_client_list
    try:
        data = await request.json()
        server_name = data.get("serverName", "")

        if not server_name:
            raise HTTPException(status_code=400, detail="No server names provided")

        # ç§»é™¤æŒ‡å®šçš„MCPæœåŠ¡å™¨
        current_settings = await load_settings()
        if server_name in current_settings['mcpServers']:
            del current_settings['mcpServers'][server_name]
            await save_settings(current_settings)
            settings = current_settings

            # ä»mcp_client_listä¸­ç§»é™¤
            if server_name in mcp_client_list:
                mcp_client_list[server_name].disabled = True

            return JSONResponse({"success": True, "removed": server_name})
        else:
            raise HTTPException(status_code=404, detail="Server not found")
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON format")
    except Exception as e:
        logger.error(f"ç§»é™¤MCPæœåŠ¡å™¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/remove_memory")
async def remove_memory_endpoint(request: Request):
    data = await request.json()
    memory_id = data.get("memoryId")
    if memory_id:
        try:
            # åˆ é™¤MEMORY_CACHE_DIRç›®å½•ä¸‹çš„memory_idæ–‡ä»¶å¤¹
            memory_dir = os.path.join(MEMORY_CACHE_DIR, memory_id)
            shutil.rmtree(memory_dir)
            return JSONResponse({"success": True, "message": "Memory removed"})
        except Exception as e:
            return JSONResponse({"success": False, "message": str(e)})
    else:
        return JSONResponse({"success": False, "message": "No memoryId provided"})

@app.post("/remove_agent")
async def remove_agent_endpoint(request: Request):
    data = await request.json()
    agent_id = data.get("agentId")
    if agent_id:
        try:
            # åˆ é™¤AGENT_CACHE_DIRç›®å½•ä¸‹çš„agent_idæ–‡ä»¶å¤¹
            agent_dir = os.path.join(AGENT_DIR, f"{agent_id}.json")
            shutil.rmtree(agent_dir)
            return JSONResponse({"success": True, "message": "Agent removed"})
        except Exception as e:
            return JSONResponse({"success": False, "message": str(e)})
    else:
        return JSONResponse({"success": False, "message": "No agentId provided"})

@app.post("/a2a/initialize")
async def initialize_a2a(request: Request):
    from python_a2a import A2AClient
    data = await request.json()
    try:
        client = A2AClient(data['url'])
        agent_card = client.agent_card.to_json()
        agent_card = json.loads(agent_card)
        return JSONResponse({
            **agent_card,
            "status": "ready",
            "enabled": True
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

# åœ¨ç°æœ‰è·¯ç”±ä¹‹åæ·»åŠ healthè·¯ç”±
@app.get("/health")
async def health_check():
    return {"status": "ok"}


@app.post("/load_file")
async def load_file_endpoint(request: Request, files: List[UploadFile] = File(None)):
    fastapi_base_url = str(request.base_url)
    logger.info(f"Received request with content type: {request.headers.get('Content-Type')}")
    file_links = []
    textFiles = []
    imageFiles = []
    content_type = request.headers.get('Content-Type', '')
    try:
        if 'multipart/form-data' in content_type:
            # å¤„ç†æµè§ˆå™¨ä¸Šä¼ çš„æ–‡ä»¶
            if not files:
                raise HTTPException(status_code=400, detail="No files provided")
            
            for file in files:
                file_extension = os.path.splitext(file.filename)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                with open(destination, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file.filename
                }
                file_links.append(file_link)
                file_meta = {
                    "unique_filename": unique_filename,
                    "original_filename": file.filename,
                }
                # file_extensionç§»é™¤ç‚¹å·
                file_extension = file_extension[1:]
                if file_extension in ALLOWED_EXTENSIONS:
                    textFiles.append(file_meta)
                elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                    imageFiles.append(file_meta)
        elif 'application/json' in content_type:
            # å¤„ç†Electronå‘é€çš„JSONæ–‡ä»¶è·¯å¾„
            data = await request.json()
            logger.info(f"Processing JSON data: {data}")
            
            for file_info in data.get("files", []):
                file_path = file_info.get("path")
                file_name = file_info.get("name", os.path.basename(file_path))
                
                if not os.path.isfile(file_path):
                    logger.error(f"File not found: {file_path}")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                file_extension = os.path.splitext(file_name)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•
                with open(file_path, "rb") as src, open(destination, "wb") as dst:
                    dst.write(src.read())
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file_name
                }
                file_links.append(file_link)
                file_meta = {
                    "unique_filename": unique_filename,
                    "original_filename": file_name,
                }
                # file_extensionç§»é™¤ç‚¹å·
                file_extension = file_extension[1:]
                if file_extension in ALLOWED_EXTENSIONS:
                    textFiles.append(file_meta)
                elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                    imageFiles.append(file_meta)
        else:
            raise HTTPException(status_code=400, detail="Unsupported Content-Type")
        return JSONResponse(content={"success": True, "fileLinks": file_links , "textFiles": textFiles, "imageFiles": imageFiles})
    
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/delete_file")
async def delete_file_endpoint(request: Request):
    data = await request.json()
    file_name = data.get("fileName")
    file_path = os.path.join(UPLOAD_FILES_DIR, file_name)
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            return JSONResponse(content={"success": True})
        else:
            return JSONResponse(content={"success": False, "message": "File not found"})
    except Exception as e:
        return JSONResponse(content={"success": False, "message": str(e)})

@app.post("/create_kb")
async def create_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")
    
    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_kb, kb_id)
    
    return {"success": True, "message": "çŸ¥è¯†åº“å¤„ç†å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢çŠ¶æ€"}

@app.post("/remove_kb")
async def remove_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")

    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    try:
        background_tasks.add_task(remove_kb, kb_id)
    except Exception as e:
        return {"success": False, "message": str(e)}
    return {"success": True, "message": "çŸ¥è¯†åº“å·²åˆ é™¤"}

# åˆ é™¤çŸ¥è¯†åº“
async def remove_kb(kb_id):
    # åˆ é™¤KB_DIR/kb_idç›®å½•
    kb_dir = os.path.join(KB_DIR, str(kb_id))
    if os.path.exists(kb_dir):
        shutil.rmtree(kb_dir)
    else:
        print(f"KB directory {kb_dir} does not exist.")
    return

# æ·»åŠ çŠ¶æ€å­˜å‚¨
kb_status = {}
@app.get("/kb_status/{kb_id}")
async def get_kb_status(kb_id):
    status = kb_status.get(kb_id, "not_found")
    print (f"kb_status: {kb_id} - {status}")
    return {"kb_id": kb_id, "status": status}

# ä¿®æ”¹ process_kb
async def process_kb(kb_id):
    kb_status[kb_id] = "processing"
    try:
        from py.know_base import process_knowledge_base
        await process_knowledge_base(kb_id)
        kb_status[kb_id] = "completed"
    except Exception as e:
        kb_status[kb_id] = f"failed: {str(e)}"

# å®šä¹‰è¯·æ±‚ä½“
class QQBotConfig(BaseModel):
    QQAgent: str
    memoryLimit: int
    appid: str
    secret: str
    separators: List[str]

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æœºå™¨äººè¿›ç¨‹
qq_bot_process = None
current_bot_config = None

class MyClient(botpy.Client):
    def __init__(self,start_event, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.is_running = False
        self.QQAgent = "super-model"
        self.memoryLimit = 10
        self.memoryList = {}
        self.start_event = start_event
        self.separators = ['ã€‚', '\n', 'ï¼Ÿ', 'ï¼']

    async def on_ready(self):
        self.is_running = True
        self.start_event.set()

    async def on_c2c_message_create(self, message: C2CMessage):
        if not self.is_running:
            return
        
        client = AsyncOpenAI(
            api_key="super-secret-key",
            base_url=f"http://127.0.0.1:{PORT}/v1"
        )
        user_content = []
        if message.attachments:
            for attachment in message.attachments:
                if attachment.content_type.startswith("image/"):
                    image_url = attachment.url
                    user_content.append({"type": "image_url", "image_url": {"url": image_url}})
        if user_content:
            user_content.append({"type": "text", "text": message.content})
        else:
            user_content = message.content
        print(f"User content: {user_content}")
        c_id = message.author.user_openid
        if c_id not in self.memoryList:
            self.memoryList[c_id] = []
        self.memoryList[c_id].append({"role": "user", "content": user_content})

        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†
        if not hasattr(self, 'msg_seq_counters'):
            self.msg_seq_counters = {}
        self.msg_seq_counters.setdefault(c_id, 1)
        
        if not hasattr(self, 'processing_states'):
            self.processing_states = {}
        self.processing_states[c_id] = {
            "text_buffer": "",
            "image_buffer": "",
            "image_cache": []
        }

        try:
            # æµå¼è°ƒç”¨API
            stream = await client.chat.completions.create(
                model=self.QQAgent,
                messages=self.memoryList[c_id],
                stream=True
            )
            
            full_response = []
            async for chunk in stream:
                content = chunk.choices[0].delta.content or ""
                full_response.append(content)
                
                # æ›´æ–°ç¼“å†²åŒº
                state = self.processing_states[c_id]
                state["text_buffer"] += content
                state["image_buffer"] += content

                # å¤„ç†æ–‡æœ¬å®æ—¶å‘é€
                while True:
                    if self.separators == []:
                        break
                    # æŸ¥æ‰¾åˆ†éš”ç¬¦ï¼ˆã€‚æˆ–\nï¼‰
                    buffer = state["text_buffer"]
                    split_pos = -1
                    for i, c in enumerate(buffer):
                        if c in self.separators:
                            split_pos = i + 1
                            break
                    if split_pos == -1:
                        break

                    # åˆ†å‰²å¹¶å¤„ç†å½“å‰æ®µè½
                    current_chunk = buffer[:split_pos]
                    state["text_buffer"] = buffer[split_pos:]
                    
                    # æ¸…æ´—å¹¶å‘é€æ–‡å­—
                    clean_text = self._clean_text(current_chunk)
                    if clean_text:
                        await self._send_text_message(message, clean_text)
                    
                    # æå–å›¾ç‰‡åˆ°ç¼“å­˜ï¼ˆä¸å‘é€ï¼‰
                    self._extract_images_to_cache(c_id, current_chunk)

            # å¤„ç†å‰©ä½™æ–‡æœ¬
            if state["text_buffer"]:
                clean_text = self._clean_text(state["text_buffer"])
                if clean_text:
                    await self._send_text_message(message, clean_text)
            
            # æœ€ç»ˆå›¾ç‰‡å‘é€
            await self._send_cached_images(message)

            # è®°å¿†ç®¡ç†
            full_content = "".join(full_response)
            self.memoryList[c_id].append({"role": "assistant", "content": full_content})
            if self.memoryLimit > 0:
                while len(self.memoryList[c_id]) > self.memoryLimit:
                    self.memoryList[c_id].pop(0)

        except Exception as e:
            print(f"å¤„ç†å¼‚å¸¸: {e}")
        finally:
            # æ¸…ç†çŠ¶æ€
            del self.processing_states[c_id]

    def _extract_images_to_cache(self, c_id, content):
        """æ¸è¿›å¼å›¾ç‰‡é“¾æ¥æå–"""
        state = self.processing_states[c_id]
        temp_buffer = state["image_buffer"] + content
        state["image_buffer"] = ""  # é‡ç½®ç¼“å†²åŒº
        
        # åŒ¹é…å®Œæ•´å›¾ç‰‡é“¾æ¥
        pattern = r'!\[.*?\]\((https?://[^\s\)]+)'
        matches = re.finditer(pattern, temp_buffer)
        for match in matches:
            state["image_cache"].append(match.group(1))
        
        # å¤„ç†æœªé—­åˆçš„å›¾ç‰‡æ ‡è®°
        last_unclosed = re.search(r'!\[.*?\]\([^)]*$', temp_buffer)
        if last_unclosed:
            state["image_buffer"] = last_unclosed.group()

    async def _send_text_message(self, message, text):
        """å‘é€æ–‡æœ¬æ¶ˆæ¯å¹¶æ›´æ–°åºå·"""
        c_id = message.author.user_openid
        await message._api.post_c2c_message(
            openid=message.author.user_openid,
            msg_type=0,
            msg_id=message.id,
            content=text,
            msg_seq=self.msg_seq_counters[c_id]
        )
        self.msg_seq_counters[c_id] += 1

    async def _send_cached_images(self, message):
        """æ‰¹é‡å‘é€ç¼“å­˜çš„å›¾ç‰‡"""
        c_id = message.author.user_openid
        state = self.processing_states.get(c_id, {})
        
        for url in state.get("image_cache", []):
            try:
                # é“¾æ¥æœ‰æ•ˆæ€§éªŒè¯
                if not re.match(r'^https?://', url):
                    continue
                # ç”¨requestè·å–å›¾ç‰‡ï¼Œä¿è¯å›¾ç‰‡å­˜åœ¨
                response = requests.get(url)
                # ä¸Šä¼ åª’ä½“æ–‡ä»¶
                upload_media = await message._api.post_c2c_file(
                    openid=message.author.user_openid,
                    file_type=1,
                    url=url
                )
                # å‘é€å¯Œåª’ä½“æ¶ˆæ¯
                await message._api.post_c2c_message(
                    openid=message.author.user_openid,
                    msg_type=7,
                    msg_id=message.id,
                    media=upload_media,
                    msg_seq=self.msg_seq_counters[c_id]
                )
                self.msg_seq_counters[c_id] += 1
            except Exception as e:
                print(f"å›¾ç‰‡å‘é€å¤±è´¥: {e}")

    def _clean_text(self, text):
        """ä¸‰çº§å†…å®¹æ¸…æ´—"""
        # ç§»é™¤å›¾ç‰‡æ ‡è®°
        clean = re.sub(r'!\[.*?\]\(.*?\)', '', text)
        # ç§»é™¤è¶…é“¾æ¥
        clean = re.sub(r'\[.*?\]\(.*?\)', '', clean)
        # ç§»é™¤çº¯URL
        clean = re.sub(r'https?://\S+', '', clean)
        return clean.strip()

    
    async def on_group_at_message_create(self, message: GroupMessage):
        if not self.is_running:
            return
        
        client = AsyncOpenAI(
            api_key="super-secret-key",
            base_url=f"http://127.0.0.1:{PORT}/v1"
        )
        user_content = []
        if message.attachments:
            for attachment in message.attachments:
                if attachment.content_type.startswith("image/"):
                    image_url = attachment.url
                    try:
                        # ç”¨requestè·å–å›¾ç‰‡ï¼Œä¿è¯å›¾ç‰‡å­˜åœ¨
                        response = requests.get(image_url)
                        user_content.append({"type": "image_url", "image_url": {"url": image_url}})
                    except Exception as e:
                        print(f"å›¾ç‰‡è·å–å¤±è´¥: {e}")
        if user_content:
            user_content.append({"type": "text", "text": message.content})
        else:
            user_content = message.content
        g_id = message.group_openid
        if g_id not in self.memoryList:
            self.memoryList[g_id] = []
        self.memoryList[g_id].append({"role": "user", "content": user_content})

        # åˆå§‹åŒ–ç¾¤ç»„çŠ¶æ€
        if not hasattr(self, 'group_states'):
            self.group_states = {}
        self.group_states[g_id] = {
            "msg_seq": 1,
            "text_buffer": "",
            "image_buffer": "",
            "image_cache": []
        }

        try:
            # æµå¼APIè°ƒç”¨
            stream = await client.chat.completions.create(
                model=self.QQAgent,
                messages=self.memoryList[g_id],
                stream=True
            )
            
            full_response = []
            async for chunk in stream:
                content = chunk.choices[0].delta.content or ""
                full_response.append(content)
                state = self.group_states[g_id]
                
                # æ›´æ–°æ–‡æœ¬ç¼“å†²åŒº
                state["text_buffer"] += content
                state["image_buffer"] += content

                # å¤„ç†æ–‡æœ¬åˆ†æ®µ
                while True:
                    if self.separators == []:
                        break
                    # æŸ¥æ‰¾åˆ†éš”ç¬¦ï¼ˆã€‚æˆ–\nï¼‰
                    buffer = state["text_buffer"]
                    split_pos = -1
                    for i, c in enumerate(buffer):
                        if c in self.separators:
                            split_pos = i + 1
                            break
                    if split_pos == -1:
                        break

                    # å¤„ç†å½“å‰æ®µè½
                    current_chunk = buffer[:split_pos]
                    state["text_buffer"] = buffer[split_pos:]
                    
                    # æ¸…æ´—å¹¶å‘é€æ–‡å­—
                    clean_text = self._clean_group_text(current_chunk)
                    if clean_text:
                        await self._send_group_text(message, clean_text, state)
                    
                    # æå–å›¾ç‰‡åˆ°ç¼“å­˜
                    self._cache_group_images(g_id, current_chunk)

            # å¤„ç†å‰©ä½™æ–‡æœ¬
            if self.group_states[g_id]["text_buffer"]:
                clean_text = self._clean_group_text(self.group_states[g_id]["text_buffer"])
                if clean_text:
                    await self._send_group_text(message, clean_text, state)

            # å‘é€ç¼“å­˜å›¾ç‰‡
            await self._send_group_images(message, g_id)

            # è®°å¿†ç®¡ç†
            full_content = "".join(full_response)
            self.memoryList[g_id].append({"role": "assistant", "content": full_content})
            if self.memoryLimit > 0:
                while len(self.memoryList[g_id]) > self.memoryLimit:
                    self.memoryList[g_id].pop(0)

        except Exception as e:
            print(f"ç¾¤èŠå¤„ç†å¼‚å¸¸: {e}")
        finally:
            # æ¸…ç†çŠ¶æ€
            del self.group_states[g_id]

    def _cache_group_images(self, g_id, content):
        """æ¸è¿›å¼å›¾ç‰‡ç¼“å­˜"""
        state = self.group_states[g_id]
        temp_buffer = state["image_buffer"] + content
        state["image_buffer"] = ""
        
        # åŒ¹é…å®Œæ•´å›¾ç‰‡é“¾æ¥
        pattern = r'!\[.*?\]\((https?://[^\s\)]+)'
        matches = re.finditer(pattern, temp_buffer)
        for match in matches:
            state["image_cache"].append(match.group(1))
        
        # å¤„ç†æœªé—­åˆæ ‡è®°
        last_unclosed = re.search(r'!\[.*?\]\([^)]*$', temp_buffer)
        if last_unclosed:
            state["image_buffer"] = last_unclosed.group()

    async def _send_group_text(self, message, text, state):
        """å‘é€ç¾¤èŠæ–‡å­—æ¶ˆæ¯"""
        await message._api.post_group_message(
            group_openid=message.group_openid,
            msg_type=0,
            msg_id=message.id,
            content=text,
            msg_seq=state["msg_seq"]
        )
        state["msg_seq"] += 1

    async def _send_group_images(self, message, g_id):
        """æ‰¹é‡å‘é€ç¾¤èŠå›¾ç‰‡"""
        state = self.group_states.get(g_id, {})
        for url in state.get("image_cache", []):
            try:
                # é“¾æ¥æœ‰æ•ˆæ€§éªŒè¯
                if not url.startswith(('http://', 'https://')):
                    continue
                # ç”¨requestè·å–å›¾ç‰‡ï¼Œä¿è¯å›¾ç‰‡å­˜åœ¨
                response = requests.get(url)
                # ä¸Šä¼ ç¾¤æ–‡ä»¶
                upload_media = await message._api.post_group_file(
                    group_openid=message.group_openid,
                    file_type=1,
                    url=url
                )
                # å‘é€ç¾¤åª’ä½“æ¶ˆæ¯
                await message._api.post_group_message(
                    group_openid=message.group_openid,
                    msg_type=7,
                    msg_id=message.id,
                    media=upload_media,
                    msg_seq=state["msg_seq"]
                )
                state["msg_seq"] += 1
            except Exception as e:
                print(f"ç¾¤å›¾ç‰‡å‘é€å¤±è´¥: {e}")

    def _clean_group_text(self, text):
        """ç¾¤èŠæ–‡æœ¬ä¸‰çº§æ¸…æ´—"""
        # ç§»é™¤å›¾ç‰‡æ ‡è®°
        clean = re.sub(r'!\[.*?\]\(.*?\)', '', text)
        # ç§»é™¤è¶…é“¾æ¥
        clean = re.sub(r'\[.*?\]\(.*?\)', '', clean)
        # ç§»é™¤çº¯URL
        clean = re.sub(r'https?://\S+', '', clean)
        return clean.strip()


def run_bot_process(config: QQBotConfig,start_event,shared_dict):
    """åœ¨æ–°è¿›ç¨‹ä¸­è¿è¡Œæœºå™¨äººçš„å‡½æ•°"""
    # é…ç½®å­è¿›ç¨‹çš„æ—¥å¿—ç³»ç»Ÿ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    QQlogger = logging.getLogger("QQBotProcess")
    QQlogger.info("å­è¿›ç¨‹å¯åŠ¨ï¼Œå¼€å§‹é…ç½®æœºå™¨äºº...")

    # ç§»é™¤è‡ªå®šä¹‰äº‹ä»¶å¾ªç¯å’Œä¿¡å·å¤„ç†
    # ç›´æ¥ä½¿ç”¨åŸºç±»çš„runæ–¹æ³•å¯åŠ¨
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯æ—¶ä¼ å…¥å¯åŠ¨äº‹ä»¶
        QQclient = MyClient(start_event, intents=botpy.Intents(public_messages=True))
        QQclient.QQAgent = config.QQAgent
        QQclient.memoryLimit = config.memoryLimit
        QQclient.separators = config.separators
        
        # è¿è¡Œæœºå™¨äºº
        QQclient.run(appid=config.appid, secret=config.secret)
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶å­˜å…¥å…±äº«å­—å…¸
        shared_dict['error'] = str(e)
        start_event.set()  # ç¡®ä¿äº‹ä»¶è¢«è®¾ç½®
        QQlogger.error(f"æœºå™¨äººå¯åŠ¨å¤±è´¥: {str(e)}")
    finally:
        QQclient.is_running = False
        # æ¸…ç†èµ„æº
        QQclient.memoryList.clear()
        QQlogger.info("æœºå™¨äººè¿›ç¨‹å·²å®Œå…¨åœæ­¢")

@app.post("/start_qq_bot")
async def start_qq_bot(config: QQBotConfig):
    global qq_bot_process, current_bot_config
    
    if qq_bot_process and qq_bot_process.is_alive():
        return JSONResponse(
            status_code=400,
            content={"success": False, "message": "QQæœºå™¨äººå·²ç»åœ¨è¿è¡Œ"}
        )

    # ä½¿ç”¨Manageråˆ›å»ºå…±äº«å¯¹è±¡
    with Manager() as manager:
        shared_dict = manager.dict()
        start_event = Event()
        
        # åˆ›å»ºå¹¶å¯åŠ¨è¿›ç¨‹
        qq_bot_process = multiprocessing.Process(
            target=run_bot_process,
            args=(config, start_event, shared_dict),
            name="QQBotProcess"
        )
        
        start_event.clear()  # æ¸…é™¤äº‹ä»¶çŠ¶æ€
        qq_bot_process.start()
        logger.info(f"QQæœºå™¨äººè¿›ç¨‹å·²å¯åŠ¨ï¼ŒPID: {qq_bot_process.pid}")
        
        # ç­‰å¾…å¯åŠ¨ç»“æœï¼ˆæœ€å¤š10ç§’ï¼‰
        try:
            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­åŒæ­¥ç­‰å¾…
            loop = asyncio.get_event_loop()
            event_set = await loop.run_in_executor(
                None, 
                lambda: start_event.wait(timeout=10.0)
            )
        except Exception as e:
            qq_bot_process.terminate()
            return JSONResponse(
                status_code=500,
                content={"success": False, "message": f"ç­‰å¾…æœºå™¨äººå¯åŠ¨æ—¶å‡ºé”™: {str(e)}"}
            )
        
        # æ£€æŸ¥å¯åŠ¨ç»“æœ
        if 'error' in shared_dict:
            error_msg = shared_dict['error']
            qq_bot_process.terminate()
            qq_bot_process.join(timeout=1.0)
            qq_bot_process = None
            return JSONResponse(
                status_code=500,
                content={"success": False, "message": f"æœºå™¨äººå¯åŠ¨å¤±è´¥: {error_msg}"}
            )
        
        if not event_set:
            qq_bot_process.terminate()
            qq_bot_process.join(timeout=1.0)
            qq_bot_process = None
            return JSONResponse(
                status_code=500,
                content={"success": False, "message": "æœºå™¨äººå¯åŠ¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"}
            )
        
        # å¯åŠ¨æˆåŠŸ
        current_bot_config = config
        return {
            "success": True,
            "message": "QQæœºå™¨äººå·²æˆåŠŸå¯åŠ¨",
            "pid": qq_bot_process.pid
        }

# é‡æ–°åŠ è½½QQæœºå™¨äºº
@app.post("/reload_qq_bot")
async def reload_qq_bot(config: QQBotConfig):
    """
    é‡æ–°åŠ è½½QQæœºå™¨äººé…ç½®
    æ”¯æŒçƒ­é‡è½½ï¼šå…ˆåœæ­¢å½“å‰æœºå™¨äººï¼Œç„¶åç”¨æ–°é…ç½®å¯åŠ¨
    """
    global qq_bot_process, current_bot_config
    
    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è½½
    if current_bot_config and config == current_bot_config:
        return {"message": "é…ç½®æœªå˜åŒ–ï¼Œæ— éœ€é‡è½½"}
    
    # 2. è®°å½•å½“å‰çŠ¶æ€
    was_running = False
    if qq_bot_process and qq_bot_process.is_alive():
        was_running = True
        pid = qq_bot_process.pid
        logger.info(f"æœºå™¨äººæ­£åœ¨è¿è¡Œ(PID: {pid})ï¼Œå°†å…ˆåœæ­¢å†é‡æ–°å¯åŠ¨")
        
        # åœæ­¢å½“å‰æœºå™¨äºº
        try:
            # å‘é€SIGTERMä¿¡å·
            os.kill(qq_bot_process.pid, signal.SIGTERM)
            logger.info(f"å·²å‘æœºå™¨äººè¿›ç¨‹ {pid} å‘é€ SIGTERM ä¿¡å·")
            
            # ç­‰å¾…æœ€å¤š3ç§’
            start_time = time.time()
            while time.time() - start_time < 3:
                if not qq_bot_process.is_alive():
                    break
                await asyncio.sleep(0.1)
            
            if qq_bot_process.is_alive():
                # å¦‚æœè¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œå¼ºåˆ¶ç»ˆæ­¢
                logger.warning(f"æœºå™¨äººè¿›ç¨‹ {pid} æœªå“åº”ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                qq_bot_process.terminate()
                qq_bot_process.join(timeout=1.0)
        except ProcessLookupError:
            logger.warning("æœºå™¨äººè¿›ç¨‹å·²ä¸å­˜åœ¨")
        except Exception as e:
            logger.error(f"åœæ­¢æœºå™¨äººæ—¶å‡ºé”™: {str(e)}")
        finally:
            qq_bot_process = None
    
    # 3. ä¿å­˜æ–°é…ç½®
    current_bot_config = config
    
    # 4. å¦‚æœä¹‹å‰æ˜¯è¿è¡ŒçŠ¶æ€ï¼Œé‡æ–°å¯åŠ¨
    if was_running:
        # åˆ›å»ºå¹¶å¯åŠ¨è¿›ç¨‹
        qq_bot_process = multiprocessing.Process(
            target=run_bot_process,
            args=(config,),
            name="QQBotProcess"
        )
        
        qq_bot_process.start()
        logger.info(f"QQæœºå™¨äººå·²é‡æ–°å¯åŠ¨ï¼Œæ–°PID: {qq_bot_process.pid}")
        
        return {
            "message": "QQæœºå™¨äººé…ç½®å·²é‡è½½å¹¶é‡æ–°å¯åŠ¨",
            "pid": qq_bot_process.pid,
            "config_changed": True
        }
    
    # 5. å¦‚æœä¹‹å‰æœªè¿è¡Œï¼Œåªæ›´æ–°é…ç½®
    logger.info("QQæœºå™¨äººé…ç½®å·²æ›´æ–°ï¼Œä½†æœªå¯åŠ¨ï¼ˆæœºå™¨äººä¹‹å‰æœªè¿è¡Œï¼‰")
    return {
        "message": "QQæœºå™¨äººé…ç½®å·²æ›´æ–°",
        "pid": None,
        "config_changed": True
    }

# åœæ­¢QQæœºå™¨äºº
@app.post("/stop_qq_bot")
async def stop_qq_bot():
    global qq_bot_process
    
    if not qq_bot_process or not qq_bot_process.is_alive():
        raise HTTPException(status_code=400, detail="QQæœºå™¨äººæœªåœ¨è¿è¡Œ")

    try:
        # é¦–å…ˆå°è¯•ä¼˜é›…åœæ­¢ï¼ˆå‘é€ä¿¡å·ï¼‰
        os.kill(qq_bot_process.pid, signal.SIGTERM)
        logger.info(f"å·²å‘æœºå™¨äººè¿›ç¨‹ {qq_bot_process.pid} å‘é€ SIGTERM ä¿¡å·")
        
        # ç­‰å¾…æœ€å¤š2ç§’
        start_time = time.time()
        while time.time() - start_time < 2:
            if not qq_bot_process.is_alive():
                break
            await asyncio.sleep(0.1)
        
        if qq_bot_process.is_alive():
            # å¦‚æœè¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œå¼ºåˆ¶ç»ˆæ­¢
            logger.warning(f"æœºå™¨äººè¿›ç¨‹ {qq_bot_process.pid} æœªå“åº”ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            qq_bot_process.terminate()
            qq_bot_process.join(timeout=1.0)
        print("æœºå™¨äººè¿›ç¨‹å·²åœæ­¢")
    except ProcessLookupError:
        logger.warning("æœºå™¨äººè¿›ç¨‹å·²ä¸å­˜åœ¨")
    except Exception as e:
        logger.error(f"åœæ­¢æœºå™¨äººæ—¶å‡ºé”™: {str(e)}")
    finally:
        qq_bot_process = None
    
    return {"success": True,"message": "QQæœºå™¨äººå·²åœæ­¢"}

# æ£€æŸ¥æœºå™¨äººçŠ¶æ€å’Œé…ç½®
@app.get("/qq_bot_status")
async def qq_bot_status():
    global qq_bot_process, current_bot_config
    
    status = {
        "is_running": False,
        "pid": None,
        "config": current_bot_config.model_dump() if current_bot_config else None
    }
    
    if qq_bot_process and qq_bot_process.is_alive():
        status["is_running"] = True
        status["pid"] = qq_bot_process.pid
    
    return status


settings_lock = asyncio.Lock()
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    active_connections.append(websocket)

    try:
        async with settings_lock:  # è¯»å–æ—¶åŠ é”
            current_settings = await load_settings()
        await websocket.send_json({"type": "settings", "data": current_settings})
        while True:
            data = await websocket.receive_json()
            if data.get("type") == "ping":
                await websocket.send_json({"type": "pong"})
            elif data.get("type") == "save_settings":
                await save_settings(data.get("data", {}))
                # å‘é€ç¡®è®¤æ¶ˆæ¯ï¼ˆæºå¸¦ç›¸åŒ correlationIdï¼‰
                await websocket.send_json({
                    "type": "settings_saved",
                    "correlationId": data.get("correlationId"),
                    "success": True
                })
            elif data.get("type") == "get_settings":
                settings = await load_settings()
                await websocket.send_json({"type": "settings", "data": settings})
            elif data.get("type") == "save_agent":
                current_settings = await load_settings()
                
                # ç”Ÿæˆæ™ºèƒ½ä½“IDå’Œé…ç½®è·¯å¾„
                agent_id = str(shortuuid.ShortUUID().random(length=8))
                config_path = os.path.join(AGENT_DIR, f"{agent_id}.json")
                
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(current_settings, f, indent=4, ensure_ascii=False)
                
                # æ›´æ–°ä¸»é…ç½®
                current_settings['agents'][agent_id] = {
                    "id": agent_id,
                    "name": data['data']['name'],
                    "system_prompt": data['data']['system_prompt'],
                    "config_path": config_path,
                    "enabled": False,
                }
                await save_settings(current_settings)
                
                # å¹¿æ’­æ›´æ–°åçš„é…ç½®
                await websocket.send_json({
                    "type": "settings",
                    "data": current_settings
                })
    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        active_connections.remove(websocket)

mcp = FastApiMCP(
    app,
    name="Agent party MCP - chat with multiple agents",
    include_operations=["get_agents", "chat_with_agent_party"],
)

mcp.mount()

app.mount("/uploaded_files", StaticFiles(directory=UPLOAD_FILES_DIR), name="uploaded_files")
app.mount("/node_modules", StaticFiles(directory=os.path.join(base_path, "node_modules")), name="node_modules")
app.mount("/", StaticFiles(directory=os.path.join(base_path, "static"), html=True), name="static")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=HOST, port=PORT)
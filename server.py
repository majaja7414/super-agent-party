# -- coding: utf-8 --
import os
import sys
# åœ¨ç¨‹åºæœ€å¼€å§‹è®¾ç½®
if hasattr(sys, '_MEIPASS'):
    # æ‰“åŒ…åçš„ç¨‹åº
    os.environ['PYTHONPATH'] = sys._MEIPASS
    os.environ['PATH'] = sys._MEIPASS + os.pathsep + os.environ.get('PATH', '')

import asyncio
import copy
from functools import partial
import json
import random
import re
import shutil
import signal
from urllib.parse import urlparse
from fastapi import BackgroundTasks, FastAPI, File, HTTPException, UploadFile, WebSocket, Request
from fastapi_mcp import FastApiMCP
import logging
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
from pydantic import BaseModel
from fastapi import status
from fastapi.responses import JSONResponse, StreamingResponse
import uuid
import time
from typing import List, Dict,Optional
import shortuuid
from py.mcp_clients import McpClient
from contextlib import asynccontextmanager,suppress
import requests
import asyncio
from concurrent.futures import ThreadPoolExecutor

import argparse
from mem0 import Memory
from py.qq_bot_manager import QQBotManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
parser = argparse.ArgumentParser(description="Run the ASGI application server.")
parser.add_argument("--host", default="127.0.0.1", help="Host for the ASGI server, default is 127.0.0.1")
parser.add_argument("--port", type=int, default=3456, help="Port for the ASGI server, default is 3456")
args = parser.parse_args()
HOST = args.host
PORT = args.port

os.environ["no_proxy"] = "localhost,127.0.0.1"
local_timezone = None
settings = None
client = None
reasoner_client = None
mcp_client_list = {}
locales = {}
_TOOL_HOOKS = {}
cur_random = []
ALLOWED_EXTENSIONS = [
  # åŠå…¬æ–‡æ¡£
  'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'pdf', 'pages', 
  'numbers', 'key', 'rtf', 'odt',
  
  # ç¼–ç¨‹å¼€å‘
  'js', 'ts', 'py', 'java', 'c', 'cpp', 'h', 'hpp', 'go', 'rs',
  'swift', 'kt', 'dart', 'rb', 'php', 'html', 'css', 'scss', 'less',
  'vue', 'svelte', 'jsx', 'tsx', 'json', 'xml', 'yml', 'yaml', 
  'sql', 'sh',
  
  # æ•°æ®é…ç½®
  'csv', 'tsv', 'txt', 'md', 'log', 'conf', 'ini', 'env', 'toml'
]
ALLOWED_IMAGE_EXTENSIONS = ['png', 'jpg', 'jpeg', 'gif', 'webp', 'bmp']

from py.get_setting import load_settings,save_settings,base_path,configure_host_port,UPLOAD_FILES_DIR,AGENT_DIR,MEMORY_CACHE_DIR,KB_DIR
from py.llm_tool import get_image_base64,get_image_media_type


configure_host_port(args.host, args.port)

@asynccontextmanager
async def lifespan(app: FastAPI): 
    from py.get_setting import init_db
    await init_db()
    global settings, client, reasoner_client, mcp_client_list,local_timezone,logger,locales
    with open(base_path + "/config/locales.json", "r", encoding="utf-8") as f:
        locales = json.load(f)
    from tzlocal import get_localzone
    local_timezone = get_localzone()
    settings = await load_settings()
    if settings:
        client = AsyncOpenAI(api_key=settings['api_key'], base_url=settings['base_url'])
        reasoner_client = AsyncOpenAI(api_key=settings['reasoner']['api_key'],base_url=settings['reasoner']['base_url'])
    else:
        client = AsyncOpenAI()
        reasoner_client = AsyncOpenAI()
    mcp_init_tasks = []
    async def init_mcp_with_timeout(server_name, server_config):
        """å¸¦è¶…æ—¶å¤„ç†çš„å¼‚æ­¥åˆå§‹åŒ–å‡½æ•°"""
        try:
            mcp_client = McpClient()
            if not server_config['disabled']:
                await asyncio.wait_for(
                    mcp_client.initialize(server_name, server_config),
                    timeout=6
                )
            return server_name, mcp_client, None
        except asyncio.TimeoutError:
            logger.error(f"MCP client {server_name} initialization timeout")
            return server_name, None, "timeout"
        except Exception as e:
            logger.error(f"MCP client {server_name} initialization failed: {str(e)}")
            return server_name, None, "error"
    if settings:
        # åˆ›å»ºæ‰€æœ‰åˆå§‹åŒ–ä»»åŠ¡
        for server_name, server_config in settings['mcpServers'].items():
            task = asyncio.create_task(init_mcp_with_timeout(server_name, server_config))
            mcp_init_tasks.append(task)
        # ç«‹å³ç»§ç»­æ‰§è¡Œä¸ç­‰å¾…
        # é€šè¿‡å›è°ƒå¤„ç†ç»“æœ
        async def check_results():
            """åå°æ”¶é›†ä»»åŠ¡ç»“æœ"""
            for task in asyncio.as_completed(mcp_init_tasks):
                server_name, mcp_client, error = await task
                if error:
                    settings['mcpServers'][server_name]['disabled'] = True
                    settings['mcpServers'][server_name]['processingStatus'] = 'server_error'
                    mcp_client_list[server_name] = McpClient()
                    mcp_client_list[server_name].disabled = True
                else:
                    mcp_client_list[server_name] = mcp_client
            await save_settings(settings)  # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€ä¿å­˜
            await broadcast_settings_update(settings)  # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€å¹¿æ’­
        # åœ¨åå°è¿è¡Œç»“æœæ”¶é›†
        asyncio.create_task(check_results())
    yield
# WebSocketç«¯ç‚¹å¢åŠ è¿æ¥ç®¡ç†
active_connections = []
# æ–°å¢å¹¿æ’­å‡½æ•°
async def broadcast_settings_update(settings):
    """å‘æ‰€æœ‰WebSocketè¿æ¥æ¨é€é…ç½®æ›´æ–°"""
    for connection in active_connections:  # éœ€è¦ç»´æŠ¤å…¨å±€è¿æ¥åˆ—è¡¨
        try:
            await connection.send_json({
                "type": "settings",
                "data": settings  # ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„æœ€æ–°é…ç½®
            })
        except Exception as e:
            logger.error(f"Broadcast failed: {e}")

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def t(text: str) -> str:
    global locales
    settings = await load_settings()
    target_language = settings["systemSettings"]["language"]
    return locales[target_language].get(text, text)

async def get_image_content(image_url: str) -> str:
    import hashlib
    settings = await load_settings()
    base64_image = await get_image_base64(image_url)
    media_type = await get_image_media_type(image_url)
    url= f"data:{media_type};base64,{base64_image}"
    image_hash = hashlib.md5(image_url.encode()).hexdigest()
    content = ""
    if settings['vision']['enabled']:
        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt")):
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "r", encoding='utf-8') as f:
                content += f"\n\nå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
        else:
            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": url}}]
            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
            response = await client.chat.completions.create(
                model=settings['vision']['model'],
                messages = [{"role": "user", "content": images_content}],
                temperature=settings['vision']['temperature'],
            )
            content = f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "w", encoding='utf-8') as f:
                f.write(str(response.choices[0].message.content))
    else:           
        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt")):
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "r", encoding='utf-8') as f:
                content += f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
        else:
            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": url}}]
            client = AsyncOpenAI(api_key=settings['api_key'],base_url=settings['base_url'])
            response = await client.chat.completions.create(
                model=settings['model'],
                messages = [{"role": "user", "content": images_content}],
                temperature=settings['temperature'],
            )
            content = f"\n\nnå›¾ç‰‡(URL:{image_url} å“ˆå¸Œå€¼ï¼š{image_hash})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
            with open(os.path.join(UPLOAD_FILES_DIR, f"{image_hash}.txt"), "w", encoding='utf-8') as f:
                f.write(str(response.choices[0].message.content))
    return content

async def dispatch_tool(tool_name: str, tool_params: dict,settings: dict) -> str:
    global mcp_client_list,_TOOL_HOOKS
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        jina_crawler_async,
        Crawl4Ai_search_async, 
    )
    from py.know_base import query_knowledge_base
    from py.agent_tool import agent_tool_call
    from py.a2a_tool import a2a_tool_call
    from py.llm_tool import custom_llm_tool
    from py.pollinations import pollinations_image,openai_image,siliconflow_image
    from py.load_files import get_file_content
    from py.code_interpreter import e2b_code_async,local_run_code_async
    from py.custom_http import fetch_custom_http
    _TOOL_HOOKS = {
        "DDGsearch_async": DDGsearch_async,
        "searxng_async": searxng_async,
        "Tavily_search_async": Tavily_search_async,
        "query_knowledge_base": query_knowledge_base,
        "jina_crawler_async": jina_crawler_async,
        "Crawl4Ai_search_async": Crawl4Ai_search_async,
        "agent_tool_call": agent_tool_call,
        "a2a_tool_call": a2a_tool_call,
        "custom_llm_tool": custom_llm_tool,
        "pollinations_image":pollinations_image,
        "get_file_content":get_file_content,
        "get_image_content": get_image_content,
        "e2b_code_async": e2b_code_async,
        "local_run_code_async": local_run_code_async,
        "openai_image": openai_image,
        "siliconflow_image": siliconflow_image,
        "Bing_search_async": Bing_search_async,
        "Google_search_async": Google_search_async,
        "Brave_search_async": Brave_search_async,
        "Exa_search_async": Exa_search_async,
        "Serper_search_async": Serper_search_async,
        "bochaai_search_async": bochaai_search_async,
    }
    if "multi_tool_use." in tool_name:
        tool_name = tool_name.replace("multi_tool_use.", "")
    if "custom_http_" in tool_name:
        tool_name = tool_name.replace("custom_http_", "")
        print(tool_name)
        settings_custom_http = settings['custom_http']
        for custom in settings_custom_http:
            if custom['name'] == tool_name:
                tool_custom_http = custom
                break
        method = tool_custom_http['method']
        url = tool_custom_http['url']
        headers = tool_custom_http['headers']
        result = await fetch_custom_http(method, url, headers, tool_params)
        return str(result)
    if tool_name not in _TOOL_HOOKS:
        for server_name, mcp_client in mcp_client_list.items():
            if tool_name in mcp_client._conn.tools:
                result = await mcp_client.call_tool(tool_name, tool_params)
                return str(result.model_dump())
        return None
    tool_call = _TOOL_HOOKS[tool_name]
    try:
        ret_out = await tool_call(**tool_params)
        return ret_out
    except Exception as e:
        logger.error(f"Error calling tool {tool_name}: {e}")
        return f"Error calling tool {tool_name}: {e}"


class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str = None
    temperature: float = 0.7
    tools: dict = None
    stream: bool = False
    max_tokens: int = None
    top_p: float = 1
    frequency_penalty: float = 0
    presence_penalty: float = 0
    fileLinks: List[str] = None
    enable_thinking: bool = False
    enable_deep_research: bool = False
    enable_web_search: bool = False

async def message_without_images(messages: List[Dict]) -> List[Dict]:
    if messages:
        for message in messages:
            if 'content' in message:
                # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
                if isinstance(message['content'], list):
                    for item in message['content']:
                        if isinstance(item, dict) and item['type'] == 'text':
                            message['content'] = item['text']
                            break
    return messages

async def images_in_messages(messages: List[Dict],fastapi_base_url: str) -> List[Dict]:
    import hashlib
    images = []
    index = 0
    for message in messages:
        image_urls = []
        if 'content' in message:
            # message['content'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
            if isinstance(message['content'], list):
                for item in message['content']:
                    if isinstance(item, dict) and item['type'] == 'image_url':
                        # å¦‚æœitem["image_url"]["url"]æ˜¯httpæˆ–httpså¼€å¤´ï¼Œåˆ™è½¬æ¢æˆbase64
                        if item["image_url"]["url"].startswith("http"):
                            image_url = item["image_url"]["url"]
                            # å¯¹image_urlåˆ†è§£å‡ºbaseURLï¼Œä¸fastapi_base_urlæ¯”è¾ƒï¼Œå¦‚æœç›¸åŒï¼Œå°†image_urlçš„baseURLæ›¿æ¢æˆ127.0.0.1:PORT
                            if fastapi_base_url in image_url:
                                image_url = image_url.replace(fastapi_base_url, f"http://127.0.0.1:{PORT}/")
                            base64_image = await get_image_base64(image_url)
                            media_type = await get_image_media_type(image_url)
                            item["image_url"]["url"] = f"data:{media_type};base64,{base64_image}"
                            item["image_url"]["hash"] = hashlib.md5(item["image_url"]["url"].encode()).hexdigest()
                        image_urls.append(item)
        if image_urls:
            images.append({'index': index, 'images': image_urls})
        index += 1
    return images

async def images_add_in_messages(request_messages: List[Dict], images: List[Dict], settings: dict) -> List[Dict]:
    messages=copy.deepcopy(request_messages)
    if settings['vision']['enabled']:
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            images_content = [{"type": "text", "text": "è¯·ä»”ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ï¼ŒåŒ…å«å›¾ç‰‡ä¸­å¯èƒ½å­˜åœ¨çš„æ–‡å­—ã€æ•°å­—ã€é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"},{"type": "image_url", "image_url": {"url": item['image_url']['url']}}]
                            client = AsyncOpenAI(api_key=settings['vision']['api_key'],base_url=settings['vision']['base_url'])
                            response = await client.chat.completions.create(
                                model=settings['vision']['model'],
                                messages = [{"role": "user", "content": images_content}],
                                temperature=settings['vision']['temperature'],
                            )
                            messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(response.choices[0].message.content)+"\n\n"
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "w", encoding='utf-8') as f:
                                f.write(str(response.choices[0].message.content))
    else:           
        for image in images:
            index = image['index']
            if index < len(messages):
                if 'content' in messages[index]:
                    for item in image['images']:
                        # å¦‚æœuploaded_files/{item['image_url']['hash']}.txtå­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¦åˆ™è°ƒç”¨vision api
                        if os.path.exists(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt")):
                            with open(os.path.join(UPLOAD_FILES_DIR, f"{item['image_url']['hash']}.txt"), "r", encoding='utf-8') as f:
                                messages[index]['content'] += f"\n\nç”¨æˆ·å‘é€çš„å›¾ç‰‡(å“ˆå¸Œå€¼ï¼š{item['image_url']['hash']})ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"+str(f.read())+"\n\n"
                        else:
                            messages[index]['content'] = [{"type": "text", "text": messages[index]['content']}]
                            messages[index]['content'].append({"type": "image_url", "image_url": {"url": item['image_url']['url']}})
    return messages

async def tools_change_messages(request: ChatRequest, settings: dict):
    if settings['tools']['time']['enabled']:
        time_message = f"æ¶ˆæ¯å‘é€æ—¶é—´ï¼š{local_timezone}  {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n\n"
        request.messages[-1]['content'] = time_message + request.messages[-1]['content']
    if settings['tools']['inference']['enabled']:
        inference_message = "å›ç­”ç”¨æˆ·å‰è¯·å…ˆæ€è€ƒæ¨ç†ï¼Œå†å›ç­”é—®é¢˜ï¼Œä½ çš„æ€è€ƒæ¨ç†çš„è¿‡ç¨‹å¿…é¡»æ”¾åœ¨<think>ä¸</think>ä¹‹é—´ã€‚\n\n"
        request.messages[-1]['content'] = f"{inference_message}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
    if settings['tools']['formula']['enabled']:
        latex_message = "\n\nå½“ä½ æƒ³ä½¿ç”¨latexå…¬å¼æ—¶ï¼Œä½ å¿…é¡»æ˜¯ç”¨ ['$', '$'] ä½œä¸ºè¡Œå†…å…¬å¼å®šç•Œç¬¦ï¼Œä»¥åŠ ['$$', '$$'] ä½œä¸ºè¡Œé—´å…¬å¼å®šç•Œç¬¦ã€‚\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += latex_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': latex_message})
    if settings['tools']['language']['enabled']:
        language_message = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€æ¨ç†åˆ†ææ€è€ƒï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨ç†åˆ†æï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += language_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': language_message})
    return request

def get_drs_stage(DRS_STAGE):
    if DRS_STAGE == 1:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºæ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µï¼Œä½ éœ€è¦åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶ç»™å‡ºæ˜ç¡®çš„éœ€æ±‚æè¿°ã€‚å¦‚æœç”¨æˆ·çš„éœ€æ±‚æè¿°ä¸æ˜ç¡®ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
    elif DRS_STAGE == 2:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºæŸ¥è¯¢æœç´¢é˜¶æ®µï¼Œåˆ©ç”¨ä½ çš„çŸ¥è¯†åº“ã€äº’è”ç½‘æœç´¢ã€æ•°æ®åº“æŸ¥è¯¢å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼Œè¿™äº›å·¥å…·ä¸ä¸€å®šä¼šæä¾›ï¼‰ï¼ŒæŸ¥è¯¢å®Œæˆä»»åŠ¡æ‰€éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ã€‚"
    elif DRS_STAGE == 3:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºç”Ÿæˆç»“æœé˜¶æ®µï¼Œæ ¹æ®å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®Œæˆä»»åŠ¡ï¼Œç”Ÿæˆå›ç­”ã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä½ ç”Ÿæˆä¸€ä¸ªè¶…è¿‡2000å­—çš„å›ç­”ï¼Œä½ å¯ä»¥å°è¯•å°†è¯¥ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯æ¬¡åªå®Œæˆå…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚"
    else:
        drs_msg = "å½“å‰é˜¶æ®µä¸ºç”Ÿæˆç»“æœé˜¶æ®µï¼Œæ ¹æ®å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®Œæˆä»»åŠ¡ï¼Œç”Ÿæˆå›ç­”ã€‚å¦‚æœç”¨æˆ·è¦æ±‚ä½ ç”Ÿæˆä¸€ä¸ªè¶…è¿‡2000å­—çš„å›ç­”ï¼Œä½ å¯ä»¥å°è¯•å°†è¯¥ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯æ¬¡åªå®Œæˆå…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚"
    return drs_msg  

def get_drs_stage_name(DRS_STAGE):
    if DRS_STAGE == 1:
        drs_stage_name = "æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ"
    elif DRS_STAGE == 2:
        drs_stage_name = "æŸ¥è¯¢æœç´¢é˜¶æ®µ"
    elif DRS_STAGE == 3:
        drs_stage_name = "ç”Ÿæˆç»“æœé˜¶æ®µ"
    else:
        drs_stage_name = "ç”Ÿæˆç»“æœé˜¶æ®µ"
    return drs_stage_name

def get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content):
    drs_stage_name = get_drs_stage_name(DRS_STAGE)
    if DRS_STAGE == 1:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

### å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}

### å¦‚æœä¸éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®éœ€æ±‚ï¼Œè¿›å…¥å¹¶è¿›å…¥æŸ¥è¯¢æœç´¢é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "search",
    "unfinished_task": ""
}}
"""
    elif DRS_STAGE == 2:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

### å¦‚æœéœ€è¦ç»§ç»­æŸ¥è¯¢ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_search",
    "unfinished_task": "è¿™é‡Œå¡«å…¥ç»§ç»­æŸ¥è¯¢çš„ä¿¡æ¯"
}}

### å¦‚æœä¸éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®éœ€æ±‚ï¼Œè¿›å…¥å¹¶è¿›å…¥æŸ¥è¯¢æœç´¢é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "answer",
    "unfinished_task": ""
}}
"""    
    else:
        search_prompt = f"""
# å½“å‰çŠ¶æ€ï¼š

## åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

## å½“å‰ç»“æœï¼š
{full_content}

## å½“å‰é˜¶æ®µï¼š
{drs_stage_name}

# æ·±åº¦ç ”ç©¶ä¸€å…±æœ‰ä¸‰ä¸ªé˜¶æ®µï¼š1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ

## å½“å‰é˜¶æ®µï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š

å¦‚æœåˆå§‹ä»»åŠ¡å·²å®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœåˆå§‹ä»»åŠ¡æœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}
"""    
    return search_prompt

async def generate_stream_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search):
    global mcp_client_list
    DRS_STAGE = 1 # 1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ
    images = await images_in_messages(request.messages,fastapi_base_url)
    request.messages = await message_without_images(request.messages)
    from py.load_files import get_files_content,file_tool,image_tool
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        bing_tool,
        google_tool,
        brave_tool,
        exa_tool,
        serper_tool,
        bochaai_tool,
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base,rerank_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool,openai_image_tool,siliconflow_image_tool
    from py.code_interpreter import e2b_code_tool,local_run_code_tool
    m0 = None
    memoryId = None
    if settings["memorySettings"]["is_memory"]:
        memoryId = settings["memorySettings"]["selectedMemory"]
        cur_memory = None
        for memory in settings["memories"]:
            if memory["id"] == memoryId:
                cur_memory = memory
                break
        if cur_memory:
            
            config={
                "embedder": {
                    "provider": 'openai',
                    "config": {
                        "model": cur_memory['model'],
                        "api_key": cur_memory['api_key'],
                        "openai_base_url":cur_memory["base_url"],
                        "embedding_dims":1024
                    },
                },
                "llm": {
                    "provider": 'openai',
                    "config": {
                        "model": settings['model'],
                        "api_key": settings['api_key'],
                        "openai_base_url":settings["base_url"]
                    }
                },
                "vector_store": {
                    "provider": "faiss",
                    "config": {
                        "collection_name": "agent-party",
                        "path": os.path.join(MEMORY_CACHE_DIR,memoryId),
                        "distance_strategy": "euclidean",
                        "embedding_model_dims": 1024
                    }
                }
            }
            m0 = Memory.from_config(config)
    open_tag = "<think>"
    close_tag = "</think>"
    try:
        tools = request.tools or []
        if mcp_client_list:
            for server_name, mcp_client in mcp_client_list.items():
                if server_name in settings['mcpServers']:
                    if 'disabled' not in settings['mcpServers'][server_name]:
                        settings['mcpServers'][server_name]['disabled'] = False
                    if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                        function = await mcp_client.get_openai_functions()
                        if function:
                            tools.extend(function)
        get_llm_tool_fuction = await get_llm_tool(settings)
        if get_llm_tool_fuction:
            tools.append(get_llm_tool_fuction)
        get_agent_tool_fuction = await get_agent_tool(settings)
        if get_agent_tool_fuction:
            tools.append(get_agent_tool_fuction)
        get_a2a_tool_fuction = await get_a2a_tool(settings)
        if get_a2a_tool_fuction:
            tools.append(get_a2a_tool_fuction)
        if settings['text2imgSettings']['enabled']:
            if settings['text2imgSettings']['engine'] == 'pollinations':
                tools.append(pollinations_image_tool)
            elif settings['text2imgSettings']['engine'] == 'openai':
                if settings['text2imgSettings']['vendor'] == 'siliconflow':
                    tools.append(siliconflow_image_tool)
                else:
                    tools.append(openai_image_tool)
        if settings['tools']['getFile']['enabled']:
            tools.append(file_tool)
            tools.append(image_tool)
        if settings["codeSettings"]['enabled']:
            if settings["codeSettings"]["engine"] == "e2b":
                tools.append(e2b_code_tool)
            elif settings["codeSettings"]["engine"] == "sandbox":
                tools.append(local_run_code_tool)
        if settings["custom_http"]:
            for custom_http in settings["custom_http"]:
                if custom_http["enabled"]:
                    if custom_http['body'] == "":
                        custom_http['body'] = "{}"
                    custom_http_tool = {
                        "type": "function",
                        "function": {
                            "name": f"custom_http_{custom_http['name']}",
                            "description": f"{custom_http['description']}",
                            "parameters": json.loads(custom_http['body']),
                        },
                    }
                    tools.append(custom_http_tool)
        print(tools)
        source_prompt = ""
        if request.fileLinks:
            print("fileLinks",request.fileLinks)
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            fileLinks_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += fileLinks_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': fileLinks_message})
            source_prompt += fileLinks_message
        user_prompt = request.messages[-1]['content']
        if m0:
            lore_content = ""
            assistant_reply = ""
            # æ‰¾å‡ºrequest.messagesä¸­ä¸Šæ¬¡çš„assistantå›å¤
            for i in range(len(request.messages)-1, -1, -1):
                if request.messages[i]['role'] == 'assistant':
                    assistant_reply = request.messages[i]['content']
                    break
            if cur_memory["lorebook"]:
                for lore in cur_memory["lorebook"]:
                    if lore["name"] != "" and (lore["name"] in user_prompt or lore["name"] in assistant_reply):
                        lore_content = lore_content + "\n\n" + f"{lore['name']}ï¼š{lore['value']}"
            global cur_random 
            # å¦‚æœrequest.messagesä¸­ä¸åŒ…å«assistantå›å¤ï¼Œè¯´æ˜æ˜¯é¦–æ¬¡æé—®ï¼Œè§¦å‘éšæœºè®¾å®š
            if not assistant_reply:
                # å¦‚æœ cur_memory ä¸­æœ‰ random æ¡ç›®
                if cur_memory.get("random") and len(cur_memory["random"]) > 0:
                    # éšæœºé€‰æ‹©ä¸€ä¸ª random æ¡ç›®
                    random_entry = random.choice(cur_memory["random"])
                    if random_entry.get("value"):
                        lore_content = lore_content + "\n\n" + f"{random_entry['value']}"
                        cur_random.append({"id":memoryId,"value":random_entry["value"]})
                        print("æ–°éšæœºè®¾å®šï¼š",{"id":memoryId,"value":random_entry["value"]})
            else:
                for item in cur_random:
                    if item["id"] == memoryId:
                        lore_content = lore_content + "\n\n" + f"{item['value']}"
                        print("æ²¿ç”¨éšæœºè®¾å®šï¼š",{"id":memoryId,"value":item['value']})  
                        break 
            memoryLimit = settings["memorySettings"]["memoryLimit"]
            try:
                relevant_memories = m0.search(query=user_prompt, user_id=memoryId, limit=memoryLimit)
                relevant_memories = json.dumps(relevant_memories, ensure_ascii=False)
            except Exception as e:
                print("m0.search error:",e)
                relevant_memories = ""
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"
            else:
                request.messages.insert(0, {'role': 'system', 'content': "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"})
            if cur_memory["basic_character"]:
                print("æ·»åŠ è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"})
            if lore_content:
                print("æ·»åŠ ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"})
        request = await tools_change_messages(request, settings)
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        async def stream_generator(user_prompt,DRS_STAGE):
            kb_list = []
            if settings["knowledgeBases"]:
                for kb in settings["knowledgeBases"]:
                    if kb["enabled"] and kb["processingStatus"] == "completed":
                        kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
            if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
                if kb_list:
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": f"{await t("KB_search")}\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    all_kb_content = []
                    # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                    for kb in kb_list:
                        kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                        all_kb_content.extend(kb_content)
                        if settings["KBSettings"]["is_rerank"]:
                            all_kb_content = await rerank_knowledge_base(user_prompt,all_kb_content)
                    if all_kb_content:
                        all_kb_content = json.dumps(all_kb_content, ensure_ascii=False, indent=4)
                        kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                        request.messages[-1]['content'] += f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}"
                                                # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥UPLOAD_FILES_DIRæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                            f.write(str(all_kb_content))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[{await t("search_result")}]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
            if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
                if kb_list:
                    kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                    print(kb_list_message)
                    if request.messages and request.messages[0]['role'] == 'system':
                        request.messages[0]['content'] += kb_list_message
                    else:
                        request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
            else:
                kb_list = []
            if settings['webSearch']['enabled'] or enable_web_search:
                if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": f"{await t("web_search")}\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        results = await DDGsearch_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'searxng':
                        results = await searxng_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'tavily':
                        results = await Tavily_search_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'bing':
                        results = await Bing_search_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'google':
                        results = await Google_search_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'brave':
                        results = await Brave_search_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'exa':
                        results = await Exa_search_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'serper':
                        results = await Serper_search_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'bochaai':
                        results = await bochaai_search_async(user_prompt)
                    if results:
                        request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}\n\nè¯·æ ¹æ®è”ç½‘æœç´¢ç»“æœç»„ç»‡ä½ çš„å›ç­”ï¼Œå¹¶ç¡®ä¿ä½ çš„å›ç­”æ˜¯å‡†ç¡®çš„ã€‚"
                        # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                            f.write(str(results))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[{await t("search_result")}]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
                if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        tools.append(duckduckgo_tool)
                    elif settings['webSearch']['engine'] == 'searxng':
                        tools.append(searxng_tool)
                    elif settings['webSearch']['engine'] == 'tavily':
                        tools.append(tavily_tool)
                    elif settings['webSearch']['engine'] == 'bing':
                        tools.append(bing_tool)
                    elif settings['webSearch']['engine'] == 'google':
                        tools.append(google_tool)
                    elif settings['webSearch']['engine'] == 'brave':
                        tools.append(brave_tool)
                    elif settings['webSearch']['engine'] == 'exa':
                        tools.append(exa_tool)
                    elif settings['webSearch']['crawler'] == 'serper':
                        tools.append(serper_tool)
                    elif settings['webSearch']['crawler'] == 'bochaai':
                        tools.append(bochaai_tool)

                    if settings['webSearch']['crawler'] == 'jina':
                        tools.append(jina_crawler_tool)
                    elif settings['webSearch']['crawler'] == 'crawl4ai':
                        tools.append(Crawl4Ai_tool)
            if kb_list:
                tools.append(kb_tool)
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                deepsearch_messages = copy.deepcopy(request.messages)
                deepsearch_messages[-1]['content'] += "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚"
                print(request.messages[-1]['content'])
                response = await client.chat.completions.create(
                    model=model,
                    messages=deepsearch_messages,
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                user_prompt = response.choices[0].message.content
                deepsearch_chunk = {
                    "choices": [{
                        "delta": {
                            "reasoning_content": f"\n\nğŸ’–{await t("start_task")}{user_prompt}\n\n",
                        }
                    }]
                }
                yield f"data: {json.dumps(deepsearch_chunk)}\n\n"
                request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
                print(request.messages[-1]['content'])
            # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
            if settings['reasoner']['enabled'] or enable_thinking:
                reasoner_messages = copy.deepcopy(request.messages)
                if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    reasoner_messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
                    drs_msg = get_drs_stage(DRS_STAGE)
                    if drs_msg:
                        reasoner_messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=True,
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                    in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                    
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue
                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            current_content = delta.get("content", "")
                            buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                            
                            # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                            while True:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                        non_reasoning = buffer[:start_pos]
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                else:
                                    # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                        reasoning_part = buffer[:end_pos]
                                        chunk_dict["choices"][0]["delta"] = {
                                            "reasoning_content": reasoning_part,
                                            "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                        }
                                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                                        full_reasoning += reasoning_part
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                        if buffer:
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": buffer,
                                                "content": ""
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += buffer
                                            buffer = ""
                                        break  # ç­‰å¾…æ›´å¤šå†…å®¹
                else:
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=True,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue

                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            reasoning_content = delta.get("reasoning_content", "")
                            if reasoning_content:
                                full_reasoning += reasoning_content
                        yield f"data: {json.dumps(chunk_dict)}\n\n"

                # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
            # çŠ¶æ€è·Ÿè¸ªå˜é‡
            in_reasoning = False
            reasoning_buffer = []
            content_buffer = []
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
                drs_msg = get_drs_stage(DRS_STAGE)
                if drs_msg:
                    request.messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            tool_calls = []
            full_content = ""
            search_not_done = False
            search_task = ""
            async for chunk in response:
                if not chunk.choices:
                    continue
                choice = chunk.choices[0]
                if choice.delta.tool_calls:  # function_calling
                    for idx, tool_call in enumerate(choice.delta.tool_calls):
                        tool = choice.delta.tool_calls[idx]
                        if len(tool_calls) <= idx:
                            tool_calls.append(tool)
                            continue
                        if tool.function.arguments:
                            # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                            if tool_calls[idx].function.arguments:
                                tool_calls[idx].function.arguments += tool.function.arguments
                            else:
                                tool_calls[idx].function.arguments = tool.function.arguments
                else:
                    # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                    chunk_dict = chunk.model_dump()
                    delta = chunk_dict["choices"][0]["delta"]
                    
                    # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                    delta.setdefault("content", "")
                    delta.setdefault("reasoning_content", "")
                    
                    # ä¼˜å…ˆå¤„ç† reasoning_content
                    if delta["reasoning_content"]:
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        continue

                    # å¤„ç†å†…å®¹
                    current_content = delta["content"]
                    buffer = current_content
                    
                    while buffer:
                        if not in_reasoning:
                            # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                            start_pos = buffer.find(open_tag)
                            if start_pos != -1:
                                # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                content_buffer.append(buffer[:start_pos])
                                buffer = buffer[start_pos+len(open_tag):]
                                in_reasoning = True
                            else:
                                content_buffer.append(buffer)
                                buffer = ""
                        else:
                            # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                            end_pos = buffer.find(close_tag)
                            if end_pos != -1:
                                # å¤„ç†æ€è€ƒå†…å®¹
                                reasoning_buffer.append(buffer[:end_pos])
                                buffer = buffer[end_pos+len(close_tag):]
                                in_reasoning = False
                            else:
                                reasoning_buffer.append(buffer)
                                buffer = ""
                    
                    # æ„é€ æ–°çš„deltaå†…å®¹
                    new_content = "".join(content_buffer)
                    new_reasoning = "".join(reasoning_buffer)
                    
                    # æ›´æ–°chunkå†…å®¹
                    delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                    delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                    
                    # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                    if in_reasoning:
                        content_buffer = [new_content.split(open_tag)[-1]] 
                    else:
                        content_buffer = []
                    reasoning_buffer = []
                    
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    full_content += delta.get("content", "")
            # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
            if content_buffer or reasoning_buffer:
                final_chunk = {
                    "choices": [{
                        "delta": {
                            "content": "".join(content_buffer),
                            "reasoning_content": "".join(reasoning_buffer)
                        }
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                full_content += final_chunk["choices"][0]["delta"].get("content", "")
            if tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content)
                response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "system",
                        "content": source_prompt,
                        },
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                try:
                    response_content = json.loads(response_content)
                except json.JSONDecodeError:
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâŒ{await t("task_error")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                if response_content["status"] == "done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâœ…{await t("task_done")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ{await t("task_not_done")}\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    DRS_STAGE = 2
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ“{await t("task_need_more_info")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
                elif response_content["status"] == "search":
                    DRS_STAGE = 2
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nğŸ”{await t("enter_search_stage")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                elif response_content["status"] == "need_more_search":
                    DRS_STAGE = 2
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nğŸ”{await t("need_more_search")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "answer":
                    DRS_STAGE = 3
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\nâ­{await t("enter_answer_stage")}\n\n"
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                print("DRS_STAGE:", DRS_STAGE)
            reasoner_messages = copy.deepcopy(request.messages)
            while tool_calls or search_not_done:
                full_content = ""
                if tool_calls:
                    response_content = tool_calls[0].function
                    if response_content.name in  ["DDGsearch_async","searxng_async", "Bing_search_async", "Google_search_async", "Brave_search_async", "Exa_search_async", "Serper_search_async","bochaai_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("web_search")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in  ["jina_crawler_async","Crawl4Ai_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("web_search_more")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in ["query_knowledge_base"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("knowledge_base")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    else:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\n{await t("call")}{response_content.name}{await t("tool")}\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    print(response_content.arguments)
                    modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                    print(modified_data)
                    # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                    data_list = json.loads(modified_data)
                    results = await dispatch_tool(response_content.name, data_list[0],settings)
                    print(results)
                    if results is None:
                        chunk = {
                            "id": "extra_tools",
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "tool_calls":modified_data,
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        break
                    if response_content.name in ["query_knowledge_base"]:
                        if settings["KBSettings"]["is_rerank"]:
                            results = await rerank_knowledge_base(user_prompt,results)
                        results = json.dumps(results, ensure_ascii=False, indent=4)
                    request.messages.append(
                        {
                            "tool_calls": [
                                {
                                    "id": tool_calls[0].id,
                                    "function": {
                                        "arguments": json.dumps(data_list[0]),
                                        "name": response_content.name,
                                    },
                                    "type": tool_calls[0].type,
                                }
                            ],
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    request.messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_calls[0].id,
                            "name": response_content.name,
                            "content": str(results),
                        }
                    )
                    if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                        request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
                    reasoner_messages.append(
                        {
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    reasoner_messages.append(
                        {
                            "role": "user",
                            "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                        }
                    )
                    # è·å–æ—¶é—´æˆ³å’Œuuid
                    timestamp = time.time()
                    uid = str(uuid.uuid4())
                    # æ„é€ æ–‡ä»¶å
                    filename = f"{timestamp}_{uid}.txt"
                    # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                    with open(os.path.join(UPLOAD_FILES_DIR, filename), "w", encoding='utf-8') as f:
                        f.write(str(results))            
                    # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                    fileLink=f"{fastapi_base_url}uploaded_files/{filename}"
                    tool_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\n[{response_content.name}{await t("tool_result")}]({fileLink})\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(tool_chunk)}\n\n"
                # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
                if settings['reasoner']['enabled'] or enable_thinking:
                    if tools:
                        reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                    for modelProvider in settings['modelProviders']: 
                        if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                            vendor = modelProvider['vendor']
                            break
                    msg = await images_add_in_messages(reasoner_messages, images,settings)
                    if vendor == 'Ollama':
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            temperature=settings['reasoner']['temperature']
                        )
                        full_reasoning = ""
                        buffer = ""  # è·¨chunkçš„å†…å®¹ç¼“å†²åŒº
                        in_reasoning = False  # æ˜¯å¦åœ¨æ ‡ç­¾å†…
                        
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                current_content = delta.get("content", "")
                                buffer += current_content  # ç´¯ç§¯åˆ°ç¼“å†²åŒº
                                
                                # å®æ—¶å¤„ç†ç¼“å†²åŒºå†…å®¹
                                while True:
                                    if not in_reasoning:
                                        # å¯»æ‰¾å¼€æ”¾æ ‡ç­¾
                                        start_pos = buffer.find(open_tag)
                                        if start_pos != -1:
                                            # å¼€æ”¾æ ‡ç­¾å‰çš„å†…å®¹ï¼ˆéæ€è€ƒå†…å®¹ï¼‰
                                            non_reasoning = buffer[:start_pos]
                                            buffer = buffer[start_pos+len(open_tag):]
                                            in_reasoning = True
                                        else:
                                            break  # æ— å¼€æ”¾æ ‡ç­¾ï¼Œä¿ç•™åç»­å¤„ç†
                                    else:
                                        # å¯»æ‰¾é—­åˆæ ‡ç­¾
                                        end_pos = buffer.find(close_tag)
                                        if end_pos != -1:
                                            # æå–æ€è€ƒå†…å®¹å¹¶æ„é€ å“åº”
                                            reasoning_part = buffer[:end_pos]
                                            chunk_dict["choices"][0]["delta"] = {
                                                "reasoning_content": reasoning_part,
                                                "content": ""  # æ¸…é™¤éæ€è€ƒå†…å®¹
                                            }
                                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                                            full_reasoning += reasoning_part
                                            buffer = buffer[end_pos+len(close_tag):]
                                            in_reasoning = False
                                        else:
                                            # å‘é€æœªé—­åˆçš„ä¸­é—´å†…å®¹
                                            if buffer:
                                                chunk_dict["choices"][0]["delta"] = {
                                                    "reasoning_content": buffer,
                                                    "content": ""
                                                }
                                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                                full_reasoning += buffer
                                                buffer = ""
                                            break  # ç­‰å¾…æ›´å¤šå†…å®¹
                    else:
                        # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                        reasoner_stream = await reasoner_client.chat.completions.create(
                            model=settings['reasoner']['model'],
                            messages=msg,
                            stream=True,
                            max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                            temperature=settings['reasoner']['temperature']
                        )
                        full_reasoning = ""
                        # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                        async for chunk in reasoner_stream:
                            if not chunk.choices:
                                continue

                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0].get("delta", {})
                            if delta:
                                reasoning_content = delta.get("reasoning_content", "")
                                if reasoning_content:
                                    full_reasoning += reasoning_content
                            yield f"data: {json.dumps(chunk_dict)}\n\n"

                    # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                    request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
                msg = await images_add_in_messages(request.messages, images,settings)
                if tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        tools=tools,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p or settings['top_p'],
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                        temperature=request.temperature,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p or settings['top_p'],
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                tool_calls = []
                async for chunk in response:
                    if not chunk.choices:
                        continue
                    if chunk.choices:
                        choice = chunk.choices[0]
                        if choice.delta.tool_calls:  # function_calling
                            for idx, tool_call in enumerate(choice.delta.tool_calls):
                                tool = choice.delta.tool_calls[idx]
                                if len(tool_calls) <= idx:
                                    tool_calls.append(tool)
                                    continue
                                if tool.function.arguments:
                                    # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                                    if tool_calls[idx].function.arguments:
                                        tool_calls[idx].function.arguments += tool.function.arguments
                                    else:
                                        tool_calls[idx].function.arguments = tool.function.arguments
                        else:
                            # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0]["delta"]
                            
                            # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                            delta.setdefault("content", "")
                            delta.setdefault("reasoning_content", "")

                             # ä¼˜å…ˆå¤„ç† reasoning_content
                            if delta["reasoning_content"]:
                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                continue
                            
                            # å¤„ç†å†…å®¹
                            current_content = delta["content"]
                            buffer = current_content
                            
                            while buffer:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                        content_buffer.append(buffer[:start_pos])
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        content_buffer.append(buffer)
                                        buffer = ""
                                else:
                                    # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # å¤„ç†æ€è€ƒå†…å®¹
                                        reasoning_buffer.append(buffer[:end_pos])
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        reasoning_buffer.append(buffer)
                                        buffer = ""
                            
                            # æ„é€ æ–°çš„deltaå†…å®¹
                            new_content = "".join(content_buffer)
                            new_reasoning = "".join(reasoning_buffer)
                            
                            # æ›´æ–°chunkå†…å®¹
                            delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                            delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                            
                            # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                            if in_reasoning:
                                content_buffer = [new_content.split(open_tag)[-1]] 
                            else:
                                content_buffer = []
                            reasoning_buffer = []
                            
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            full_content += delta.get("content", "")
                # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
                if content_buffer or reasoning_buffer:
                    final_chunk = {
                        "choices": [{
                            "delta": {
                                "content": "".join(content_buffer),
                                "reasoning_content": "".join(reasoning_buffer)
                            }
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                    full_content += final_chunk["choices"][0]["delta"].get("content", "")
                if tool_calls:
                    pass
                elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                    search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,full_content)
                    response = await client.chat.completions.create(
                        model=model,
                        messages=[                        
                            {
                            "role": "system",
                            "content": source_prompt,
                            },
                            {
                            "role": "user",
                            "content": search_prompt,
                            }
                        ],
                        temperature=0.5,
                        extra_body = extra_params, # å…¶ä»–å‚æ•°
                    )
                    response_content = response.choices[0].message.content
                    # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                    if "```json" in response_content:
                        try:
                            response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                        except:
                            # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                            response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                    try:
                        response_content = json.loads(response_content)
                    except json.JSONDecodeError:
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâŒ{await t("task_error")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                    if response_content["status"] == "done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâœ…{await t("task_done")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "not_done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ{await t("task_not_done")}\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "need_more_info":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ“{await t("task_need_more_info")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "search":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nğŸ”{await t("enter_search_stage")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        drs_msg = get_drs_stage(DRS_STAGE)
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": drs_msg,
                            }
                        )
                    elif response_content["status"] == "need_more_search":
                        DRS_STAGE = 2
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nğŸ”{await t("need_more_search")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "answer":
                        DRS_STAGE = 3
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\nâ­{await t("enter_answer_stage")}\n\n"
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        drs_msg = get_drs_stage(DRS_STAGE)
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": drs_msg,
                            }
                        )
                    print("DRS_STAGE:", DRS_STAGE)
            yield "data: [DONE]\n\n"
            if m0:
                messages=[
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                    {
                        "role": "assistant",
                        "content": full_content,
                    }
                ]
                executor = ThreadPoolExecutor()
                async def add_async():
                    loop = asyncio.get_event_loop()
                    # ç»‘å®š user_id å…³é”®å­—å‚æ•°
                    func = partial(m0.add, user_id=memoryId)
                    # ä¼ é€’ messages ä½œä¸ºä½ç½®å‚æ•°
                    await loop.run_in_executor(executor, func, messages)
                    print("çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")

                asyncio.create_task(add_async())
                print("çŸ¥è¯†åº“æ›´æ–°ä»»åŠ¡å·²æäº¤")
            return
        
        return StreamingResponse(
            stream_generator(user_prompt, DRS_STAGE),
            media_type="text/event-stream",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    except Exception as e:
        # å¦‚æœe.status_codeå­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å®ƒä½œä¸ºHTTPçŠ¶æ€ç ï¼Œå¦åˆ™ä½¿ç”¨500
        return JSONResponse(
            status_code=getattr(e, "status_code", 500),
            content={"error": str(e)},
        )

async def generate_complete_response(client,reasoner_client, request: ChatRequest, settings: dict,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search):
    global mcp_client_list
    DRS_STAGE = 1 # 1: æ˜ç¡®ç”¨æˆ·éœ€æ±‚é˜¶æ®µ 2: æŸ¥è¯¢æœç´¢é˜¶æ®µ 3: ç”Ÿæˆç»“æœé˜¶æ®µ
    from py.load_files import get_files_content,file_tool,image_tool
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        Bing_search_async,
        Google_search_async,
        Brave_search_async,
        Exa_search_async,
        Serper_search_async,
        bochaai_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        bing_tool,
        google_tool,
        brave_tool,
        exa_tool,
        serper_tool,
        bochaai_tool,
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool,query_knowledge_base,rerank_knowledge_base
    from py.agent_tool import get_agent_tool
    from py.a2a_tool import get_a2a_tool
    from py.llm_tool import get_llm_tool
    from py.pollinations import pollinations_image_tool,openai_image_tool,siliconflow_image_tool
    from py.code_interpreter import e2b_code_tool,local_run_code_tool
    m0 = None
    if settings["memorySettings"]["is_memory"]:
        memoryId = settings["memorySettings"]["selectedMemory"]
        cur_memory = None
        for memory in settings["memories"]:
            if memory["id"] == memoryId:
                cur_memory = memory
                break
        if cur_memory:

            config={
                "embedder": {
                    "provider": 'openai',
                    "config": {
                        "model": cur_memory['model'],
                        "api_key": cur_memory['api_key'],
                        "openai_base_url":cur_memory["base_url"],
                        "embedding_dims":1024
                    },
                },
                "llm": {
                    "provider": 'openai',
                    "config": {
                        "model": settings['model'],
                        "api_key": settings['api_key'],
                        "openai_base_url":settings["base_url"]
                    }
                },
                "vector_store": {
                    "provider": "faiss",
                    "config": {
                        "collection_name": "agent-party",
                        "path": os.path.join(MEMORY_CACHE_DIR,memoryId),
                        "distance_strategy": "euclidean",
                        "embedding_model_dims": 1024
                    }
                }
            }
            m0 = Memory.from_config(config)
    images = await images_in_messages(request.messages,fastapi_base_url)
    request.messages = await message_without_images(request.messages)
    open_tag = "<think>"
    close_tag = "</think>"
    tools = request.tools or []
    tools = request.tools or []
    if mcp_client_list:
        for server_name, mcp_client in mcp_client_list.items():
            if server_name in settings['mcpServers']:
                if 'disabled' not in settings['mcpServers'][server_name]:
                    settings['mcpServers'][server_name]['disabled'] = False
                if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                    function = await mcp_client.get_openai_functions()
                    if function:
                        tools.extend(function)
    get_llm_tool_fuction = await get_llm_tool(settings)
    if get_llm_tool_fuction:
        tools.append(get_llm_tool_fuction)
    get_agent_tool_fuction = await get_agent_tool(settings)
    if get_agent_tool_fuction:
        tools.append(get_agent_tool_fuction)
    get_a2a_tool_fuction = await get_a2a_tool(settings)
    if get_a2a_tool_fuction:
        tools.append(get_a2a_tool_fuction)
    if settings['text2imgSettings']['enabled']:
        if settings['text2imgSettings']['engine'] == 'pollinations':
            tools.append(pollinations_image_tool)
        elif settings['text2imgSettings']['engine'] == 'openai':
            if settings['text2imgSettings']['vendor'] == 'siliconflow':
                tools.append(siliconflow_image_tool)
            else:
                tools.append(openai_image_tool)
    if settings['tools']['getFile']['enabled']:
        tools.append(file_tool)
        tools.append(image_tool)
    if settings["codeSettings"]['enabled']:
        if settings["codeSettings"]["engine"] == "e2b":
            tools.append(e2b_code_tool)
        elif settings["codeSettings"]["engine"] == "sandbox":
            tools.append(local_run_code_tool)
    if settings["custom_http"]:
        for custom_http in settings["custom_http"]:
            if custom_http["enabled"]:
                if custom_http['body'] == "":
                    custom_http['body'] = "{}"
                custom_http_tool = {
                    "type": "function",
                    "function": {
                        "name": f"custom_http_{custom_http['name']}",
                        "description": f"{custom_http['description']}",
                        "parameters": json.loads(custom_http['body']),
                    },
                }
                tools.append(custom_http_tool)
    search_not_done = False
    search_task = ""
    print(tools)
    try:
        model = settings['model']
        extra_params = settings['extra_params']
        # ç§»é™¤extra_paramsè¿™ä¸ªlistä¸­"name"ä¸åŒ…å«éç©ºç™½ç¬¦çš„é”®å€¼å¯¹
        if extra_params:
            for extra_param in extra_params:
                if not extra_param['name'].strip():
                    extra_params.remove(extra_param)
            # åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            extra_params = {item['name']: item['value'] for item in extra_params}
        else:
            extra_params = {}
        if request.fileLinks:
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            system_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += system_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': system_message})
        kb_list = []
        user_prompt = request.messages[-1]['content']
        if m0:
            lore_content = ""
            assistant_reply = ""
            # æ‰¾å‡ºrequest.messagesä¸­ä¸Šæ¬¡çš„assistantå›å¤
            for i in range(len(request.messages)-1, -1, -1):
                if request.messages[i]['role'] == 'assistant':
                    assistant_reply = request.messages[i]['content']
                    break
            if cur_memory["lorebook"]:
                for lore in cur_memory["lorebook"]:
                    if lore["name"] != "" and (lore["name"] in user_prompt or lore["name"] in assistant_reply):
                        lore_content = lore_content + "\n\n" + f"{lore['name']}ï¼š{lore['value']}"
            global cur_random 
            # å¦‚æœrequest.messagesä¸­ä¸åŒ…å«assistantå›å¤ï¼Œè¯´æ˜æ˜¯é¦–æ¬¡æé—®ï¼Œè§¦å‘éšæœºè®¾å®š
            if not assistant_reply:
                # å¦‚æœ cur_memory ä¸­æœ‰ random æ¡ç›®
                if cur_memory.get("random") and len(cur_memory["random"]) > 0:
                    # éšæœºé€‰æ‹©ä¸€ä¸ª random æ¡ç›®
                    random_entry = random.choice(cur_memory["random"])
                    if random_entry.get("value"):
                        lore_content = lore_content + "\n\n" + f"{random_entry['value']}"
                        cur_random.append({"id":memoryId,"value":random_entry["value"]})
                        print("æ–°éšæœºè®¾å®šï¼š",{"id":memoryId,"value":random_entry["value"]})
            else:
                for item in cur_random:
                    if item["id"] == memoryId:
                        lore_content = lore_content + "\n\n" + f"{item['value']}"
                        print("æ²¿ç”¨éšæœºè®¾å®šï¼š",{"id":memoryId,"value":item['value']})  
                        break  
            memoryLimit = settings["memorySettings"]["memoryLimit"]
            try:
                print("æŸ¥è¯¢è®°å¿†")
                relevant_memories = m0.search(query=user_prompt, user_id=memoryId, limit=memoryLimit)
                relevant_memories = json.dumps(relevant_memories, ensure_ascii=False)
                print("æŸ¥è¯¢è®°å¿†ç»“æŸ")
            except Exception as e:
                print("m0.search error:",e)
                relevant_memories = ""
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"
            else:
                request.messages.insert(0, {'role': 'system', 'content': "ä¹‹å‰çš„ç›¸å…³è®°å¿†ï¼š\n\n" + relevant_memories + "\n\nç›¸å…³ç»“æŸ\n\n"})
            if cur_memory["basic_character"]:
                print("æ·»åŠ è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "è§’è‰²è®¾å®šï¼š\n\n" + cur_memory["basic_character"] + "\n\nè§’è‰²è®¾å®šç»“æŸ\n\n"})
            if lore_content:
                print("æ·»åŠ ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n")
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"
                else:
                    request.messages.insert(0, {'role': 'system', 'content': "ä¸–ç•Œè§‚è®¾å®šï¼š\n\n" + lore_content + "\n\nä¸–ç•Œè§‚è®¾å®šç»“æŸ\n\n"})
        if settings["knowledgeBases"]:
            for kb in settings["knowledgeBases"]:
                if kb["enabled"] and kb["processingStatus"] == "completed":
                    kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
        if settings["KBSettings"]["when"] == "before_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                all_kb_content = []
                # ç”¨query_knowledge_baseå‡½æ•°æŸ¥è¯¢kb_listä¸­æ‰€æœ‰çš„çŸ¥è¯†åº“
                for kb in kb_list:
                    kb_content = await query_knowledge_base(kb["kb_id"],user_prompt)
                    all_kb_content.extend(kb_content)
                    if settings["KBSettings"]["is_rerank"]:
                        all_kb_content = await rerank_knowledge_base(user_prompt,all_kb_content)
                if all_kb_content:
                    kb_message = f"\n\nå¯å‚è€ƒçš„çŸ¥è¯†åº“å†…å®¹ï¼š{all_kb_content}"
                    request.messages[-1]['content'] += f"{kb_message}\n\nç”¨æˆ·ï¼š{user_prompt}"
        if settings["KBSettings"]["when"] == "after_thinking" or settings["KBSettings"]["when"] == "both":
            if kb_list:
                kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
                if request.messages and request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] += kb_list_message
                else:
                    request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
        else:
            kb_list = []
        request = await tools_change_messages(request, settings)
        if settings['webSearch']['enabled'] or enable_web_search:
            if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    results = await DDGsearch_async(user_prompt)
                elif settings['webSearch']['engine'] == 'searxng':
                    results = await searxng_async(user_prompt)
                elif settings['webSearch']['engine'] == 'tavily':
                    results = await Tavily_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'bing':
                    results = await Bing_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'google':
                    results = await Google_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'brave':
                    results = await Brave_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'exa':
                    results = await Exa_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'serper':
                    results = await Serper_search_async(user_prompt)
                elif settings['webSearch']['engine'] == 'bochaai':
                    results = await bochaai_search_async(user_prompt)
                if results:
                    request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}"
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    tools.append(duckduckgo_tool)
                elif settings['webSearch']['engine'] == 'searxng':
                    tools.append(searxng_tool)
                elif settings['webSearch']['engine'] == 'tavily':
                    tools.append(tavily_tool)
                elif settings['webSearch']['engine'] == 'bing':
                    tools.append(bing_tool)
                elif settings['webSearch']['engine'] == 'google':
                    tools.append(google_tool)
                elif settings['webSearch']['engine'] == 'brave':
                    tools.append(brave_tool)
                elif settings['webSearch']['engine'] == 'exa':
                    tools.append(exa_tool)
                elif settings['webSearch']['crawler'] == 'serper':
                    tools.append(serper_tool)
                elif settings['webSearch']['crawler'] == 'bochaai':
                    tools.append(bochaai_tool)

                if settings['webSearch']['crawler'] == 'jina':
                    tools.append(jina_crawler_tool)
                elif settings['webSearch']['crawler'] == 'crawl4ai':
                    tools.append(Crawl4Ai_tool)
        if kb_list:
            tools.append(kb_tool)
        if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            deepsearch_messages = copy.deepcopy(request.messages)
            deepsearch_messages[-1]['content'] += "\n\nå°†ç”¨æˆ·æå‡ºçš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸€ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€çŸ­çš„è¯æ¦‚æ‹¬å³å¯ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚å¦‚æœæ˜¯éå¸¸ç®€å•çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥åªç»™å‡ºä¸€ä¸ªæ­¥éª¤å³å¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯éœ€è¦æ‹†åˆ†æˆå¤šä¸ªæ­¥éª¤çš„ã€‚"
            response = await client.chat.completions.create(
                model=model,
                messages=deepsearch_messages,
                temperature=0.5, 
                max_tokens=512,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            user_prompt = response.choices[0].message.content
            request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
        if settings['reasoner']['enabled'] or enable_thinking:
            reasoner_messages = copy.deepcopy(request.messages)
            if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                drs_msg = get_drs_stage(DRS_STAGE)
                if drs_msg:
                    reasoner_messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
                reasoner_messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
            if tools:
                reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
            for modelProvider in settings['modelProviders']: 
                if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                    vendor = modelProvider['vendor']
                    break
            msg = await images_add_in_messages(reasoner_messages, images,settings)    
            if vendor == 'Ollama':
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    temperature=settings['reasoner']['temperature']
                )
                # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                start_index = reasoning_content.find(open_tag) + len(open_tag)
                end_index = reasoning_content.find(close_tag)
                if start_index != -1 and end_index != -1:
                    reasoning_content = reasoning_content[start_index:end_index]
                else:
                    reasoning_content = ""
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoning_content
            else:
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=msg,
                    stream=False,
                    max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    temperature=settings['reasoner']['temperature']
                )
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        if settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ­¥éª¤ï¼š{user_prompt}\n\n"
            drs_msg = get_drs_stage(DRS_STAGE)
            if drs_msg:
                request.messages[-1]['content'] += f"\n\n{drs_msg}\n\n"
        msg = await images_add_in_messages(request.messages, images,settings)
        if tools:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                tools=tools,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p or settings['top_p'],
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                temperature=request.temperature,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p or settings['top_p'],
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
        if response.choices[0].message.tool_calls:
            pass
        elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
            search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,response.choices[0].message.content)
            research_response = await client.chat.completions.create(
                model=model,
                messages=[
                    {
                    "role": "user",
                    "content": search_prompt,
                    }
                ],
                temperature=0.5,
                extra_body = extra_params, # å…¶ä»–å‚æ•°
            )
            response_content = research_response.choices[0].message.content
            print(response_content)
            # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
            if "```json" in response_content:
                try:
                    response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                except:
                    # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                    response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
            response_content = json.loads(response_content)
            if response_content["status"] == "done":
                search_not_done = False
            elif response_content["status"] == "not_done":
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "need_more_info":
                DRS_STAGE = 2
                search_not_done = False
            elif response_content["status"] == "search":
                DRS_STAGE = 2
                search_not_done = True
                drs_msg = get_drs_stage(DRS_STAGE)
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": drs_msg,
                    }
                )
            elif response_content["status"] == "need_more_search":
                DRS_STAGE = 2
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "answer":
                DRS_STAGE = 3
                search_not_done = True
                drs_msg = get_drs_stage(DRS_STAGE)
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": research_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": drs_msg,
                    }
                )
        reasoner_messages = copy.deepcopy(request.messages)
        while response.choices[0].message.tool_calls or search_not_done:
            if response.choices[0].message.tool_calls:
                assistant_message = response.choices[0].message
                response_content = assistant_message.tool_calls[0].function
                print(response_content.name)
                modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                data_list = json.loads(modified_data)
                # å­˜å‚¨å¤„ç†ç»“æœ
                results = []
                for data in data_list:
                    result = await dispatch_tool(response_content.name, data,settings) # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                    if result is not None:
                        # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                        results.append(json.dumps(result))

                # å°†æ‰€æœ‰ç»“æœæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
                combined_results = ''.join(results)
                if combined_results:
                    results = combined_results
                else:
                    results = None
                print(results)
                if results is None:
                    break
                if response_content.name in ["query_knowledge_base"]:
                    if settings["KBSettings"]["is_rerank"]:
                        results = await rerank_knowledge_base(user_prompt,results)
                    results = json.dumps(results, ensure_ascii=False, indent=4)
                request.messages.append(
                    {
                        "tool_calls": [
                            {
                                "id": assistant_message.tool_calls[0].id,
                                "function": {
                                    "arguments": response_content.arguments,
                                    "name": response_content.name,
                                },
                                "type": assistant_message.tool_calls[0].type,
                            }
                        ],
                        "role": "assistant",
                        "content": str(response_content),
                    }
                )
                request.messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": assistant_message.tool_calls[0].id,
                        "name": response_content.name,
                        "content": str(results),
                    }
                )
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
            reasoner_messages.append(
                {
                    "role": "assistant",
                    "content": str(response_content),
                }
            )
            reasoner_messages.append(
                {
                    "role": "user",
                    "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                }
            )
            if settings['reasoner']['enabled'] or enable_thinking:

                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                for modelProvider in settings['modelProviders']: 
                    if modelProvider['id'] == settings['reasoner']['selectedProvider']:
                        vendor = modelProvider['vendor']
                        break
                msg = await images_add_in_messages(reasoner_messages, images,settings)
                if vendor == 'Ollama':
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        temperature=settings['reasoner']['temperature']
                    )
                    # å°†æ¨ç†ç»“æœä¸­çš„æ€è€ƒå†…å®¹æå–å‡ºæ¥
                    reasoning_content = reasoner_response.model_dump()['choices'][0]['message']['content']
                    # open_tagå’Œclose_tagä¹‹é—´çš„å†…å®¹
                    start_index = reasoning_content.find(open_tag) + len(open_tag)
                    end_index = reasoning_content.find(close_tag)
                    if start_index != -1 and end_index != -1:
                        reasoning_content = reasoning_content[start_index:end_index]
                    else:
                        reasoning_content = ""
                    request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoning_content
                else:
                    reasoner_response = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=msg,
                        stream=False,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
            msg = await images_add_in_messages(request.messages, images,settings)
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    tools=tools,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=msg,  # æ·»åŠ å›¾ç‰‡ä¿¡æ¯åˆ°æ¶ˆæ¯
                    temperature=request.temperature,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p or settings['top_p'],
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
            print(response)
            if response.choices[0].message.tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled'] or enable_deep_research: 
                search_prompt = get_drs_stage_system_message(DRS_STAGE,user_prompt,response.choices[0].message.content)
                research_response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                    extra_body = extra_params, # å…¶ä»–å‚æ•°
                )
                response_content = research_response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                response_content = json.loads(response_content)
                if response_content["status"] == "done":
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    DRS_STAGE = 2
                    search_not_done = False
                elif response_content["status"] == "search":
                    DRS_STAGE = 2
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
                elif response_content["status"] == "need_more_search":
                    DRS_STAGE = 2
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­æŸ¥è¯¢å¦‚ä¸‹ä¿¡æ¯ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\n"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "answer":
                    DRS_STAGE = 3
                    search_not_done = True
                    drs_msg = get_drs_stage(DRS_STAGE)
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": research_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": drs_msg,
                        }
                    )
       # å¤„ç†å“åº”å†…å®¹
        response_dict = response.model_dump()
        content = response_dict["choices"][0]['message']['content']
        if open_tag in content and close_tag in content:
            reasoning_content = re.search(fr'{open_tag}(.*?)\{close_tag}', content, re.DOTALL)
            if reasoning_content:
                # å­˜å‚¨åˆ° reasoning_content å­—æ®µ
                response_dict["choices"][0]['message']['reasoning_content'] = reasoning_content.group(1).strip()
                # ç§»é™¤åŸå†…å®¹ä¸­çš„æ ‡ç­¾éƒ¨åˆ†
                response_dict["choices"][0]['message']['content'] = re.sub(fr'{open_tag}(.*?)\{close_tag}', '', content, flags=re.DOTALL).strip()
        if settings['reasoner']['enabled'] or enable_thinking:
            response_dict["choices"][0]['message']['reasoning_content'] = reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        if m0:
            messages=[
                {
                    "role": "user",
                    "content": user_prompt,
                },
                {
                    "role": "assistant",
                    "content": response_dict["choices"][0]['message']['content'],
                }
            ]
            executor = ThreadPoolExecutor()
            async def add_async():
                loop = asyncio.get_event_loop()
                # ç»‘å®š user_id å…³é”®å­—å‚æ•°
                func = partial(m0.add, user_id=memoryId)
                # ä¼ é€’ messages ä½œä¸ºä½ç½®å‚æ•°
                await loop.run_in_executor(executor, func, messages)
                print("çŸ¥è¯†åº“æ›´æ–°å®Œæˆ")

            asyncio.create_task(add_async())
        return JSONResponse(content=response_dict)
    except Exception as e:
        return JSONResponse(
            content={"error": {"message": e.message, "type": "api_error", "code": e.code}}
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/models")
async def get_models():
    """
    è·å–æ¨¡å‹åˆ—è¡¨
    """
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            Model(
                id=agent["name"],  
                created=0,  
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
            for agent in agents.values()  
        ]
        # æ·»åŠ é»˜è®¤çš„ 'super-model'
        model_data.append(
            Model(
                id='super-model',
                created=0,
                object="model",
                owned_by="super-agent-party"  # éç©ºå­—ç¬¦ä¸²
            )
        )

        # æ„é€ å®Œæ•´ SyncPage å“åº”
        response = SyncPage[Model](
            object="list",
            data=model_data,
            has_more=False  # æ·»åŠ åˆ†é¡µæ ‡è®°
        )
        # ç›´æ¥è¿”å›æ¨¡å‹å­—å…¸ï¼Œç”± FastAPI è‡ªåŠ¨åºåˆ—åŒ–ä¸º JSON
        return response.model_dump()  
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": e.message,
                    "type": e.type or "api_error",
                    "code": e.code
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/agents",operation_id="get_agents")
async def get_agents():
    """
    è·å–æ¨¡å‹åˆ—è¡¨
    """
    from openai.types import Model
    from openai.pagination import SyncPage
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = await load_settings()
        agents = current_settings['agents']
        # æ„é€ ç¬¦åˆ OpenAI æ ¼å¼çš„ Model å¯¹è±¡
        model_data = [
            {
                "name": agent["name"],
                "description": agent["system_prompt"],
            }
            for agent in agents.values()  
        ]
        # æ·»åŠ é»˜è®¤çš„ 'super-model'
        model_data.append(
            {
                "name": 'super-model',
                "description": "Super-Agent-Party default agent",
            }
        )
        return model_data
        
    except Exception as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": e.message,
                    "type": e.type or "api_error",
                    "code": e.code
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

class ProviderModelRequest(BaseModel):
    url: str
    api_key: str

@app.post("/v1/providers/models")
async def fetch_provider_models(request: ProviderModelRequest):
    try:
        # ä½¿ç”¨ä¼ å…¥çš„provideré…ç½®åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        client = AsyncOpenAI(api_key=request.api_key, base_url=request.url)
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = await client.models.list()
        # æå–æ¨¡å‹IDå¹¶è¿”å›
        return JSONResponse(content={"data": [model.id for model in model_list.data]})
    except Exception as e:
        # å¤„ç†å¼‚å¸¸ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions", operation_id="chat_with_agent_party")
async def chat_endpoint(request: ChatRequest,fastapi_request: Request):
    """
    ç”¨æ¥ä¸agent partyä¸­çš„æ¨¡å‹èŠå¤©
    messages: å¿…å¡«é¡¹ï¼ŒèŠå¤©è®°å½•ï¼ŒåŒ…æ‹¬roleå’Œcontent
    model: å¯é€‰é¡¹ï¼Œé»˜è®¤ä½¿ç”¨ 'super-model'ï¼Œå¯ä»¥ç”¨get_models()è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    stream: å¯é€‰é¡¹ï¼Œé»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æµå¼å“åº”
    enable_thinking: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
    enable_deep_research: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨æ·±åº¦ç ”ç©¶æ¨¡å¼
    enable_web_search: é»˜è®¤ä¸ºFalseï¼Œæ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
    """
    fastapi_base_url = str(fastapi_request.base_url)
    global client, settings,reasoner_client,mcp_client_list
    model = request.model or 'super-model' # é»˜è®¤ä½¿ç”¨ 'super-model'
    enable_thinking = request.enable_thinking or False
    enable_deep_research = request.enable_deep_research or False
    enable_web_search = request.enable_web_search or False
    if model == 'super-model':
        current_settings = await load_settings()
        # åŠ¨æ€æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        if (current_settings['api_key'] != settings['api_key'] 
            or current_settings['base_url'] != settings['base_url']):
            client = AsyncOpenAI(
                api_key=current_settings['api_key'],
                base_url=current_settings['base_url'] or "https://api.openai.com/v1",
            )
        if (current_settings['reasoner']['api_key'] != settings['reasoner']['api_key'] 
            or current_settings['reasoner']['base_url'] != settings['reasoner']['base_url']):
            reasoner_client = AsyncOpenAI(
                api_key=current_settings['reasoner']['api_key'],
                base_url=current_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
            )
        # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
        if current_settings['system_prompt']:
            if request.messages[0]['role'] == 'system':
                request.messages[0]['content'] = current_settings['system_prompt'] + "\n\n" + request.messages[0]['content']
            else:
                request.messages.insert(0, {'role': 'system', 'content': current_settings['system_prompt']})
        if current_settings != settings:
            settings = current_settings
        try:
            if request.stream:
                return await generate_stream_response(client,reasoner_client, request, settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
            return await generate_complete_response(client,reasoner_client, request, settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    else:
        current_settings = await load_settings()
        agentSettings = current_settings['agents'].get(model, {})
        if not agentSettings:
            for agentId , agentConfig in current_settings['agents'].items():
                if current_settings['agents'][agentId]['name'] == model:
                    agentSettings = current_settings['agents'][agentId]
                    break
        if not agentSettings:
            return JSONResponse(
                status_code=404,
                content={"error": {"message": f"Agent {model} not found", "type": "not_found", "code": 404}}
            )
        if agentSettings['config_path']:
            with open(agentSettings['config_path'], 'r' , encoding='utf-8') as f:
                agent_settings = json.load(f)
            # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
            if agentSettings['system_prompt']:
                if request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] = agentSettings['system_prompt'] + "\n\n" + request.messages[0]['content']
                else:
                    request.messages.insert(0, {'role': 'system', 'content': agentSettings['system_prompt']})
        agent_client = AsyncOpenAI(
            api_key=agent_settings['api_key'],
            base_url=agent_settings['base_url'] or "https://api.openai.com/v1",
        )
        agent_reasoner_client = AsyncOpenAI(
            api_key=agent_settings['reasoner']['api_key'],
            base_url=agent_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
        )
        try:
            if request.stream:
                return await generate_stream_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
            return await generate_complete_response(agent_client,agent_reasoner_client, request, agent_settings,fastapi_base_url,enable_thinking,enable_deep_research,enable_web_search)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )

# æ·»åŠ çŠ¶æ€å­˜å‚¨
mcp_status = {}
@app.post("/create_mcp")
async def create_mcp_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    mcp_id = data.get("mcpId")
    
    if not mcp_id:
        raise HTTPException(status_code=400, detail="Missing mcpId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_mcp, mcp_id)
    
    return {"success": True, "message": "MCPæœåŠ¡å™¨åˆå§‹åŒ–å·²å¼€å§‹"}
@app.get("/mcp_status/{mcp_id}")
async def get_mcp_status(mcp_id: str):
    status = mcp_status.get(mcp_id, "not_found")
    return {"mcp_id": mcp_id, "status": status}
async def process_mcp(mcp_id: str):
    global mcp_client_list
    mcp_status[mcp_id] = "initializing"
    try:
        # è·å–å¯¹åº”æœåŠ¡å™¨çš„é…ç½®
        cur_settings = await load_settings()
        server_config = cur_settings['mcpServers'][mcp_id]
        
        # æ‰§è¡Œåˆå§‹åŒ–é€»è¾‘
        mcp_client_list[mcp_id] = McpClient()    
        await asyncio.wait_for(mcp_client_list[mcp_id].initialize(mcp_id, server_config), timeout=6)
        mcp_status[mcp_id] = "ready"
        mcp_client_list[mcp_id].disabled = False
        
    except Exception as e:
        mcp_client_list[mcp_id].disabled = True
        mcp_status[mcp_id] = f"failed: {str(e)}"

@app.post("/api/remove_mcp")
async def remove_mcp_server(request: Request):
    global settings, mcp_client_list
    try:
        data = await request.json()
        server_name = data.get("serverName", "")

        if not server_name:
            raise HTTPException(status_code=400, detail="No server names provided")

        # ç§»é™¤æŒ‡å®šçš„MCPæœåŠ¡å™¨
        current_settings = await load_settings()
        if server_name in current_settings['mcpServers']:
            del current_settings['mcpServers'][server_name]
            await save_settings(current_settings)
            settings = current_settings

            # ä»mcp_client_listä¸­ç§»é™¤
            if server_name in mcp_client_list:
                mcp_client_list[server_name].disabled = True
                await mcp_client_list[server_name].close()
                del mcp_client_list[server_name]
                print(f"å…³é—­MCPæœåŠ¡å™¨: {server_name}")

            return JSONResponse({"success": True, "removed": server_name})
        else:
            raise HTTPException(status_code=404, detail="Server not found")
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON format")
    except Exception as e:
        logger.error(f"ç§»é™¤MCPæœåŠ¡å™¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/remove_memory")
async def remove_memory_endpoint(request: Request):
    data = await request.json()
    memory_id = data.get("memoryId")
    if memory_id:
        try:
            # åˆ é™¤MEMORY_CACHE_DIRç›®å½•ä¸‹çš„memory_idæ–‡ä»¶å¤¹
            memory_dir = os.path.join(MEMORY_CACHE_DIR, memory_id)
            shutil.rmtree(memory_dir)
            return JSONResponse({"success": True, "message": "Memory removed"})
        except Exception as e:
            return JSONResponse({"success": False, "message": str(e)})
    else:
        return JSONResponse({"success": False, "message": "No memoryId provided"})

@app.post("/remove_agent")
async def remove_agent_endpoint(request: Request):
    data = await request.json()
    agent_id = data.get("agentId")
    if agent_id:
        try:
            # åˆ é™¤AGENT_CACHE_DIRç›®å½•ä¸‹çš„agent_idæ–‡ä»¶å¤¹
            agent_dir = os.path.join(AGENT_DIR, f"{agent_id}.json")
            shutil.rmtree(agent_dir)
            return JSONResponse({"success": True, "message": "Agent removed"})
        except Exception as e:
            return JSONResponse({"success": False, "message": str(e)})
    else:
        return JSONResponse({"success": False, "message": "No agentId provided"})

@app.post("/a2a/initialize")
async def initialize_a2a(request: Request):
    from python_a2a import A2AClient
    data = await request.json()
    try:
        client = A2AClient(data['url'])
        agent_card = client.agent_card.to_json()
        agent_card = json.loads(agent_card)
        return JSONResponse({
            **agent_card,
            "status": "ready",
            "enabled": True
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

# åœ¨ç°æœ‰è·¯ç”±ä¹‹åæ·»åŠ healthè·¯ç”±
@app.get("/health")
async def health_check():
    return {"status": "ok"}


@app.post("/load_file")
async def load_file_endpoint(request: Request, files: List[UploadFile] = File(None)):
    fastapi_base_url = str(request.base_url)
    logger.info(f"Received request with content type: {request.headers.get('Content-Type')}")
    file_links = []
    textFiles = []
    imageFiles = []
    content_type = request.headers.get('Content-Type', '')
    try:
        if 'multipart/form-data' in content_type:
            # å¤„ç†æµè§ˆå™¨ä¸Šä¼ çš„æ–‡ä»¶
            if not files:
                raise HTTPException(status_code=400, detail="No files provided")
            
            for file in files:
                file_extension = os.path.splitext(file.filename)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                with open(destination, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file.filename
                }
                file_links.append(file_link)
                file_meta = {
                    "unique_filename": unique_filename,
                    "original_filename": file.filename,
                }
                # file_extensionç§»é™¤ç‚¹å·
                file_extension = file_extension[1:]
                if file_extension in ALLOWED_EXTENSIONS:
                    textFiles.append(file_meta)
                elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                    imageFiles.append(file_meta)
        elif 'application/json' in content_type:
            # å¤„ç†Electronå‘é€çš„JSONæ–‡ä»¶è·¯å¾„
            data = await request.json()
            logger.info(f"Processing JSON data: {data}")
            
            for file_info in data.get("files", []):
                file_path = file_info.get("path")
                file_name = file_info.get("name", os.path.basename(file_path))
                
                if not os.path.isfile(file_path):
                    logger.error(f"File not found: {file_path}")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                file_extension = os.path.splitext(file_name)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_FILES_DIR, unique_filename)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•
                with open(file_path, "rb") as src, open(destination, "wb") as dst:
                    dst.write(src.read())
                
                file_link = {
                    "path": f"{fastapi_base_url}uploaded_files/{unique_filename}",
                    "name": file_name
                }
                file_links.append(file_link)
                file_meta = {
                    "unique_filename": unique_filename,
                    "original_filename": file_name,
                }
                # file_extensionç§»é™¤ç‚¹å·
                file_extension = file_extension[1:]
                if file_extension in ALLOWED_EXTENSIONS:
                    textFiles.append(file_meta)
                elif file_extension in ALLOWED_IMAGE_EXTENSIONS:
                    imageFiles.append(file_meta)
        else:
            raise HTTPException(status_code=400, detail="Unsupported Content-Type")
        return JSONResponse(content={"success": True, "fileLinks": file_links , "textFiles": textFiles, "imageFiles": imageFiles})
    
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/delete_file")
async def delete_file_endpoint(request: Request):
    data = await request.json()
    file_name = data.get("fileName")
    file_path = os.path.join(UPLOAD_FILES_DIR, file_name)
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            return JSONResponse(content={"success": True})
        else:
            return JSONResponse(content={"success": False, "message": "File not found"})
    except Exception as e:
        return JSONResponse(content={"success": False, "message": str(e)})

@app.post("/create_kb")
async def create_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")
    
    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_kb, kb_id)
    
    return {"success": True, "message": "çŸ¥è¯†åº“å¤„ç†å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢çŠ¶æ€"}

@app.post("/remove_kb")
async def remove_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")

    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    try:
        background_tasks.add_task(remove_kb, kb_id)
    except Exception as e:
        return {"success": False, "message": str(e)}
    return {"success": True, "message": "çŸ¥è¯†åº“å·²åˆ é™¤"}

# åˆ é™¤çŸ¥è¯†åº“
async def remove_kb(kb_id):
    # åˆ é™¤KB_DIR/kb_idç›®å½•
    kb_dir = os.path.join(KB_DIR, str(kb_id))
    if os.path.exists(kb_dir):
        shutil.rmtree(kb_dir)
    else:
        print(f"KB directory {kb_dir} does not exist.")
    return

# æ·»åŠ çŠ¶æ€å­˜å‚¨
kb_status = {}
@app.get("/kb_status/{kb_id}")
async def get_kb_status(kb_id):
    status = kb_status.get(kb_id, "not_found")
    print (f"kb_status: {kb_id} - {status}")
    return {"kb_id": kb_id, "status": status}

# ä¿®æ”¹ process_kb
async def process_kb(kb_id):
    kb_status[kb_id] = "processing"
    try:
        from py.know_base import process_knowledge_base
        await process_knowledge_base(kb_id)
        kb_status[kb_id] = "completed"
    except Exception as e:
        kb_status[kb_id] = f"failed: {str(e)}"

# å®šä¹‰è¯·æ±‚ä½“
class QQBotConfig(BaseModel):
    QQAgent: str
    memoryLimit: int
    appid: str
    secret: str
    separators: List[str]
    reasoningVisible: bool
    quickRestart: bool

# å…¨å±€æœºå™¨äººç®¡ç†å™¨
qq_bot_manager = QQBotManager()

@app.post("/start_qq_bot")
async def start_qq_bot(config: QQBotConfig):
    try:
        qq_bot_manager.start_bot(config)
        return {
            "success": True,
            "message": "QQæœºå™¨äººå·²æˆåŠŸå¯åŠ¨",
            "environment": "thread-based"
        }
    except Exception as e:
        logger.error(f"å¯åŠ¨QQæœºå™¨äººå¤±è´¥: {e}")
        return JSONResponse(
            status_code=400,  # æ”¹ä¸º 400 è¡¨ç¤ºå®¢æˆ·ç«¯é”™è¯¯
            content={
                "success": False, 
                "message": f"å¯åŠ¨å¤±è´¥: {str(e)}",
                "error_type": "startup_error"
            }
        )

@app.post("/stop_qq_bot")
async def stop_qq_bot():
    try:
        qq_bot_manager.stop_bot()
        return {"success": True, "message": "QQæœºå™¨äººå·²åœæ­¢"}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )

@app.get("/qq_bot_status")
async def qq_bot_status():
    status = qq_bot_manager.get_status()
    # å¦‚æœæœ‰å¯åŠ¨é”™è¯¯ï¼Œåœ¨çŠ¶æ€ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯
    if status.get("startup_error") and not status.get("is_running"):
        status["error_message"] = f"å¯åŠ¨å¤±è´¥: {status['startup_error']}"
    return status

@app.post("/reload_qq_bot")
async def reload_qq_bot(config: QQBotConfig):
    try:
        # å…ˆåœæ­¢å†å¯åŠ¨
        qq_bot_manager.stop_bot()
        await asyncio.sleep(1)  # ç­‰å¾…å®Œå…¨åœæ­¢
        qq_bot_manager.start_bot(config)
        
        return {
            "success": True,
            "message": "QQæœºå™¨äººå·²é‡æ–°åŠ è½½",
            "config_changed": True
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": str(e)}
        )

@app.post("/add_workflow")
async def add_workflow(file: UploadFile = File(...)):
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦ä¸º JSON
    if file.content_type != "application/json":
        raise HTTPException(
            status_code=400,
            detail="Only JSON files are allowed."
        )

    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    unique_filename = f"{uuid.uuid4()}.json"
    file_path = os.path.join(UPLOAD_FILES_DIR, unique_filename)

    # ä¿å­˜æ–‡ä»¶
    try:
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to save file: {str(e)}"
        )

    # è¿”å›æ–‡ä»¶ä¿¡æ¯
    return JSONResponse(
        status_code=200,
        content={
            "success": True,
            "message": "File uploaded successfully",
            "file": {
                "unique_filename": unique_filename,
                "original_filename": file.filename,
                "url": f"/uploaded_files/{unique_filename}",
                "enabled": True
            }
        }
    )

@app.delete("/delete_workflow/{filename}")
async def delete_workflow(filename: str):
    file_path = os.path.join(UPLOAD_FILES_DIR, filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found")

    # åˆ é™¤æ–‡ä»¶
    try:
        os.remove(file_path)
        return JSONResponse(
            status_code=200,
            content={"success": True, "message": "File deleted successfully"}
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete file: {str(e)}"
        )

settings_lock = asyncio.Lock()
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    active_connections.append(websocket)

    try:
        async with settings_lock:  # è¯»å–æ—¶åŠ é”
            current_settings = await load_settings()
        await websocket.send_json({"type": "settings", "data": current_settings})
        while True:
            data = await websocket.receive_json()
            if data.get("type") == "ping":
                await websocket.send_json({"type": "pong"})
            elif data.get("type") == "save_settings":
                await save_settings(data.get("data", {}))
                # å‘é€ç¡®è®¤æ¶ˆæ¯ï¼ˆæºå¸¦ç›¸åŒ correlationIdï¼‰
                await websocket.send_json({
                    "type": "settings_saved",
                    "correlationId": data.get("correlationId"),
                    "success": True
                })
            elif data.get("type") == "get_settings":
                settings = await load_settings()
                await websocket.send_json({"type": "settings", "data": settings})
            elif data.get("type") == "save_agent":
                current_settings = await load_settings()
                
                # ç”Ÿæˆæ™ºèƒ½ä½“IDå’Œé…ç½®è·¯å¾„
                agent_id = str(shortuuid.ShortUUID().random(length=8))
                config_path = os.path.join(AGENT_DIR, f"{agent_id}.json")
                
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(current_settings, f, indent=4, ensure_ascii=False)
                
                # æ›´æ–°ä¸»é…ç½®
                current_settings['agents'][agent_id] = {
                    "id": agent_id,
                    "name": data['data']['name'],
                    "system_prompt": data['data']['system_prompt'],
                    "config_path": config_path,
                    "enabled": False,
                }
                await save_settings(current_settings)
                
                # å¹¿æ’­æ›´æ–°åçš„é…ç½®
                await websocket.send_json({
                    "type": "settings",
                    "data": current_settings
                })
    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        active_connections.remove(websocket)

mcp = FastApiMCP(
    app,
    name="Agent party MCP - chat with multiple agents",
    include_operations=["get_agents", "chat_with_agent_party"],
)

mcp.mount()

app.mount("/uploaded_files", StaticFiles(directory=UPLOAD_FILES_DIR), name="uploaded_files")
app.mount("/node_modules", StaticFiles(directory=os.path.join(base_path, "node_modules")), name="node_modules")
app.mount("/", StaticFiles(directory=os.path.join(base_path, "static"), html=True), name="static")

# ç®€åŒ–mainå‡½æ•°
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host=HOST,
        port=PORT
    )
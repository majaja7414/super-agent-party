# -- coding: utf-8 --
import asyncio
import copy
import json
import os
import re
from fastapi import BackgroundTasks, FastAPI, File, HTTPException, UploadFile, WebSocket, Request
import logging
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI, APIStatusError
from pydantic import BaseModel
from fastapi import status
from fastapi.responses import JSONResponse, StreamingResponse
import uuid
import time
import shutil
from typing import List, Dict

import shortuuid
from py.mcp_clients import McpClient
from py.get_setting import load_settings,save_settings,base_path,in_docker
from contextlib import asynccontextmanager
os.environ["no_proxy"] = "localhost,127.0.0.1"
if in_docker():
    HOST = '0.0.0.0'
else:
    HOST = '127.0.0.1'
PORT = 3456
local_timezone = None
logger = None
settings = None
client = None
reasoner_client = None
mcp_client_list = {}
_TOOL_HOOKS = {}

@asynccontextmanager
async def lifespan(app: FastAPI): 
    global settings, client, reasoner_client, mcp_client_list,local_timezone,logger
    from tzlocal import get_localzone
    local_timezone = get_localzone()
    logger = logging.getLogger(__name__)
    settings = load_settings()
    if settings:
        client = AsyncOpenAI(api_key=settings['api_key'], base_url=settings['base_url'])
        reasoner_client = AsyncOpenAI(api_key=settings['reasoner']['api_key'],base_url=settings['reasoner']['base_url'])
    else:
        client = AsyncOpenAI()
        reasoner_client = AsyncOpenAI()
    if settings:
        for server_name,server_config in settings['mcpServers'].items():
            mcp_client_list[server_name] = McpClient()
            # åˆå§‹åŒ–mcpå®¢æˆ·ç«¯ï¼Œé™åˆ¶10ç§’å†…ï¼Œå¦åˆ™è·³è¿‡
            try:
                await asyncio.wait_for(mcp_client_list[server_name].initialize(server_name, server_config), timeout=5)
            except asyncio.TimeoutError:
                logger.error(f"Failed to initialize MCP client for {server_name} in 5 seconds")
                mcp_client_list[server_name].disabled = True
                del settings['mcpServers'][server_name]
                save_settings(settings)
    yield

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def dispatch_tool(tool_name: str, tool_params: dict) -> str:
    global mcp_client_list,_TOOL_HOOKS
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        jina_crawler_async,
        Crawl4Ai_search_async, 
    )
    from py.know_base import query_knowledge_base
    _TOOL_HOOKS = {
        "DDGsearch_async": DDGsearch_async,
        "searxng_async": searxng_async,
        "Tavily_search_async": Tavily_search_async,
        "query_knowledge_base": query_knowledge_base,
        "jina_crawler_async": jina_crawler_async,
        "Crawl4Ai_search_async": Crawl4Ai_search_async,
    }
    if "multi_tool_use." in tool_name:
        tool_name = tool_name.replace("multi_tool_use.", "")
    if tool_name not in _TOOL_HOOKS:
        for server_name, mcp_client in mcp_client_list.items():
            if tool_name in mcp_client.tools_list:
                result = await mcp_client.call_tool(tool_name, tool_params)
                return str(result.model_dump())
        return None
    tool_call = _TOOL_HOOKS[tool_name]
    try:
        ret_out = await tool_call(**tool_params)
        return ret_out
    except Exception as e:
        logger.error(f"Error calling tool {tool_name}: {e}")
        return f"Error calling tool {tool_name}: {e}"


class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str = None
    temperature: float = 0.7
    tools: dict = None
    stream: bool = False
    max_tokens: int = None
    top_p: float = 1
    frequency_penalty: float = 0
    presence_penalty: float = 0
    fileLinks: List[str] = None

def tools_change_messages(request: ChatRequest, settings: dict):
    if settings['tools']['time']['enabled']:
        request.messages[-1]['content'] = f"å½“å‰ç³»ç»Ÿæ—¶é—´ï¼š{local_timezone}  {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
    if settings['tools']['inference']['enabled']:
        inference_message = "å›ç­”ç”¨æˆ·å‰è¯·å…ˆæ€è€ƒæ¨ç†ï¼Œå†å›ç­”é—®é¢˜ï¼Œä½ çš„æ€è€ƒæ¨ç†çš„è¿‡ç¨‹å¿…é¡»æ”¾åœ¨<think>ä¸</think>ä¹‹é—´ã€‚\n\n"
        request.messages[-1]['content'] = f"{inference_message}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
    if settings['tools']['formula']['enabled']:
        latex_message = "\n\nå½“ä½ æƒ³ä½¿ç”¨latexå…¬å¼æ—¶ï¼Œä½ å¿…é¡»æ˜¯ç”¨ ['$', '$'] ä½œä¸ºè¡Œå†…å…¬å¼å®šç•Œç¬¦ï¼Œä»¥åŠ ['$$', '$$'] ä½œä¸ºè¡Œé—´å…¬å¼å®šç•Œç¬¦ã€‚\n\n"
        if request.messages and request.messages[0]['role'] == 'system':
            request.messages[0]['content'] += latex_message
        else:
            request.messages.insert(0, {'role': 'system', 'content': latex_message})
    return request

async def generate_stream_response(client,reasoner_client, request: ChatRequest, settings: dict):
    global mcp_client_list
    from py.load_files import get_files_content
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool
    try:
        tools = request.tools or []
        print(tools)
        if mcp_client_list:
            for server_name, mcp_client in mcp_client_list.items():
                if server_name in settings['mcpServers']:
                    if 'disabled' not in settings['mcpServers'][server_name]:
                        settings['mcpServers'][server_name]['disabled'] = False
                    if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                        function = await mcp_client.get_openai_functions()
                        if function:
                            tools.extend(function)
        source_prompt = ""
        if request.fileLinks:
            # éå†æ–‡ä»¶é“¾æ¥åˆ—è¡¨
            for file_link in request.fileLinks:
                # å¦‚æœfile_linkæ˜¯http://${HOST}:${PORT}å¼€å¤´
                if file_link.startswith(f"http://${HOST}:{PORT}"):
                    # å°†"http://${HOST}:{PORT}"æ›¿æ¢ä¸º"uploaded_files"
                    file_link = file_link.replace(f"http://{HOST}:{PORT}", "uploaded_files")
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            fileLinks_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += fileLinks_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': fileLinks_message})
            source_prompt += fileLinks_message
        kb_list = []
        if settings["knowledgeBases"]:
            for kb in settings["knowledgeBases"]:
                if kb["enabled"] and kb["processingStatus"] == "completed":
                    kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
        if kb_list:
            kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += kb_list_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
        user_prompt = request.messages[-1]['content']
        request = tools_change_messages(request, settings)
        model = settings['model']
        async def stream_generator(user_prompt):
            if settings['webSearch']['enabled']:
                if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                    chunk_dict = {
                        "id": "webSearch",
                        "choices": [
                            {
                                "finish_reason": None,
                                "index": 0,
                                "delta": {
                                    "role":"assistant",
                                    "content": "",
                                    "reasoning_content": "æ€è€ƒå‰è”ç½‘æœç´¢ä¸­ï¼Œè¯·ç¨å€™...\n\n"
                                }
                            }
                        ]
                    }
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        results = await DDGsearch_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'searxng':
                        results = await searxng_async(user_prompt)
                    elif settings['webSearch']['engine'] == 'tavily':
                        results = await Tavily_search_async(user_prompt)
                    if results:
                        request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}\n\nè¯·æ ¹æ®è”ç½‘æœç´¢ç»“æœç»„ç»‡ä½ çš„å›ç­”ï¼Œå¹¶ç¡®ä¿ä½ çš„å›ç­”æ˜¯å‡†ç¡®çš„ã€‚"
                        # è·å–æ—¶é—´æˆ³å’Œuuid
                        timestamp = time.time()
                        uid = str(uuid.uuid4())
                        # æ„é€ æ–‡ä»¶å
                        filename = f"{timestamp}_{uid}.txt"
                        # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                        with open(f"uploaded_files/{filename}", "w", encoding='utf-8') as f:
                            f.write(str(results))           
                        # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                        fileLink=f"http://{HOST}:{PORT}/uploaded_files/{filename}"
                        tool_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": f"\n\n[æœç´¢ç»“æœ]({fileLink})\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(tool_chunk)}\n\n"
                if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                    if settings['webSearch']['engine'] == 'duckduckgo':
                        tools.append(duckduckgo_tool)
                    elif settings['webSearch']['engine'] == 'searxng':
                        tools.append(searxng_tool)
                    elif settings['webSearch']['engine'] == 'tavily':
                        tools.append(tavily_tool)
                    if settings['webSearch']['crawler'] == 'jina':
                        tools.append(jina_crawler_tool)
                    elif settings['webSearch']['crawler'] == 'crawl4ai':
                        tools.append(Crawl4Ai_tool)
            if kb_list:
                tools.append(kb_tool)
            if settings['tools']['deepsearch']['enabled']: 
                deepsearch_messages = copy.deepcopy(request.messages)
                deepsearch_messages[-1]['content'] += "\n\næ€»ç»“æ¦‚æ‹¬ä¸€ä¸‹ç”¨æˆ·çš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚"
                print(request.messages[-1]['content'])
                response = await client.chat.completions.create(
                    model=model,
                    messages=deepsearch_messages,
                    temperature=0.5
                )
                user_prompt = response.choices[0].message.content
                deepsearch_chunk = {
                    "choices": [{
                        "delta": {
                            "reasoning_content": f"\n\nğŸ’–å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼š{user_prompt}\n\n",
                        }
                    }]
                }
                yield f"data: {json.dumps(deepsearch_chunk)}\n\n"
                request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
                print(request.messages[-1]['content'])
            # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
            if settings['reasoner']['enabled']:
                reasoner_messages = copy.deepcopy(request.messages)
                if settings['tools']['language']['enabled']:
                    reasoner_messages[-1]['content'] = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€æ¨ç†åˆ†ææ€è€ƒï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨ç†åˆ†æï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\nç”¨æˆ·ï¼š" + reasoner_messages[-1]['content']
                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                reasoner_stream = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=reasoner_messages,
                    stream=True,
                    max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    temperature=settings['reasoner']['temperature']
                )
                full_reasoning = ""
                # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                async for chunk in reasoner_stream:
                    if not chunk.choices:
                        continue

                    chunk_dict = chunk.model_dump()
                    delta = chunk_dict["choices"][0].get("delta", {})
                    if delta:
                        reasoning_content = delta.get("reasoning_content", "")
                        if reasoning_content:
                            full_reasoning += reasoning_content
                    yield f"data: {json.dumps(chunk_dict)}\n\n"

                # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
            # çŠ¶æ€è·Ÿè¸ªå˜é‡
            in_reasoning = False
            reasoning_buffer = []
            content_buffer = []
            open_tag = "<think>"
            close_tag = "</think>"
            if settings['tools']['language']['enabled']:
                request.messages[-1]['content'] = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€å›ç­”é—®é¢˜ï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=request.messages,
                    temperature=request.temperature,
                    tools=tools,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p,
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=request.messages,
                    temperature=request.temperature,
                    stream=True,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p,
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                )
            tool_calls = []
            full_content = ""
            search_not_done = False
            search_task = ""
            async for chunk in response:
                if not chunk.choices:
                    continue
                choice = chunk.choices[0]
                if choice.delta.tool_calls:  # function_calling
                    for idx, tool_call in enumerate(choice.delta.tool_calls):
                        tool = choice.delta.tool_calls[idx]
                        if len(tool_calls) <= idx:
                            tool_calls.append(tool)
                            continue
                        if tool.function.arguments:
                            # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                            tool_calls[idx].function.arguments += tool.function.arguments
                else:
                    # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                    chunk_dict = chunk.model_dump()
                    delta = chunk_dict["choices"][0]["delta"]
                    
                    # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                    delta.setdefault("content", "")
                    delta.setdefault("reasoning_content", "")
                    
                    # ä¼˜å…ˆå¤„ç† reasoning_content
                    if delta["reasoning_content"]:
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                        continue

                    # å¤„ç†å†…å®¹
                    current_content = delta["content"]
                    buffer = current_content
                    
                    while buffer:
                        if not in_reasoning:
                            # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                            start_pos = buffer.find(open_tag)
                            if start_pos != -1:
                                # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                content_buffer.append(buffer[:start_pos])
                                buffer = buffer[start_pos+len(open_tag):]
                                in_reasoning = True
                            else:
                                content_buffer.append(buffer)
                                buffer = ""
                        else:
                            # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                            end_pos = buffer.find(close_tag)
                            if end_pos != -1:
                                # å¤„ç†æ€è€ƒå†…å®¹
                                reasoning_buffer.append(buffer[:end_pos])
                                buffer = buffer[end_pos+len(close_tag):]
                                in_reasoning = False
                            else:
                                reasoning_buffer.append(buffer)
                                buffer = ""
                    
                    # æ„é€ æ–°çš„deltaå†…å®¹
                    new_content = "".join(content_buffer)
                    new_reasoning = "".join(reasoning_buffer)
                    
                    # æ›´æ–°chunkå†…å®¹
                    delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                    delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                    
                    # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                    if in_reasoning:
                        content_buffer = [new_content.split(open_tag)[-1]] 
                    else:
                        content_buffer = []
                    reasoning_buffer = []
                    
                    yield f"data: {json.dumps(chunk_dict)}\n\n"
                    full_content += delta.get("content", "")
            # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
            if content_buffer or reasoning_buffer:
                final_chunk = {
                    "choices": [{
                        "delta": {
                            "content": "".join(content_buffer),
                            "reasoning_content": "".join(reasoning_buffer)
                        }
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                full_content += final_chunk["choices"][0]["delta"].get("content", "")
            if tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled']: 
                search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{full_content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
                response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "system",
                        "content": source_prompt,
                        },
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5
                )
                response_content = response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                response_content = json.loads(response_content)
                if response_content["status"] == "done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": "\n\nâœ…ä»»åŠ¡å®Œæˆ\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": "\n\nâä»»åŠ¡æœªå®Œæˆ\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": full_content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    search_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": "\n\nâ“éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(search_chunk)}\n\n"
                    search_not_done = False
            reasoner_messages = copy.deepcopy(request.messages)
            while tool_calls or search_not_done:
                full_content = ""
                if tool_calls:
                    response_content = tool_calls[0].function
                    if response_content.name in  ["DDGsearch_async","searxng_async", "Tavily_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": "\n\næ€è€ƒåè”ç½‘æœç´¢ä¸­ï¼Œè¯·ç¨å€™...\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in  ["jina_crawler_async","Crawl4Ai_search_async"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": "\n\næœç´¢ç½‘é¡µè¯¦ç»†ä¿¡æ¯ä¸­ï¼Œè¯·ç¨å€™...\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    elif response_content.name in ["query_knowledge_base"]:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": "\n\næŸ¥è¯¢çŸ¥è¯†åº“ä¸­ï¼Œè¯·ç¨å€™...\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    else:
                        chunk_dict = {
                            "id": "webSearch",
                            "choices": [
                                {
                                    "finish_reason": None,
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "reasoning_content": f"\n\nè°ƒç”¨{response_content.name}å·¥å…·ä¸­ï¼Œè¯·ç¨å€™...\n\n"
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk_dict)}\n\n"
                    print(response_content.arguments)
                    modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                    print(modified_data)
                    # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                    data_list = json.loads(modified_data)
                    results = await dispatch_tool(response_content.name, data_list[0])
                    if results is None:
                        chunk = {
                            "id": "extra_tools",
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "role":"assistant",
                                        "content": "",
                                        "tool_calls":tool_calls,
                                    }
                                }
                            ]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        break
                    request.messages.append(
                        {
                            "tool_calls": [
                                {
                                    "id": tool_calls[0].id,
                                    "function": {
                                        "arguments": json.dumps(data_list[0]),
                                        "name": response_content.name,
                                    },
                                    "type": tool_calls[0].type,
                                }
                            ],
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    request.messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_calls[0].id,
                            "name": response_content.name,
                            "content": str(results),
                        }
                    )
                    if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                        request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
                    reasoner_messages.append(
                        {
                            "role": "assistant",
                            "content": str(response_content),
                        }
                    )
                    reasoner_messages.append(
                        {
                            "role": "user",
                            "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                        }
                    )
                    # è·å–æ—¶é—´æˆ³å’Œuuid
                    timestamp = time.time()
                    uid = str(uuid.uuid4())
                    # æ„é€ æ–‡ä»¶å
                    filename = f"{timestamp}_{uid}.txt"
                    # å°†æœç´¢ç»“æœå†™å…¥uploaded_fileæ–‡ä»¶å¤¹ä¸‹çš„filenameæ–‡ä»¶
                    with open(f"uploaded_files/{filename}", "w", encoding='utf-8') as f:
                        f.write(str(results))            
                    # å°†æ–‡ä»¶é“¾æ¥æ›´æ–°ä¸ºæ–°çš„é“¾æ¥
                    fileLink=f"http://{HOST}:{PORT}/uploaded_files/{filename}"
                    tool_chunk = {
                        "choices": [{
                            "delta": {
                                "reasoning_content": f"\n\n[{response_content.name}å·¥å…·ç»“æœ]({fileLink})\n\n",
                            }
                        }]
                    }
                    yield f"data: {json.dumps(tool_chunk)}\n\n"
                # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å‹
                if settings['reasoner']['enabled']:
                    # æµå¼è°ƒç”¨æ¨ç†æ¨¡å‹
                    reasoner_stream = await reasoner_client.chat.completions.create(
                        model=settings['reasoner']['model'],
                        messages=reasoner_messages,
                        stream=True,
                        max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        temperature=settings['reasoner']['temperature']
                    )
                    full_reasoning = ""
                    # å¤„ç†æ¨ç†æ¨¡å‹çš„æµå¼å“åº”
                    async for chunk in reasoner_stream:
                        if not chunk.choices:
                            continue

                        chunk_dict = chunk.model_dump()
                        delta = chunk_dict["choices"][0].get("delta", {})
                        if delta:
                            reasoning_content = delta.get("reasoning_content", "")
                            if reasoning_content:
                                full_reasoning += reasoning_content
                        
                        yield f"data: {json.dumps(chunk_dict)}\n\n"

                    # åœ¨æ¨ç†ç»“æŸåæ·»åŠ å®Œæ•´æ¨ç†å†…å®¹åˆ°æ¶ˆæ¯
                    request.messages[-1]['content'] += f"\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š{full_reasoning}"
                if tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=request.messages,
                        temperature=request.temperature,
                        tools=tools,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p,
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=request.messages,
                        temperature=request.temperature,
                        stream=True,
                        max_tokens=request.max_tokens or settings['max_tokens'],
                        top_p=request.top_p,
                        frequency_penalty=request.frequency_penalty,
                        presence_penalty=request.presence_penalty,
                    )
                tool_calls = []
                async for chunk in response:
                    if not chunk.choices:
                        continue
                    if chunk.choices:
                        choice = chunk.choices[0]
                        if choice.delta.tool_calls:  # function_calling
                            for idx, tool_call in enumerate(choice.delta.tool_calls):
                                tool = choice.delta.tool_calls[idx]
                                if len(tool_calls) <= idx:
                                    tool_calls.append(tool)
                                    continue
                                if tool.function.arguments:
                                    # functionå‚æ•°ä¸ºæµå¼å“åº”ï¼Œéœ€è¦æ‹¼æ¥
                                    tool_calls[idx].function.arguments += tool.function.arguments
                        else:
                            # åˆ›å»ºåŸå§‹chunkçš„æ‹·è´
                            chunk_dict = chunk.model_dump()
                            delta = chunk_dict["choices"][0]["delta"]
                            
                            # åˆå§‹åŒ–å¿…è¦å­—æ®µ
                            delta.setdefault("content", "")
                            delta.setdefault("reasoning_content", "")

                             # ä¼˜å…ˆå¤„ç† reasoning_content
                            if delta["reasoning_content"]:
                                yield f"data: {json.dumps(chunk_dict)}\n\n"
                                continue
                            
                            # å¤„ç†å†…å®¹
                            current_content = delta["content"]
                            buffer = current_content
                            
                            while buffer:
                                if not in_reasoning:
                                    # å¯»æ‰¾å¼€å§‹æ ‡ç­¾
                                    start_pos = buffer.find(open_tag)
                                    if start_pos != -1:
                                        # å¤„ç†å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹
                                        content_buffer.append(buffer[:start_pos])
                                        buffer = buffer[start_pos+len(open_tag):]
                                        in_reasoning = True
                                    else:
                                        content_buffer.append(buffer)
                                        buffer = ""
                                else:
                                    # å¯»æ‰¾ç»“æŸæ ‡ç­¾
                                    end_pos = buffer.find(close_tag)
                                    if end_pos != -1:
                                        # å¤„ç†æ€è€ƒå†…å®¹
                                        reasoning_buffer.append(buffer[:end_pos])
                                        buffer = buffer[end_pos+len(close_tag):]
                                        in_reasoning = False
                                    else:
                                        reasoning_buffer.append(buffer)
                                        buffer = ""
                            
                            # æ„é€ æ–°çš„deltaå†…å®¹
                            new_content = "".join(content_buffer)
                            new_reasoning = "".join(reasoning_buffer)
                            
                            # æ›´æ–°chunkå†…å®¹
                            delta["content"] = new_content.strip("\x00")  # ä¿ç•™æœªå®Œæˆå†…å®¹
                            delta["reasoning_content"] = new_reasoning.strip("\x00") or None
                            
                            # é‡ç½®ç¼“å†²åŒºä½†ä¿ç•™æœªå®Œæˆéƒ¨åˆ†
                            if in_reasoning:
                                content_buffer = [new_content.split(open_tag)[-1]] 
                            else:
                                content_buffer = []
                            reasoning_buffer = []
                            
                            yield f"data: {json.dumps(chunk_dict)}\n\n"
                            full_content += delta.get("content", "")
                # æœ€ç»ˆflushæœªå®Œæˆå†…å®¹
                if content_buffer or reasoning_buffer:
                    final_chunk = {
                        "choices": [{
                            "delta": {
                                "content": "".join(content_buffer),
                                "reasoning_content": "".join(reasoning_buffer)
                            }
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                    full_content += final_chunk["choices"][0]["delta"].get("content", "")
                if tool_calls:
                    pass
                elif settings['tools']['deepsearch']['enabled']: 
                    search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{full_content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
                    response = await client.chat.completions.create(
                        model=model,
                        messages=[                        
                            {
                            "role": "system",
                            "content": source_prompt,
                            },
                            {
                            "role": "user",
                            "content": search_prompt,
                            }
                        ],
                        temperature=0.5,
                    )
                    response_content = response.choices[0].message.content
                    # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                    if "```json" in response_content:
                        try:
                            response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                        except:
                            # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                            response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                    response_content = json.loads(response_content)
                    if response_content["status"] == "done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": "\n\nâœ…ä»»åŠ¡å®Œæˆ\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
                    elif response_content["status"] == "not_done":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": "\n\nâä»»åŠ¡æœªå®Œæˆ\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = True
                        search_task = response_content["unfinished_task"]
                        task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                        request.messages.append(
                            {
                                "role": "assistant",
                                "content": full_content,
                            }
                        )
                        request.messages.append(
                            {
                                "role": "user",
                                "content": task_prompt,
                            }
                        )
                    elif response_content["status"] == "need_more_info":
                        search_chunk = {
                            "choices": [{
                                "delta": {
                                    "reasoning_content": "\n\nâ“éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚\n\n",
                                }
                            }]
                        }
                        yield f"data: {json.dumps(search_chunk)}\n\n"
                        search_not_done = False
            yield "data: [DONE]\n\n"

        return StreamingResponse(
            stream_generator(user_prompt),
            media_type="text/event-stream",
            headers={
                "Content-Type": "text/event-stream",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    except APIStatusError as e:
        return JSONResponse(
            status_code=e.status_code,
            content={"error": {"message": e.message, "type": "api_error", "code": e.code}}
        )

async def generate_complete_response(client,reasoner_client, request: ChatRequest, settings: dict):
    global mcp_client_list
    from py.load_files import get_files_content
    from py.web_search import (
        DDGsearch_async, 
        searxng_async, 
        Tavily_search_async,
        duckduckgo_tool, 
        searxng_tool, 
        tavily_tool, 
        jina_crawler_tool, 
        Crawl4Ai_tool
    )
    from py.know_base import kb_tool
    open_tag = "<think>"
    close_tag = "</think>"
    tools = request.tools or []
    tools = request.tools or []
    print(tools)
    if mcp_client_list:
        for server_name, mcp_client in mcp_client_list.items():
            if server_name in settings['mcpServers']:
                if 'disabled' not in settings['mcpServers'][server_name]:
                    settings['mcpServers'][server_name]['disabled'] = False
                if settings['mcpServers'][server_name]['disabled'] == False and settings['mcpServers'][server_name]['processingStatus'] == 'ready':
                    function = await mcp_client.get_openai_functions()
                    if function:
                        tools.extend(function)
    search_not_done = False
    search_task = ""
    try:
        model = settings['model']
        if request.fileLinks:
            # éå†æ–‡ä»¶é“¾æ¥åˆ—è¡¨
            for file_link in request.fileLinks:
                # å¦‚æœfile_linkæ˜¯http://${HOST}:${PORT}å¼€å¤´
                if file_link.startswith(f"http://${HOST}:{PORT}"):
                    # å°†"http://${HOST}:{PORT}"æ›¿æ¢ä¸º"uploaded_files"
                    file_link = file_link.replace(f"http://{HOST}:{PORT}", "uploaded_files")
            # å¼‚æ­¥è·å–æ–‡ä»¶å†…å®¹
            files_content = await get_files_content(request.fileLinks)
            system_message = f"\n\nç›¸å…³æ–‡ä»¶å†…å®¹ï¼š{files_content}"
            
            # ä¿®å¤å­—ç¬¦ä¸²æ‹¼æ¥é”™è¯¯
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += system_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': system_message})
        kb_list = []
        if settings["knowledgeBases"]:
            for kb in settings["knowledgeBases"]:
                if kb["enabled"] and kb["processingStatus"] == "completed":
                    kb_list.append({"kb_id":kb["id"],"name": kb["name"],"introduction":kb["introduction"]})
        if kb_list:
            kb_list_message = f"\n\nå¯è°ƒç”¨çš„çŸ¥è¯†åº“åˆ—è¡¨ï¼š{json.dumps(kb_list, ensure_ascii=False)}"
            if request.messages and request.messages[0]['role'] == 'system':
                request.messages[0]['content'] += kb_list_message
            else:
                request.messages.insert(0, {'role': 'system', 'content': kb_list_message})
        user_prompt = request.messages[-1]['content']
        request = tools_change_messages(request, settings)
        if settings['webSearch']['enabled']:
            if settings['webSearch']['when'] == 'before_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    results = await DDGsearch_async(user_prompt)
                elif settings['webSearch']['engine'] == 'searxng':
                    results = await searxng_async(user_prompt)
                elif settings['webSearch']['engine'] == 'tavily':
                    results = await Tavily_search_async(user_prompt)
                if results:
                    request.messages[-1]['content'] += f"\n\nè”ç½‘æœç´¢ç»“æœï¼š{results}"
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                if settings['webSearch']['engine'] == 'duckduckgo':
                    tools.append(duckduckgo_tool)
                elif settings['webSearch']['engine'] == 'searxng':
                    tools.append(searxng_tool)
                elif settings['webSearch']['engine'] == 'tavily':
                    tools.append(tavily_tool)
                if settings['webSearch']['crawler'] == 'jina':
                    tools.append(jina_crawler_tool)
                elif settings['webSearch']['crawler'] == 'crawl4ai':
                    tools.append(Crawl4Ai_tool)
        if kb_list:
            tools.append(kb_tool)
        if settings['tools']['deepsearch']['enabled']: 
            deepsearch_messages = copy.deepcopy(request.messages)
            deepsearch_messages[-1]['content'] += "/n/næ€»ç»“æ¦‚æ‹¬ä¸€ä¸‹ç”¨æˆ·çš„é—®é¢˜æˆ–ç»™å‡ºçš„å½“å‰ä»»åŠ¡ï¼Œæ— éœ€å›ç­”æˆ–æ‰§è¡Œè¿™äº›å†…å®¹ï¼Œç›´æ¥è¿”å›æ€»ç»“å³å¯ï¼Œä½†ä¸èƒ½çœç•¥é—®é¢˜æˆ–ä»»åŠ¡çš„ç»†èŠ‚ã€‚å¦‚æœç”¨æˆ·è¾“å…¥çš„åªæ˜¯é—²èŠæˆ–è€…ä¸åŒ…å«ä»»åŠ¡å’Œé—®é¢˜ï¼Œç›´æ¥æŠŠç”¨æˆ·è¾“å…¥é‡å¤è¾“å‡ºä¸€éå³å¯ã€‚"
            response = await client.chat.completions.create(
                model=model,
                messages=deepsearch_messages,
                temperature=0.5, 
                max_tokens=512
            )
            user_prompt = response.choices[0].message.content
            request.messages[-1]['content'] += f"\n\nå¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºé—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œç›´æ¥é—²èŠå³å¯ï¼Œå¦‚æœç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–è€…ä»»åŠ¡ï¼Œä»»åŠ¡æè¿°ä¸æ¸…æ™°æˆ–è€…ä½ éœ€è¦è¿›ä¸€æ­¥äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œä½ å¯ä»¥æš‚æ—¶ä¸å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯åˆ†æéœ€è¦è®©ç”¨æˆ·è¿›ä¸€æ­¥æ˜ç¡®å“ªäº›éœ€æ±‚ã€‚"
        if settings['reasoner']['enabled']:
            reasoner_messages = copy.deepcopy(request.messages)
            if settings['tools']['language']['enabled']:
                reasoner_messages[-1]['content'] = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€æ¨ç†åˆ†ææ€è€ƒï¼Œä¸è¦ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨ç†åˆ†æï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\nç”¨æˆ·ï¼š" + reasoner_messages[-1]['content']
            if tools:
                reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
            reasoner_response = await reasoner_client.chat.completions.create(
                model=settings['reasoner']['model'],
                messages=reasoner_messages,
                stream=False,
                max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                temperature=settings['reasoner']['temperature']
            )
            request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        if settings['tools']['language']['enabled']:
            request.messages[-1]['content'] = f"è¯·ä½¿ç”¨{settings['tools']['language']['language']}è¯­è¨€å›ç­”é—®é¢˜ï¼Œè¯­æ°”é£æ ¼ä¸º{settings['tools']['language']['tone']}\n\nç”¨æˆ·ï¼š" + request.messages[-1]['content']
        if tools:
            response = await client.chat.completions.create(
                model=model,
                messages=request.messages,
                temperature=request.temperature,
                tools=tools,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p,
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
            )
        else:
            response = await client.chat.completions.create(
                model=model,
                messages=request.messages,
                temperature=request.temperature,
                stream=False,
                max_tokens=request.max_tokens or settings['max_tokens'],
                top_p=request.top_p,
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
            )
        if response.choices[0].message.tool_calls:
            pass
        elif settings['tools']['deepsearch']['enabled']: 
            search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{response.choices[0].message.content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
            search_response = await client.chat.completions.create(
                model=model,
                messages=[
                    {
                    "role": "user",
                    "content": search_prompt,
                    }
                ],
                temperature=0.5,
            )
            response_content = search_response.choices[0].message.content
            print(response_content)
            # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
            if "```json" in response_content:
                try:
                    response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                except:
                    # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                    response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
            response_content = json.loads(response_content)
            if response_content["status"] == "done":
                search_not_done = False
            elif response_content["status"] == "not_done":
                search_not_done = True
                search_task = response_content["unfinished_task"]
                task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                request.messages.append(
                    {
                        "role": "assistant",
                        "content": search_response.choices[0].message.content,
                    }
                )
                request.messages.append(
                    {
                        "role": "user",
                        "content": task_prompt,
                    }
                )
            elif response_content["status"] == "need_more_info":
                search_not_done = False
        reasoner_messages = copy.deepcopy(request.messages)
        while response.choices[0].message.tool_calls or search_not_done:
            if response.choices[0].message.tool_calls:
                assistant_message = response.choices[0].message
                response_content = assistant_message.tool_calls[0].function
                print(response_content.name)
                modified_data = '[' + response_content.arguments.replace('}{', '},{') + ']'
                # ä½¿ç”¨json.loadsæ¥è§£æä¿®æ”¹åçš„å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                data_list = json.loads(modified_data)
                # å­˜å‚¨å¤„ç†ç»“æœ
                results = []
                for data in data_list:
                    result = await dispatch_tool(response_content.name, data) # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                    if result is not None:
                        # å°†ç»“æœæ·»åŠ åˆ°resultsåˆ—è¡¨ä¸­
                        results.append(json.dumps(result))

                # å°†æ‰€æœ‰ç»“æœæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²
                combined_results = ''.join(results)
                if combined_results:
                    results = combined_results
                else:
                    results = None
                print(results)
                if results is None:
                    break
                request.messages.append(
                    {
                        "tool_calls": [
                            {
                                "id": assistant_message.tool_calls[0].id,
                                "function": {
                                    "arguments": response_content.arguments,
                                    "name": response_content.name,
                                },
                                "type": assistant_message.tool_calls[0].type,
                            }
                        ],
                        "role": "assistant",
                        "content": str(response_content),
                    }
                )
                request.messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": assistant_message.tool_calls[0].id,
                        "name": response_content.name,
                        "content": str(results),
                    }
                )
            if settings['webSearch']['when'] == 'after_thinking' or settings['webSearch']['when'] == 'both':
                request.messages[-1]['content'] += f"\nå¯¹äºè”ç½‘æœç´¢çš„ç»“æœï¼Œå¦‚æœè”ç½‘æœç´¢çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨è”ç½‘æœç´¢æŸ¥è¯¢è¿˜æœªç»™å‡ºçš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœå·²ç»è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"
            reasoner_messages.append(
                {
                    "role": "assistant",
                    "content": str(response_content),
                }
            )
            reasoner_messages.append(
                {
                    "role": "user",
                    "content": f"{response_content.name}å·¥å…·ç»“æœï¼š"+str(results),
                }
            )
            if settings['reasoner']['enabled']:

                if tools:
                    reasoner_messages[-1]['content'] += f"å¯ç”¨å·¥å…·ï¼š{json.dumps(tools)}"
                reasoner_response = await reasoner_client.chat.completions.create(
                    model=settings['reasoner']['model'],
                    messages=request.messages,
                    stream=False,
                    max_tokens=1, # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    temperature=settings['reasoner']['temperature']
                )
                request.messages[-1]['content'] = request.messages[-1]['content'] + "\n\nå¯å‚è€ƒçš„æ¨ç†è¿‡ç¨‹ï¼š" + reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
            if tools:
                response = await client.chat.completions.create(
                    model=model,
                    messages=request.messages,
                    temperature=request.temperature,
                    tools=tools,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p,
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                )
            else:
                response = await client.chat.completions.create(
                    model=model,
                    messages=request.messages,
                    temperature=request.temperature,
                    stream=False,
                    max_tokens=request.max_tokens or settings['max_tokens'],
                    top_p=request.top_p,
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                )
            print(response)
            if response.choices[0].message.tool_calls:
                pass
            elif settings['tools']['deepsearch']['enabled']: 
                search_prompt = f"""
åˆå§‹ä»»åŠ¡ï¼š
{user_prompt}

å½“å‰ç»“æœï¼š
{response.choices[0].message.content}

è¯·åˆ¤æ–­åˆå§‹ä»»åŠ¡æ˜¯å¦è¢«å®Œæˆæˆ–éœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ã€‚

å¦‚æœå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "done",
    "unfinished_task": ""
}}

å¦‚æœæœªå®Œæˆï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "not_done",
    "unfinished_task": "è¿™é‡Œå¡«å…¥æœªå®Œæˆçš„ä»»åŠ¡"
}}

å¦‚æœéœ€è¦ç”¨æˆ·æ˜ç¡®éœ€æ±‚ï¼Œè¯·è¾“å‡ºjsonå­—ç¬¦ä¸²ï¼š
{{
    "status": "need_more_info",
    "unfinished_task": ""
}}
"""
                search_response = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                        "role": "user",
                        "content": search_prompt,
                        }
                    ],
                    temperature=0.5,
                )
                response_content = search_response.choices[0].message.content
                # ç”¨re æå–```json åŒ…è£¹jsonå­—ç¬¦ä¸² ```
                if "```json" in response_content:
                    try:
                        response_content = re.search(r'```json(.*?)```', response_content, re.DOTALL).group(1)
                    except:
                        # ç”¨re æå–```json ä¹‹åçš„å†…å®¹
                        response_content = re.search(r'```json(.*?)', response_content, re.DOTALL).group(1)
                response_content = json.loads(response_content)
                if response_content["status"] == "done":
                    search_not_done = False
                elif response_content["status"] == "not_done":
                    search_not_done = True
                    search_task = response_content["unfinished_task"]
                    task_prompt = f"è¯·ç»§ç»­å®Œæˆåˆå§‹ä»»åŠ¡ä¸­æœªå®Œæˆçš„ä»»åŠ¡ï¼š\n\n{search_task}\n\nåˆå§‹ä»»åŠ¡ï¼š{user_prompt}\n\næœ€åï¼Œè¯·ç»™å‡ºå®Œæ•´çš„åˆå§‹ä»»åŠ¡çš„æœ€ç»ˆç»“æœã€‚"
                    request.messages.append(
                        {
                            "role": "assistant",
                            "content": search_response.choices[0].message.content,
                        }
                    )
                    request.messages.append(
                        {
                            "role": "user",
                            "content": task_prompt,
                        }
                    )
                elif response_content["status"] == "need_more_info":
                    search_not_done = False
       # å¤„ç†å“åº”å†…å®¹
        response_dict = response.model_dump()
        content = response_dict["choices"][0]['message']['content']
        if open_tag in content and close_tag in content:
            reasoning_content = re.search(fr'{open_tag}(.*?)\{close_tag}', content, re.DOTALL)
            if reasoning_content:
                # å­˜å‚¨åˆ° reasoning_content å­—æ®µ
                response_dict["choices"][0]['message']['reasoning_content'] = reasoning_content.group(1).strip()
                # ç§»é™¤åŸå†…å®¹ä¸­çš„æ ‡ç­¾éƒ¨åˆ†
                response_dict["choices"][0]['message']['content'] = re.sub(fr'{open_tag}(.*?)\{close_tag}', '', content, flags=re.DOTALL).strip()
        if settings['reasoner']['enabled']:
            response_dict["choices"][0]['message']['reasoning_content'] = reasoner_response.model_dump()['choices'][0]['message']['reasoning_content']
        return JSONResponse(content=response_dict)
    except APIStatusError as e:
        return JSONResponse(
            status_code=e.status_code,
            content={"error": {"message": e.message, "type": "api_error", "code": e.code}}
        )

# åœ¨ç°æœ‰è·¯ç”±åæ·»åŠ ä»¥ä¸‹ä»£ç 
@app.get("/v1/models")
async def get_models():
    global client, settings,reasoner_client
    
    try:
        # é‡æ–°åŠ è½½æœ€æ–°è®¾ç½®
        current_settings = load_settings()
        
        # éªŒè¯APIå¯†é’¥
        if not current_settings.get("api_key"):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={
                    "error": {
                        "message": "API key not configured",
                        "type": "invalid_request_error",
                        "code": "api_key_missing"
                    }
                }
            )
        
        # åŠ¨æ€æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        if (current_settings['api_key'] != settings['api_key'] 
            or current_settings['base_url'] != settings['base_url']):
            client = AsyncOpenAI(
                api_key=current_settings['api_key'],
                base_url=current_settings['base_url'] or "https://api.openai.com/v1",
            )
            settings = current_settings
        if (current_settings['reasoner']['api_key'] != settings['reasoner']['api_key'] 
            or current_settings['reasoner']['base_url'] != settings['reasoner']['base_url']):
            reasoner_client = AsyncOpenAI(
                api_key=current_settings['reasoner']['api_key'],
                base_url=current_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
            )
            settings = current_settings
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = await client.models.list()
        
        # è½¬æ¢å“åº”æ ¼å¼ä¸å®˜æ–¹APIä¸€è‡´
        return JSONResponse(content=model_list.model_dump_json())
        
    except APIStatusError as e:
        return JSONResponse(
            status_code=e.status_code,
            content={
                "error": {
                    "message": e.message,
                    "type": e.type or "api_error",
                    "code": e.code
                }
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": {
                    "message": str(e),
                    "type": "server_error",
                    "code": 500
                }
            }
        )

class ProviderModelRequest(BaseModel):
    url: str
    api_key: str

@app.post("/v1/providers/models")
async def fetch_provider_models(request: ProviderModelRequest):
    try:
        # ä½¿ç”¨ä¼ å…¥çš„provideré…ç½®åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        client = AsyncOpenAI(api_key=request.api_key, base_url=request.url)
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_list = await client.models.list()
        # æå–æ¨¡å‹IDå¹¶è¿”å›
        return JSONResponse(content={"data": [model.id for model in model_list.data]})
    except Exception as e:
        # å¤„ç†å¼‚å¸¸ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions")
async def chat_endpoint(request: ChatRequest):
    global client, settings,reasoner_client,mcp_client_list
    model = request.model or 'super-model' # é»˜è®¤ä½¿ç”¨ 'super-model'
    if model == 'super-model':
        current_settings = load_settings()

        # åŠ¨æ€æ›´æ–°å®¢æˆ·ç«¯é…ç½®
        if (current_settings['api_key'] != settings['api_key'] 
            or current_settings['base_url'] != settings['base_url']):
            if current_settings['api_key']:
                client = AsyncOpenAI(
                    api_key=current_settings['api_key'],
                    base_url=current_settings['base_url'] or "https://api.openai.com/v1",
                )
            else:
                client = AsyncOpenAI(
                    base_url=settings['base_url'] or "https://api.openai.com/v1",
                )
        if (current_settings['reasoner']['api_key'] != settings['reasoner']['api_key'] 
            or current_settings['reasoner']['base_url'] != settings['reasoner']['base_url']):
            if current_settings['reasoner']['api_key']:
                reasoner_client = AsyncOpenAI(
                    api_key=current_settings['reasoner']['api_key'],
                    base_url=current_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
                )
            else:
                reasoner_client = AsyncOpenAI(
                    base_url=settings['reasoner']['base_url'] or "https://api.openai.com/v1",
                )

        if current_settings != settings:
            settings = current_settings
        try:
            if request.stream:
                return await generate_stream_response(client,reasoner_client, request, settings)
            return await generate_complete_response(client,reasoner_client, request, settings)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    else:
        current_settings = load_settings()
        agentSettings = current_settings['agents'].get(model, {})
        if not agentSettings:
            raise HTTPException(status_code=400, detail="Agent not found")
        if agentSettings['config_path']:
            with open(agentSettings['config_path'], 'r') as f:
                agent_settings = json.load(f)
            # å°†"system_prompt"æ’å…¥åˆ°request.messages[0].contentä¸­
            if agentSettings['system_prompt']:
                if request.messages[0]['role'] == 'system':
                    request.messages[0]['content'] = agentSettings['system_prompt'] + "\n\n" + request.messages[0].content
                else:
                    request.messages.insert(0, {'role': 'system', 'content': agentSettings['system_prompt']})
        agent_client = AsyncOpenAI(
            api_key=agent_settings['api_key'],
            base_url=agent_settings['base_url'] or "https://api.openai.com/v1",
        )
        agent_reasoner_client = AsyncOpenAI(
            api_key=agent_settings['reasoner']['api_key'],
            base_url=agent_settings['reasoner']['base_url'] or "https://api.openai.com/v1",
        )

        try:
            if request.stream:
                return await generate_stream_response(agent_client,agent_reasoner_client, request, agent_settings)
            return await generate_complete_response(agent_client,agent_reasoner_client, request, agent_settings)
        except asyncio.CancelledError:
            # å¤„ç†å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥çš„æƒ…å†µ
            print("Client disconnected")
            raise
        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"error": {"message": str(e), "type": "server_error", "code": 500}}
            )
    
# æ·»åŠ çŠ¶æ€å­˜å‚¨
mcp_status = {}
@app.post("/create_mcp")
async def create_mcp_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    mcp_id = data.get("mcpId")
    
    if not mcp_id:
        raise HTTPException(status_code=400, detail="Missing mcpId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_mcp, mcp_id)
    
    return {"success": True, "message": "MCPæœåŠ¡å™¨åˆå§‹åŒ–å·²å¼€å§‹"}
@app.get("/mcp_status/{mcp_id}")
async def get_mcp_status(mcp_id: str):
    status = mcp_status.get(mcp_id, "not_found")
    return {"mcp_id": mcp_id, "status": status}
async def process_mcp(mcp_id: str):
    global mcp_client_list
    mcp_status[mcp_id] = "initializing"
    try:
        # è·å–å¯¹åº”æœåŠ¡å™¨çš„é…ç½®
        cur_settings = load_settings()
        server_config = cur_settings['mcpServers'][mcp_id]
        
        # æ‰§è¡Œåˆå§‹åŒ–é€»è¾‘
        if mcp_id not in mcp_client_list:
            mcp_client_list[mcp_id] = McpClient()    
            await asyncio.wait_for(mcp_client_list[mcp_id].initialize(mcp_id, server_config), timeout=10)
        else:
            mcp_client_list[mcp_id].disabled = False
        mcp_status[mcp_id] = "ready"
        
    except Exception as e:
        mcp_client_list[mcp_id].disabled = True
        mcp_status[mcp_id] = f"failed: {str(e)}"
        # æ¸…ç†å¤±è´¥é…ç½®
        cur_settings['mcpServers'].pop(mcp_id, None)
        save_settings(cur_settings)

@app.post("/api/remove_mcp")
async def remove_mcp_server(request: Request):
    global settings, mcp_client_list
    try:
        data = await request.json()
        server_name = data.get("serverName", "")

        if not server_name:
            raise HTTPException(status_code=400, detail="No server names provided")

        # ç§»é™¤æŒ‡å®šçš„MCPæœåŠ¡å™¨
        current_settings = load_settings()
        if server_name in current_settings['mcpServers']:
            del current_settings['mcpServers'][server_name]
            save_settings(current_settings)
            settings = current_settings

            # ä»mcp_client_listä¸­ç§»é™¤
            if server_name in mcp_client_list:
                mcp_client_list[server_name].disabled = True

            return JSONResponse({"success": True, "removed": server_name})
        else:
            raise HTTPException(status_code=404, detail="Server not found")
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON format")
    except Exception as e:
        logger.error(f"ç§»é™¤MCPæœåŠ¡å™¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# åœ¨ç°æœ‰è·¯ç”±ä¹‹åæ·»åŠ healthè·¯ç”±
@app.get("/health")
async def health_check():
    return {"status": "ok"}

# è®¾ç½®æ–‡ä»¶å­˜å‚¨ç›®å½•
UPLOAD_DIRECTORY = "./uploaded_files"

if not os.path.exists(UPLOAD_DIRECTORY):
    os.makedirs(UPLOAD_DIRECTORY)

@app.post("/load_file")
async def load_file_endpoint(request: Request, files: List[UploadFile] = File(None)):
    logger.info(f"Received request with content type: {request.headers.get('Content-Type')}")
    file_links = []
    
    content_type = request.headers.get('Content-Type', '')
    
    try:
        if 'multipart/form-data' in content_type:
            # å¤„ç†æµè§ˆå™¨ä¸Šä¼ çš„æ–‡ä»¶
            if not files:
                raise HTTPException(status_code=400, detail="No files provided")
            
            for file in files:
                file_extension = os.path.splitext(file.filename)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_DIRECTORY, unique_filename)
                
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                with open(destination, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                file_link = {
                    "path": f"http://{HOST}:{PORT}/uploaded_files/{unique_filename}",
                    "name": file.filename
                }
                file_links.append(file_link)
        
        elif 'application/json' in content_type:
            # å¤„ç†Electronå‘é€çš„JSONæ–‡ä»¶è·¯å¾„
            data = await request.json()
            logger.info(f"Processing JSON data: {data}")
            
            for file_info in data.get("files", []):
                file_path = file_info.get("path")
                file_name = file_info.get("name", os.path.basename(file_path))
                
                if not os.path.isfile(file_path):
                    logger.error(f"File not found: {file_path}")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                file_extension = os.path.splitext(file_name)[1]
                unique_filename = f"{uuid.uuid4()}{file_extension}"
                destination = os.path.join(UPLOAD_DIRECTORY, unique_filename)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•
                with open(file_path, "rb") as src, open(destination, "wb") as dst:
                    dst.write(src.read())
                
                file_link = {
                    "path": f"http://{HOST}:{PORT}/uploaded_files/{unique_filename}",
                    "name": file_name
                }
                file_links.append(file_link)
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported Content-Type")
        
        return JSONResponse(content={"success": True, "fileLinks": file_links})
    
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create_kb")
async def create_kb_endpoint(request: Request, background_tasks: BackgroundTasks):
    data = await request.json()
    kb_id = data.get("kbId")
    
    if not kb_id:
        raise HTTPException(status_code=400, detail="Missing kbId")
    
    # å°†ä»»åŠ¡æ·»åŠ åˆ°åå°é˜Ÿåˆ—
    background_tasks.add_task(process_kb, kb_id)
    
    return {"success": True, "message": "çŸ¥è¯†åº“å¤„ç†å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢çŠ¶æ€"}

# æ·»åŠ çŠ¶æ€å­˜å‚¨
kb_status = {}
@app.get("/kb_status/{kb_id}")
async def get_kb_status(kb_id: int):
    status = kb_status.get(kb_id, "not_found")
    return {"kb_id": kb_id, "status": status}

# ä¿®æ”¹ process_kb
async def process_kb(kb_id: int):
    kb_status[kb_id] = "processing"
    try:
        from py.know_base import process_knowledge_base
        await process_knowledge_base(kb_id)
        kb_status[kb_id] = "completed"
    except Exception as e:
        kb_status[kb_id] = f"failed: {str(e)}"


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    current_settings = load_settings()
    await websocket.send_json({"type": "settings", "data": current_settings})
    
    try:
        while True:
            data = await websocket.receive_json()
            if data.get("type") == "save_settings":
                save_settings(data.get("data", {}))
                await websocket.send_json({"type": "settings_saved", "success": True})
            elif data.get("type") == "get_settings":
                settings = load_settings()
                await websocket.send_json({"type": "settings", "data": settings})
            elif data.get("type") == "save_agent":
                current_settings = load_settings()
                
                # ç”Ÿæˆæ™ºèƒ½ä½“IDå’Œé…ç½®è·¯å¾„
                agent_id = str(shortuuid.ShortUUID().random(length=8))
                os.makedirs('agents', exist_ok=True)
                config_path = os.path.join('agents', f"{agent_id}.json")
                
                with open(config_path, 'w') as f:
                    json.dump(current_settings, f, indent=4, ensure_ascii=False)
                
                # æ›´æ–°ä¸»é…ç½®
                current_settings['agents'][agent_id] = {
                    "id": agent_id,
                    "name": data['data']['name'],
                    "system_prompt": data['data']['system_prompt'],
                    "config_path": config_path,
                    "enabled": False,
                }
                save_settings(current_settings)
                
                # å¹¿æ’­æ›´æ–°åçš„é…ç½®
                await websocket.send_json({
                    "type": "settings",
                    "data": current_settings
                })
    except Exception as e:
        print(f"WebSocket error: {e}")

app.mount("/uploaded_files", StaticFiles(directory="uploaded_files"), name="uploaded_files")
app.mount("/node_modules", StaticFiles(directory=os.path.join(base_path, "node_modules")), name="node_modules")
app.mount("/", StaticFiles(directory=os.path.join(base_path, "static"), html=True), name="static")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=HOST, port=PORT)